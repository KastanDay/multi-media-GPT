{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing + Loading for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import os \n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-xklvv2mp\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-xklvv2mp\n",
      "  Resolved https://github.com/openai/CLIP.git to commit d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from clip==1.0) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from clip==1.0) (4.64.1)\n",
      "Requirement already satisfied: torch in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from clip==1.0) (1.13.0)\n",
      "Requirement already satisfied: torchvision in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from clip==1.0) (0.14.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: typing_extensions in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from torch->clip==1.0) (4.3.0)\n",
      "Requirement already satisfied: numpy in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from torchvision->clip==1.0) (1.23.3)\n",
      "Requirement already satisfied: requests in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from torchvision->clip==1.0) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from torchvision->clip==1.0) (9.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2022.9.24)\n",
      "Requirement already satisfied: cog in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (0.4.4)\n",
      "Requirement already satisfied: fastapi<1,>=0.6 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from cog) (0.87.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from cog) (2.28.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp<2,>=1.11.1 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from cog) (1.14.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from cog) (1.10.2)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from cog) (4.3.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2,>=1.11.1 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from cog) (1.14.0)\n",
      "Requirement already satisfied: uvicorn[standard]<1,>=0.12 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from cog) (0.19.0)\n",
      "Requirement already satisfied: PyYAML in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from cog) (6.0)\n",
      "Requirement already satisfied: redis<5,>=4 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from cog) (4.3.4)\n",
      "Requirement already satisfied: protobuf<=3.20 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from cog) (3.20.0)\n",
      "Requirement already satisfied: starlette==0.21.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from fastapi<1,>=0.6->cog) (0.21.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from starlette==0.21.0->fastapi<1,>=0.6->cog) (3.6.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.14.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from opentelemetry-exporter-otlp<2,>=1.11.1->cog) (1.14.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.14.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from opentelemetry-exporter-otlp<2,>=1.11.1->cog) (1.14.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.14.0->opentelemetry-exporter-otlp<2,>=1.11.1->cog) (1.56.4)\n",
      "Requirement already satisfied: opentelemetry-api~=1.12 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.14.0->opentelemetry-exporter-otlp<2,>=1.11.1->cog) (1.14.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.14.0->opentelemetry-exporter-otlp<2,>=1.11.1->cog) (2.2.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.14.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.14.0->opentelemetry-exporter-otlp<2,>=1.11.1->cog) (1.14.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.14.0->opentelemetry-exporter-otlp<2,>=1.11.1->cog) (1.43.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.35b0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from opentelemetry-sdk<2,>=1.11.1->cog) (0.35b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from opentelemetry-sdk<2,>=1.11.1->cog) (65.4.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from opentelemetry-api~=1.12->opentelemetry-exporter-otlp-proto-grpc==1.14.0->opentelemetry-exporter-otlp<2,>=1.11.1->cog) (1.2.13)\n",
      "Requirement already satisfied: async-timeout>=4.0.2 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from redis<5,>=4->cog) (4.0.2)\n",
      "Requirement already satisfied: packaging>=20.4 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from redis<5,>=4->cog) (21.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from requests<3,>=2->cog) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from requests<3,>=2->cog) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from requests<3,>=2->cog) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from requests<3,>=2->cog) (3.4)\n",
      "Requirement already satisfied: h11>=0.8 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from uvicorn[standard]<1,>=0.12->cog) (0.14.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from uvicorn[standard]<1,>=0.12->cog) (8.0.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from uvicorn[standard]<1,>=0.12->cog) (0.17.0)\n",
      "Requirement already satisfied: websockets>=10.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from uvicorn[standard]<1,>=0.12->cog) (10.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from uvicorn[standard]<1,>=0.12->cog) (0.18.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from uvicorn[standard]<1,>=0.12->cog) (0.21.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from uvicorn[standard]<1,>=0.12->cog) (0.5.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from deprecated>=1.2.6->opentelemetry-api~=1.12->opentelemetry-exporter-otlp-proto-grpc==1.14.0->opentelemetry-exporter-otlp<2,>=1.11.1->cog) (1.14.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from packaging>=20.4->redis<5,>=4->cog) (3.0.9)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette==0.21.0->fastapi<1,>=0.6->cog) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from grpcio<2.0.0,>=1.0.0->opentelemetry-exporter-otlp-proto-grpc==1.14.0->opentelemetry-exporter-otlp<2,>=1.11.1->cog) (1.16.0)\n",
      "Found existing installation: mmcv 1.7.0\n",
      "Uninstalling mmcv-1.7.0:\n",
      "  Would remove:\n",
      "    /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/mmcv-1.7.0.dist-info/*\n",
      "    /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/mmcv/*\n",
      "  Would not remove (might be manually added):\n",
      "    /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/mmcv/_ext.cpython-38-x86_64-linux-gnu.so\n",
      "    /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/mmcv/ops/csrc/parrots/corner_pool.cpp\n",
      "    /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/mmcv/ops/csrc/parrots/corner_pool_parrots.cpp\n",
      "    /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/mmcv/ops/csrc/parrots/corner_pool_pytorch.h\n",
      "    /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/mmcv/ops/csrc/pytorch/corner_pool.cpp\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in links: https://download.openmmlab.com/mmcv/dist/cu101/torch1.7.0/index.html\n",
      "Requirement already satisfied: mmcv-full==1.4.3 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (1.4.3)\n",
      "Requirement already satisfied: opencv-python>=3 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from mmcv-full==1.4.3) (4.6.0.66)\n",
      "Requirement already satisfied: yapf in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from mmcv-full==1.4.3) (0.32.0)\n",
      "Requirement already satisfied: pyyaml in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from mmcv-full==1.4.3) (6.0)\n",
      "Requirement already satisfied: packaging in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from mmcv-full==1.4.3) (21.3)\n",
      "Requirement already satisfied: addict in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from mmcv-full==1.4.3) (2.4.0)\n",
      "Requirement already satisfied: numpy in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from mmcv-full==1.4.3) (1.23.3)\n",
      "Requirement already satisfied: Pillow in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from mmcv-full==1.4.3) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages (from packaging->mmcv-full==1.4.3) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "### Installs\n",
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "! pip install cog\n",
    "! pip uninstall mmcv\n",
    "! pip install mmcv-full==1.4.3 -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.7.0/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Code to create each training sample and save each sample as a npy file.\n",
    "Npy file is a dict of the following format:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"youtube_name\": \"2ZokZgnjrYs\",                      \n",
    "  \"filename\": video_name,                      \n",
    "  \"youtube_id\": \"Hawaii Vlog\",                         \n",
    "  \"segment_length_seconds\": 15.0,\n",
    "  \"captions\": \"Daniel's chosen format here\",           \n",
    "  \"segment_start_time\":[<segment_start_times_list>],\n",
    "  \"segment_end_time\":[<segment_end_times_list>],     \n",
    "  \"frame_embeddings\":[<frame_embeddings_list>],        \n",
    "  \"audio_embeddings\":[<audio_embeddings_list>],        \n",
    "  \"text_caption_embeddings\":[<text_embeddings_list>],  \n",
    "  \"scene_graph_embeddings\":[<scene_graph_embeddings_list>],     \n",
    "  # \"labels\": [<labels_list>],                       \n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed Whisper File Format\n",
    "\n",
    "The preprocessed Whisper files are of the following format:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        'caption': \"some-caption-1\",\n",
    "        'start': caption_start_time,\n",
    "        'end': caption_end_time, \n",
    "        'segment_word_list': [{'word': 'some-word', 'start': word_start_time, 'end': word_end_time}, ...]\n",
    "    },\n",
    "    {\n",
    "        'caption': \"some-caption-2\",\n",
    "        'start': caption_start_time,\n",
    "        'end': caption_end_time, \n",
    "        'segment_word_list': [{'word': 'some-word', 'start': word_start_time, 'end': word_end_time}, ...]\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `DataPreprocessor` class, instantiate it with the data path to the folder containing .webm videos and the data path containing the Whisper extracted json files. Also, make sure you set the extension to `\".webm\"`. Then, simply call `process_using_audio_dir` with the desired output path to store the npy files. Each npy file corresponds to a single sample i.e. a single segment extracted from a video. The naming convention for the npy files is the following format: `{video_name}_segment{segment_number}.npy`. \n",
    "\n",
    "\n",
    "Example usage: \n",
    "\n",
    "\n",
    "```\n",
    "data_preprocessor = DataPreprocessor(video_data_path=\"/mnt/storage_ssd/yt1b_train_slice/\", audio_data_path=\"/mnt/storage_ssd/yt1b_train_slice_json/\", extension=\".webm\")\n",
    "data_preprocessor.process_using_audio_dir(\"./test_sample_generation\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor: \n",
    "    def __init__(self, video_data_path, audio_data_path, extension=\".mp4\", debug=True):\n",
    "        self.video_data_path = video_data_path\n",
    "        self.audio_data_path = audio_data_path\n",
    "        self.extension = extension\n",
    "        self.debug = debug\n",
    "\n",
    "        # Load the model\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Using {self.device}...\")\n",
    "\n",
    "        self.clip, self.clip_preprocess = clip.load('ViT-B/32', self.device)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Done setting up CLIP...\")\n",
    "\n",
    "        # self.scene_graph_predictor = Predictor()\n",
    "        # self.scene_graph_predictor.setup()\n",
    "\n",
    "\n",
    "    def get_frames_for_segments(self, video_name, segments):\n",
    "        if len(segments) == 0:\n",
    "            return None\n",
    "        \n",
    "        curr_segment_idx = 0\n",
    "        curr_segment_start = segments[0]['start']\n",
    "        curr_segment_end = segments[0]['end']\n",
    "\n",
    "        segment_frames = [[] for i in range(len(segments))]\n",
    "\n",
    "        assert os.path.exists(self.video_data_path+video_name+self.extension)\n",
    "\n",
    "        cap = cv2.VideoCapture(self.video_data_path+video_name+self.extension)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        frame_width = int(cap.get(3))\n",
    "        frame_height = int(cap.get(4))\n",
    "        \n",
    "        size = (frame_width, frame_height)\n",
    "\n",
    "        timestamps = [cap.get(cv2.CAP_PROP_POS_MSEC)]\n",
    "        curr_timestamp = 0.0\n",
    "\n",
    "        segments_done = False\n",
    "\n",
    "        while(cap.isOpened()):\n",
    "            frame_exists, curr_frame = cap.read()\n",
    "            \n",
    "            if frame_exists:\n",
    "                curr_timestamp = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "                timestamps.append(curr_timestamp)\n",
    "\n",
    "                if (curr_timestamp/1000.0) < curr_segment_start:\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    if (curr_timestamp/1000.0) < curr_segment_end:\n",
    "                        segment_frames[curr_segment_idx].append(curr_frame)\n",
    "                    else:\n",
    "                        curr_segment_start = segments[curr_segment_idx]['start']\n",
    "                        curr_segment_end = segments[curr_segment_idx]['end']\n",
    "                        while True:\n",
    "                            if ((curr_timestamp/1000.0) > curr_segment_start) and ((curr_timestamp/1000.0) < curr_segment_end):\n",
    "                                break\n",
    "\n",
    "                            if (curr_timestamp/1000.0) < curr_segment_start:\n",
    "                                break\n",
    "\n",
    "                            curr_segment_idx += 1\n",
    "                            if curr_segment_idx >= len(segments):\n",
    "                                # print(\"segments done hit\")\n",
    "                                segments_done = True\n",
    "                                break\n",
    "                        \n",
    "                            curr_segment_start = segments[curr_segment_idx]['start']\n",
    "                            curr_segment_end = segments[curr_segment_idx]['end']\n",
    "\n",
    "                        if segments_done:\n",
    "                            break\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        print(min(timestamps), max(timestamps))\n",
    "\n",
    "        return segment_frames\n",
    "        \n",
    "\n",
    "    def get_multimodal_features(self, video_name, segments):\n",
    "        segment_frames = self.get_frames_for_segments(video_name, segments)\n",
    "        scene_graph_features = []\n",
    "        clip_features = []\n",
    "        caption_features = []\n",
    "\n",
    "\n",
    "        # extract clip features for each segment  and extract text features for captions\n",
    "        for i, frames_list in enumerate(segment_frames):\n",
    "            if len(frames_list) == 0:\n",
    "                continue\n",
    "\n",
    "            # TODO: extract scene graph features for each segment\n",
    "            # scene_graph = self.scene_graph_predictor.predict(frames_list[0], num_rel=10)\n",
    "\n",
    "            middle_frame_idx = len(frames_list) // 2\n",
    "            sample_frame_idxs = [middle_frame_idx]\n",
    "\n",
    "            resized_frames = [Image.fromarray(cv2.resize(frames_list[frame_idx], dsize=(224, 224), interpolation=cv2.INTER_CUBIC)) for frame_idx in sample_frame_idxs]\n",
    "\n",
    "            image_input = torch.cat([self.clip_preprocess(frame).unsqueeze(0) for frame in resized_frames]).to(self.device)\n",
    "            text_inputs = torch.cat([clip.tokenize(segments[i]['caption'])]).to(self.device)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                image_features = self.clip.encode_image(image_input)\n",
    "                text_features = self.clip.encode_text(text_inputs)\n",
    "\n",
    "                clip_features.append(image_features)\n",
    "                caption_features.append(text_features)\n",
    "\n",
    "        return clip_features, scene_graph_features, caption_features, segment_frames\n",
    "\n",
    "\n",
    "    def construct_training_samples(self, video_name, output_path):\n",
    "        # initialize empty sample\n",
    "        whisper_segments = None\n",
    "        with open(self.audio_data_path+video_name+\".json\", \"r\") as whisper_f:\n",
    "            whisper_segments = json.load(whisper_f)\n",
    "\n",
    "        image_features, scene_graph_features, caption_features = self.get_multimodal_features(video_name, whisper_segments)\n",
    "\n",
    "        # assert len(image_features) == len(scene_graph_features) == len(caption_features) == len(whisper_segments)\n",
    "        \n",
    "        for i, (image_feature, scene_graph_feature, caption_feature, segment_frames) in enumerate(zip(image_features, scene_graph_features, caption_features, segment_frames)):\n",
    "            sample_dict = {\n",
    "                \"filename\": video_name,\n",
    "                \"segment_length\": whisper_segments[i]['end'] - whisper_segments[i]['start'],\n",
    "                \"captions\": whisper_segments[i]['caption'],\n",
    "                \"segment_start_time\": whisper_segments[i]['start'],\n",
    "                \"segment_end_time\": whisper_segments[i]['end'],\n",
    "                \"frame_embeddings\": image_feature,\n",
    "                \"text_caption_embeddings\": caption_feature,\n",
    "                # \"scene_graph_embeddings\": scene_graph_feature\n",
    "                \"segment_frames\": segment_frames\n",
    "            }\n",
    "\n",
    "            # TODO: Uncomment this code to start saving the files\n",
    "            # np.save(f\"{video_name}_segment{i}\", sample_dict)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Constructed training samples\")\n",
    "            \n",
    "    def process_using_audio_dir(self, output_path):\n",
    "        samples_not_found = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        if not os.path.exists(output_path):\n",
    "            # Create a new directory because it does not exist\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        all_whisper_files = os.listdir(self.audio_data_path)\n",
    "        for i in tqdm(range(len(all_whisper_files))):\n",
    "            f = all_whisper_files[i]\n",
    "            if f[-5:] == \".json\":\n",
    "                video_name = f[:-5]\n",
    "                if not os.path.exists(self.video_data_path+video_name+self.extension):\n",
    "                    samples_not_found += 1\n",
    "                else:\n",
    "                    self.construct_training_samples(video_name, output_path)\n",
    "\n",
    "                total_samples += 1\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"[WARNING] {samples_not_found}/{total_samples} are invalid\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda...\n",
      "Done setting up CLIP...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/storage_ssd/yt1b_train_slice_json/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data_preprocessor \u001b[39m=\u001b[39m DataPreprocessor(video_data_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/mnt/storage_ssd/yt1b_train_slice/\u001b[39m\u001b[39m\"\u001b[39m, audio_data_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/mnt/storage_ssd/yt1b_train_slice_json/\u001b[39m\u001b[39m\"\u001b[39m, extension\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.webm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m data_preprocessor\u001b[39m.\u001b[39;49mprocess_using_audio_dir(\u001b[39m\"\u001b[39;49m\u001b[39m./test_sample_generation\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb Cell 8\u001b[0m in \u001b[0;36mDataPreprocessor.process_using_audio_dir\u001b[0;34m(self, output_path)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=157'>158</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(output_path):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m     \u001b[39m# Create a new directory because it does not exist\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=159'>160</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(output_path)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=161'>162</a>\u001b[0m all_whisper_files \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio_data_path)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(all_whisper_files))):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m     f \u001b[39m=\u001b[39m all_whisper_files[i]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/storage_ssd/yt1b_train_slice_json/'"
     ]
    }
   ],
   "source": [
    "data_preprocessor = DataPreprocessor(video_data_path=\"/mnt/storage_ssd/yt1b_train_slice/\", audio_data_path=\"/mnt/storage_ssd/yt1b_train_slice_json/\", extension=\".webm\")\n",
    "data_preprocessor.process_using_audio_dir(\"./test_sample_generation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp_v2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "612b182cb4c3e0acfd877acc6c10f43d075b0ae43380d6b249d2d2b5490153b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
