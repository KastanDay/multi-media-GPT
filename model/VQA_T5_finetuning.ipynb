{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping for real run. \n",
    "\n",
    "Todo: \n",
    "* Define `max_source_length` and `max_target_length` for the model (otherwise truncated).\n",
    "padding token should be replaced with -100, which is the 'ignore_index' of `CrossEntorpyLoss` in PT and TF. For Flax, use `decoder_attention_mask`. \n",
    "Attention_mask. ensures madding tokens of inputs are ignored. \n",
    "\n",
    "* Install apex. \"model will automatically use apex.normalization.FusedRMSNorm instead of T5LayerNorm.\" The former uses an optimized fused kernel which is several times faster than the latter.\n",
    "\n",
    "A note on model sizes: \n",
    "T5-11B (original, not v1.1) weights in float32 are 45.2GB. \n",
    "See this post for using huggingface endpoints on SINGLE GPU for cheap inference: https://www.philschmid.de/deploy-t5-11b\n",
    "Uses mixed precision and sharding, and LLM.int8(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/bbki/kastanday/conda_envs/envs/v4_clip_preprocessing_yt1b/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel, T5Model, T5Config, AutoModelWithLMHead\n",
    "import accelerate\n",
    "# import wandb\n",
    "from tqdm import tqdm\n",
    "import lovely_tensors as lt\n",
    "import math\n",
    "from PIL import Image\n",
    "lt.monkey_patch()\n",
    "# !wandb login  -- reactivate later\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' GLOBALS '''\n",
    "NUM_EPOCHS = 1\n",
    "MODEL_NAME = \"google/t5-v1_1-base\"\n",
    "RUN_NAME = \"all_modalities\"\n",
    "BATCH_SIZE = 1\n",
    "BASE_DIR = '/scratch/bbki/kastanday/whisper'\n",
    "use_scene_graph = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Split VAL2014 into train and test datasets \\n\\nBASE_DIR = \\'/scratch/bbki/kastanday/whisper\\'\\n\\ntrain_set = []\\ntest_set = []\\n\\nfor i, img_name in enumerate(os.listdir(f\"{BASE_DIR}/vqa/val2014\")):\\n    if i >= 39000:\\n        test_set.append(os.path.join(f\"{BASE_DIR}/vqa/val2014\", img_name))\\n    elif i <= 5000:\\n        train_set.append(os.path.join(f\"{BASE_DIR}/vqa/val2014\", img_name))\\n\\nprint(len(train_set))\\nprint(len(test_set))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Split VAL2014 into train and test datasets \n",
    "\n",
    "BASE_DIR = '/scratch/bbki/kastanday/whisper'\n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for i, img_name in enumerate(os.listdir(f\"{BASE_DIR}/vqa/val2014\")):\n",
    "    if i >= 39000:\n",
    "        test_set.append(os.path.join(f\"{BASE_DIR}/vqa/val2014\", img_name))\n",
    "    elif i <= 5000:\n",
    "        train_set.append(os.path.join(f\"{BASE_DIR}/vqa/val2014\", img_name))\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "Initializing clip and scene graph models...\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: \", device)\n",
    "\n",
    "# Initialize preprocessing models for collate_fn \n",
    "print(\"Initializing clip and scene graph models...\")\n",
    "clip_model, clip_preprocess = clip.load('ViT-L/14@336px', device)\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-base\", return_special_tokens_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_imgs = []\n",
    "    input_questions = []\n",
    "    answers = []\n",
    "    scene_strs = []\n",
    "    question_ids = []\n",
    "    for elt in batch:\n",
    "        img, question, answer, scene_graph_str, question_id = elt\n",
    "        input_imgs.append(clip_preprocess(Image.fromarray(img)).unsqueeze(0))\n",
    "        input_questions.append(question)\n",
    "        answers.append(answer)\n",
    "        scene_strs.append(scene_graph_str)\n",
    "        question_ids.append(question_id)\n",
    "\n",
    "    image_input = torch.cat(input_imgs).to(device)\n",
    "    text_input = clip.tokenize(input_questions, truncate=True).to(device)\n",
    "    sg_input = clip.tokenize(scene_strs, truncate=True).to(device)\n",
    "\n",
    "    with torch.inference_mode(): # even faster than no_grad()\n",
    "        # image_features = torch.unsqueeze(clip_model.encode_image(image_input), dim=1)\n",
    "        # text_features = torch.unsqueeze(clip_model.encode_text(text_input), dim=1)\n",
    "        # sg_features = torch.unsqueeze(clip_model.encode_text(sg_input), dim=1)\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "        sg_features = clip_model.encode_text(sg_input)\n",
    "\n",
    "    # labels = t5_tokenizer(answers, padding=\"longest\", max_length=128, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    # labels[labels == t5_tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # print(\"answers: \", answers)\n",
    "    labels = t5_tokenizer(answers, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # print(\"labels: \", labels)\n",
    "\n",
    "    return image_features, text_features, sg_features, labels, question_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing train dataset...\n",
      "Stored annotations...\n",
      "Constructing question to answer dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443757/443757 [00:00<00:00, 2707980.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed question to answer dictionary...\n",
      "Processing scene graph json...\n",
      "Created img name to scene graph mapping...\n",
      "Stored questions...\n",
      "Created question ID to question mapping...\n",
      "train dataset contains 443757 annotations\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class VQA(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations, img_path, mode=\"train\"):\n",
    "        self.annotations = [] \n",
    "        self.questions = None\n",
    "        self.img_path = img_path\n",
    "        self.qid_to_question = {}\n",
    "        self.mode = mode\n",
    "\n",
    "        self.img_to_scene_graph = {}\n",
    "        self.question_to_answer = {}\n",
    "\n",
    "        print(f\"Constructing {self.mode} dataset...\")\n",
    "        with open(annotations[f\"{self.mode}_annotations\"]) as f:\n",
    "            all_annotations = json.load(f)['annotations']\n",
    "            print(\"Stored annotations...\")\n",
    "\n",
    "            self.annotations = all_annotations\n",
    "\n",
    "        self.construct_question_to_answer_dict()\n",
    "\n",
    "        with open(annotations[f\"{self.mode}_scene_graph_json\"]) as f:\n",
    "            print(\"Processing scene graph json...\")\n",
    "            for obj in f:\n",
    "                json_obj = json.loads(eval(eval(obj)))\n",
    "                if str(json_obj[\"input_img_path\"]) not in self.img_to_scene_graph:\n",
    "                    self.img_to_scene_graph[str(json_obj[\"input_img_path\"])] = str(json_obj[\"scene_graph_string\"])\n",
    "                else:\n",
    "                    print(\"Duplicate scene graphs exist!\")\n",
    "\n",
    "            print(\"Created img name to scene graph mapping...\")\n",
    "\n",
    "       \n",
    "        with open(annotations[f\"{self.mode}_questions\"]) as f:\n",
    "            self.questions = json.load(f)[\"questions\"]\n",
    "            print(f\"Stored questions...\")\n",
    "            for question in self.questions:\n",
    "                self.qid_to_question[question[\"question_id\"]] = question[\"question\"]\n",
    "            print(\"Created question ID to question mapping...\")\n",
    "            \n",
    "        \n",
    "\n",
    "        print(f\"{self.mode} dataset contains {len(self.annotations)} annotations\")\n",
    "\n",
    "    \n",
    "    def construct_question_to_answer_dict(self):\n",
    "        print(\"Constructing question to answer dictionary...\")\n",
    "        for annotation in tqdm(self.annotations):\n",
    "            self.question_to_answer[annotation[\"question_id\"]] = annotation\n",
    "            \n",
    "        print(\"Constructed question to answer dictionary...\")\n",
    "\n",
    "    def get_question_to_answer_dict(self):\n",
    "        return self.question_to_answer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        curr_annotation = self.annotations[idx]\n",
    "\n",
    "        question_id = curr_annotation[\"question_id\"]\n",
    "        image_id = curr_annotation[\"image_id\"]\n",
    "        answers = curr_annotation[\"answers\"]\n",
    "\n",
    "        # Don't need these for now but may need them for future ablations\n",
    "        # question_type = curr_annotation[\"question_type\"]\n",
    "        # answer_type = curr_annotation[\"answers\"]\n",
    "        # multiple_choice_answer = curr_annotation[\"multiple_choice_answer\"]\n",
    "\n",
    "        # VQA has multiple possible answers, can modify this later to use other answers\n",
    "        answer_choice = answers[0][\"answer\"]\n",
    "        \n",
    "        img = None\n",
    "\n",
    "        question = self.qid_to_question.get(question_id, None)\n",
    "\n",
    "        assert question is not None\n",
    "\n",
    "        padded_image_id = \"\".join((12 - len(str(image_id)))*[\"0\"]) + str(image_id)\n",
    "\n",
    "        img_path = os.path.join(self.img_path, f\"COCO_{self.mode}2014_{padded_image_id}.jpg\")\n",
    "\n",
    "        assert os.path.exists(img_path)\n",
    "            \n",
    "        img = np.asarray(Image.open(img_path))\n",
    "\n",
    "        scene_graph_str = self.img_to_scene_graph.get(img_path, None)\n",
    "       \n",
    "        assert scene_graph_str is not None\n",
    "        \n",
    "        return img, question, answer_choice, scene_graph_str, question_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "# we use the original val set as our test set since we have the annotations available (test annotations not available)\n",
    "# this will be useful for error analysis later on\n",
    "\n",
    "annotations = {\n",
    "    \"train_questions\": f\"{BASE_DIR}/vqa/v2_OpenEnded_mscoco_train2014_questions.json\",\n",
    "    \"test_questions\": f\"{BASE_DIR}/vqa/v2_OpenEnded_mscoco_test2014_questions.json\",\n",
    "    \"train_annotations\": f\"{BASE_DIR}/vqa/v2_mscoco_train2014_annotations.json\",\n",
    "    \"test_annotations\": f\"{BASE_DIR}/vqa/v2_mscoco_test2014_annotations.json\",\n",
    "    \"train_scene_graph_json\": f\"{BASE_DIR}/vqa/train_scene_graph.json\",\n",
    "    \"test_scene_graph_json\": f\"{BASE_DIR}/vqa/test_scene_graph.json\"\n",
    "}\n",
    "\n",
    "train_dataset = VQA(annotations, f\"{BASE_DIR}/vqa/train2014\", mode=\"train\")\n",
    "# test_dataset = VQA(annotations, f\"{BASE_DIR}/vqa/test2014\", mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing t5 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/443757 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m input_embeds_arr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(one_input_shape)\u001b[39m.\u001b[39mto(device) \n\u001b[1;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[1;32m     31\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m500\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     32\u001b[0m             t5\u001b[39m.\u001b[39msave_pretrained(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mBASE_DIR\u001b[39m}\u001b[39;00m\u001b[39m/vqa/model_ckpts/\u001b[39m\u001b[39m{\u001b[39;00mRUN_NAME\u001b[39m}\u001b[39;00m\u001b[39m_iter\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/bbki/kastanday/conda_envs/envs/v4_clip_preprocessing_yt1b/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/bbki/kastanday/conda_envs/envs/v4_clip_preprocessing_yt1b/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/bbki/kastanday/conda_envs/envs/v4_clip_preprocessing_yt1b/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/bbki/kastanday/conda_envs/envs/v4_clip_preprocessing_yt1b/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/bbki/kastanday/conda_envs/envs/v4_clip_preprocessing_yt1b/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn [6], line 88\u001b[0m, in \u001b[0;36mVQA.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     84\u001b[0m img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(Image\u001b[39m.\u001b[39mopen(img_path))\n\u001b[1;32m     86\u001b[0m scene_graph_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_to_scene_graph\u001b[39m.\u001b[39mget(img_path, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m---> 88\u001b[0m \u001b[39massert\u001b[39;00m scene_graph_str \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39mreturn\u001b[39;00m img, question, answer_choice, scene_graph_str, question_id\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(\"Initializing t5 model...\")\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=True).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "\n",
    "t5.train()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=t5.parameters(), lr=1e-4)\n",
    "\n",
    "iter_losses = []\n",
    "\n",
    "one_input_shape = [BATCH_SIZE, 768, 768]\n",
    "att_mask_shape = [BATCH_SIZE, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0, 0] = 1\n",
    "attn_mask_arr[0, 1] = 1\n",
    "\n",
    "if use_scene_graph:\n",
    "    attn_mask_arr[0, 2] = 1\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) \n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        if i % 500 == 0:\n",
    "            t5.save_pretrained(f\"{BASE_DIR}/vqa/model_ckpts/{RUN_NAME}_iter{i}\")\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        question_embed, img_embed, scene_graph_embed, labels, _ = batch\n",
    "\n",
    "        question_embed = question_embed.to(device)\n",
    "        img_embed = img_embed.to(device)\n",
    "        scene_graph_embed = scene_graph_embed.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        input_embeds_arr[0, 0, :] = img_embed\n",
    "        input_embeds_arr[0, 1, :] = question_embed\n",
    "        if use_scene_graph:\n",
    "            print(\"THIS SHOULD NOT BE HAPPENING\")\n",
    "            input_embeds_arr[0, 2, :] = scene_graph_embed\n",
    "\n",
    "        # labels = t5_tokenizer(\"hi my name is pranav\", return_tensors=\"pt\").input_ids.to(device)\n",
    "        # print(\"labels shaep: \", labels.shape)\n",
    "\n",
    "        outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        assert not torch.isnan(loss).any()\n",
    "\n",
    "        iter_losses.append(loss)\n",
    "\n",
    "        loss.sum().backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(t5.parameters(), 1e-2)\n",
    "\n",
    "        optimizer.step()\n",
    "        # print(\"✅ Successful training iteration\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Loss: \", loss.item())\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch} done.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5.save_pretrained(f\"{BASE_DIR}/vqa/model_ckpts/{RUN_NAME}_iter{19401}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(iter_losses))\n",
    "\n",
    "new_iter_losses = [str(t.item()) for t in iter_losses]\n",
    "\n",
    "# print(new_iter_losses)\n",
    "file1 = open('/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/iter_losses.txt', 'w')\n",
    "file1.writelines(\",\".join(new_iter_losses))\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "BATCH_SIZE = 1\n",
    "MODEL_STR = \"google/t5-v1_1-base\"\n",
    "# t5_eval = T5ForConditionalGeneration.from_pretrained(f\"{BASE_DIR}/vqa/model_ckpts/all_modalities_iter19401\", torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "t5_eval = T5ForConditionalGeneration.from_pretrained(f\"{BASE_DIR}/vqa/model_ckpts/all_modalities_iter5000\", torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_STR, return_special_tokens_mask=True)\n",
    "\n",
    "question_to_answer = test_dataset.get_question_to_answer_dict()\n",
    "\n",
    "t5_eval.eval()\n",
    "\n",
    "# TOOD: add in val dataset here. and check check for if use_scenegraph\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "iter_accuracies = []\n",
    "\n",
    "one_input_shape = [BATCH_SIZE, 768, 768]\n",
    "att_mask_shape = [BATCH_SIZE, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0, 0] = 1\n",
    "attn_mask_arr[0, 1] = 1\n",
    "attn_mask_arr[0, 2] = 1\n",
    "\n",
    "for i, batch in enumerate(tqdm(test_loader)):\n",
    "    question_embed, img_embed, scene_graph_embed, labels, question_ids = batch\n",
    "\n",
    "    answers = []\n",
    "    for question_id in question_ids:\n",
    "        answers.append([answer_obj[\"answer\"] for answer_obj in question_to_answer[question_id][\"answers\"]])\n",
    "\n",
    "    # To view the image, question and possible answers, uncomment\n",
    "    # image_id = question_to_answer[question_id][\"image_id\"]\n",
    "    # padded_image_id = \"\".join((12 - len(str(image_id)))*[\"0\"]) + str(image_id)\n",
    "    # img_path = os.path.join(self.img_path, f\"COCO_{self.pseudo_mode}2014_{padded_image_id}.jpg\")\n",
    "    # print(img_path)\n",
    "    # print(\"question: \", test_dataset.qid_to_question[question_ids[0]])\n",
    "    # print(answers)\n",
    "\n",
    "    question_embed = question_embed.to(device)\n",
    "    img_embed = img_embed.to(device)\n",
    "    scene_graph_embed = scene_graph_embed.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    input_embeds_arr[0, 0, :] = img_embed\n",
    "    input_embeds_arr[0, 1, :] = question_embed\n",
    "    input_embeds_arr[0, 2, :] = scene_graph_embed\n",
    "\n",
    "    # input_embeds_arr = torch.cat((question_embed, img_embed, scene_graph_embed), dim=1)\n",
    "\n",
    "    # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "    # output_sequences = t5_eval.generate(inputs_embeds=input_embeds_arr,  attention_mask=attn_mask_arr, do_sample=False)\n",
    "    \n",
    "    # outputs = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "    # outputs = ['no']\n",
    "    if i % 100 == 0:\n",
    "        print(\"question: \", test_dataset.qid_to_question[question_ids[0]])\n",
    "        print(\"answers: \", answers)\n",
    "        print(\"outputs: \", outputs)\n",
    "\n",
    "    for j, output in enumerate(outputs):\n",
    "        curr_answers = answers[j]\n",
    "\n",
    "        # evaluation metric for VQA: https://visualqa.org/evaluation.html\n",
    "        iter_accuracies.append(min(curr_answers.count(str(output)) / 3.0, 1.0))\n",
    "\n",
    "print(\"Accuracy: \", sum(iter_accuracies)/len(iter_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(iter_accuracies)/len(iter_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('v4_clip_preprocessing_yt1b')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d3db6712dfc0a86429b8fb30f027b0a323b7f50feb5f633548a08dcfe308692"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
