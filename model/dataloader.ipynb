{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# DEBUGGING \n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"parallel_15\"\n",
    "REMOTE_WHISPER_FILE = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_whisper_output.jsonl'\n",
    "REMOTE_CLIP_DIR  = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_clip_output'\n",
    "REMOTE_SCENE_FILE = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_scene_output.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate clip\n",
    "import clip\n",
    "\n",
    "MODEL_SIZE = 'ViT-L/14@336px'  # Best models are (1st) ViT-L/14@336px and (2nd) ViT-L/14. I don't recommend going lower.  \n",
    "clip_instance, clip_preprocess = clip.load(MODEL_SIZE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, OPTForCausalLM\n",
    "t5 = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "t5_tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "learning_rate       = 1e-4  # also good: 3e-4\n",
    "optimizer = torch.optim.AdamW(params =  t5.parameters(), lr=learning_rate) # Typically, 1e-4 and 3e-4 work well for most problems\n",
    "\n",
    "# prompt = \"Hey, are you consciours? Can you talk to me?\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate\n",
    "# generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "# tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForCausalLM, RobertaTokenizer\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "t5 = RobertaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=False).to(device) # float16, True\n",
    "t5_tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config, AutoModelWithLMHead\n",
    "\n",
    "'''\n",
    "MODEL SELECTION\n",
    "\n",
    "T5 V1.1 --  https://huggingface.co/docs/transformers/model_doc/t5v1.1 && https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511\n",
    "small - base - large - 3b/xl - 11b/xxl\n",
    "\n",
    "OG: t5-small\n",
    "\n",
    "'google/t5-base-lm-adapt' # largest on my server (without float16)\n",
    "'google/t5-xl-lm-adapt'\n",
    "\n",
    "google/t5-v1_1-large\n",
    "'''\n",
    "\n",
    "# MODEL_SIZE = \"t5-base\"\n",
    "MODEL_NAME = \"google/t5-v1_1-base\"\n",
    "# MODEL_NAME = \"google/t5-base-lm-adapt\"\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=False).to(device) # float16, True\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) â€” Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental.\n",
    "learning_rate       = 1e-4  # also good: 3e-4\n",
    "optimizer = torch.optim.AdamW(params =  t5.parameters(), lr=learning_rate) # Typically, 1e-4 and 3e-4 work well for most problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cockpit ML Debugger\n",
    "# https://cockpit.readthedocs.io/en/latest/examples/01_basic_fmnist.html\n",
    "from backpack import extend\n",
    "from cockpit import Cockpit, CockpitPlotter\n",
    "from cockpit.utils.configuration import configuration\n",
    "\n",
    "t5 = extend(t5)\n",
    "\n",
    "individual_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Create Cockpit and a plotter\n",
    "cockpit = Cockpit(t5.parameters(), quantities=configuration(\"full\"))\n",
    "plotter = CockpitPlotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' just for trying the debugger cockpit'''\n",
    "\n",
    "\n",
    "# Iterate through the batch\n",
    "clip_15 = os.listdir(REMOTE_CLIP_DIR)\n",
    "\n",
    "# Initialize embeddings\n",
    "one_input_shape = [1, 768, 768]\n",
    "att_mask_shape = [1, 768]\n",
    "embed_shape = [1, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "attn_mask_arr[0][2] = 1 # no clip\n",
    "\n",
    "t5.train()\n",
    "\n",
    "global_step = 0 \n",
    "\n",
    "with jsonlines.open(REMOTE_SCENE_FILE, 'r') as scene_reader:\n",
    "    # Zipping the scene graph with the clip + whisper embeddings\n",
    "    \n",
    "    # itr over videos\n",
    "    for scene_seg_list, clip_npz_path in tqdm.tqdm(zip(scene_reader, glob.glob(os.path.join(REMOTE_CLIP_DIR, '*'), recursive = True))):\n",
    "        try:\n",
    "            np_loaded = np.load(clip_npz_path, allow_pickle=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load compressed numpy: {e}\")\n",
    "            continue\n",
    "        object_list_of_str = []\n",
    "        scene_seg_list = json.loads(scene_seg_list)\n",
    "        \n",
    "        # iterate over segments\n",
    "        for segment_index in range(np_loaded['arr_0'].item()['total_segments']):\n",
    "            # print(np_loaded[f'arr_{segment_index}'].item()['captions'])\n",
    "            frame_embedding       = np_loaded[f'arr_{segment_index}'].item()['frame_embeddings']\n",
    "            caption_embedding     = np_loaded[f'arr_{segment_index}'].item()['text_caption_embeddings']\n",
    "            whisper_text_captions = np_loaded[f'arr_{segment_index}'].item()['captions']\n",
    "            \n",
    "            frame_embedding       = torch.from_numpy(frame_embedding.reshape((768,))).to(device)\n",
    "            caption_embedding     = torch.from_numpy(caption_embedding).to(device)\n",
    "\n",
    "            # Update embedding array\n",
    "            input_embeds_arr[0][0] = frame_embedding\n",
    "            input_embeds_arr[0][1] = caption_embedding\n",
    "            \n",
    "            # print(\"Input shapes:\")\n",
    "            # print(caption_embedding)\n",
    "            print(frame_embedding)\n",
    "            labels = t5_tokenizer(whisper_text_captions, return_tensors=\"pt\").input_ids.to(device)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "            outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            print(\"loss\")\n",
    "            print(loss)\n",
    "            logits = outputs[1]\n",
    "            # print(\"logits\")\n",
    "            # print(logits)\n",
    "            individual_losses = individual_loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            # print(\"individual_losses\")\n",
    "            # print(individual_losses)\n",
    "            # loss = \n",
    "            for item in outputs:\n",
    "                print(item)\n",
    "            ''' backwards pass '''\n",
    "            # loss.sum().backward()\n",
    "            # backward pass\n",
    "            with cockpit(\n",
    "                global_step,\n",
    "                info={\n",
    "                    \"batch_size\": one_input_shape[0],\n",
    "                    \"individual_losses\": individual_losses,\n",
    "                    \"loss\": loss,\n",
    "                    \"optimizer\": optimizer,\n",
    "                },\n",
    "            ):\n",
    "                loss.backward(create_graph=cockpit.create_graph(global_step))\n",
    "            \n",
    "            # optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            print(f\"step: {global_step}\")\n",
    "            plotter.plot(cockpit)\n",
    "            \n",
    "            print(\"Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\")\n",
    "            print(loss)\n",
    "            break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaConfig\n",
    "# OPTConfig.from_pretrained(\"roberta-base\")\n",
    "from transformers import OPTConfig\n",
    "OPTConfig.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "torch.Size([1, 2048, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "tokenized_labels\n",
      "tensor[1, 27] i64 xâˆˆ[2, 37588] Î¼=1.026e+04 Ïƒ=1.220e+04 cuda:0\n",
      "labels\n",
      "tensor[1, 2048] i64 xâˆˆ[-100, -100] Î¼=-100.000 Ïƒ=0. cuda:0\n",
      "torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plotter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m global_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep: \u001b[39m\u001b[39m{\u001b[39;00mglobal_step\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m plotter\u001b[39m.\u001b[39mplot(cockpit)\n\u001b[1;32m     82\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss ðŸ‘‡ðŸ‘‡ðŸ‘‡\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[39mprint\u001b[39m(loss)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plotter' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "''' More complete '''\n",
    "\n",
    "# Initialize embeddings\n",
    "# one_input_shape = [1, 768, 768]\n",
    "# att_mask_shape = [1, 768]\n",
    "# embed_shape = [1, 768]\n",
    "one_input_shape = [1, 2048, 768]  # torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)\n",
    "att_mask_shape = [1, 2048]        # \n",
    "embed_shape = [1, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape, device=device)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape, device=device)\n",
    "labels    = torch.zeros(att_mask_shape, dtype=int, device=device)\n",
    "print(labels.dtype)\n",
    "labels[:len(labels)] = -100\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "attn_mask_arr[0][2] = 1 # no clip\n",
    "\n",
    "t5.train()\n",
    "\n",
    "global_step = 0 \n",
    "\n",
    "with jsonlines.open(REMOTE_SCENE_FILE, 'r') as scene_reader:\n",
    "    # Zipping the scene graph with the clip + whisper embeddings\n",
    "    # itr over videos\n",
    "    for scene_seg_list, clip_npz_path in tqdm.tqdm(zip(scene_reader, glob.glob(os.path.join(REMOTE_CLIP_DIR, '*'), recursive = True))):\n",
    "        try:\n",
    "            np_loaded = np.load(clip_npz_path, allow_pickle=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load compressed numpy: {e}\")\n",
    "            continue\n",
    "        object_list_of_str = []\n",
    "        scene_seg_list = json.loads(scene_seg_list)\n",
    "        \n",
    "        # iterate over segments\n",
    "        for segment_index in range(np_loaded['arr_0'].item()['total_segments']):\n",
    "            frame_embedding       = np_loaded[f'arr_{segment_index}'].item()['frame_embeddings']\n",
    "            caption_embedding     = np_loaded[f'arr_{segment_index}'].item()['text_caption_embeddings']\n",
    "            whisper_text_captions = np_loaded[f'arr_{segment_index}'].item()['captions']\n",
    "            \n",
    "            frame_embedding       = torch.from_numpy(frame_embedding.reshape((768,))).to(device)\n",
    "            caption_embedding     = torch.from_numpy(caption_embedding).to(device)\n",
    "\n",
    "            scene_caption = scene_seg_list[segment_index]\n",
    "            scene_caption = clip.tokenize(scene_caption).to(device)\n",
    "            with torch.inference_mode(): # even faster than no_grad()\n",
    "                scene_embedding = clip_instance.encode_text(scene_caption)\n",
    "            scene_embedding = scene_embedding.reshape((768,))\n",
    "\n",
    "            # Update embedding array\n",
    "            input_embeds_arr[0][0] = frame_embedding\n",
    "            input_embeds_arr[0][1] = caption_embedding\n",
    "            input_embeds_arr[0][2] = scene_embedding\n",
    "            \n",
    "            print(\"Input shapes:\")\n",
    "            print(input_embeds_arr.shape)\n",
    "            print(scene_embedding.shape)\n",
    "            print(caption_embedding.shape)\n",
    "            print(frame_embedding.shape)\n",
    "            tokenized_labels = t5_tokenizer(whisper_text_captions, return_tensors=\"pt\").input_ids.to(device)\n",
    "            print(\"tokenized_labels\")\n",
    "            print(tokenized_labels)\n",
    "            print(\"labels\")\n",
    "            print(labels)\n",
    "            print(labels.dtype)\n",
    "            labels[0][0:len(tokenized_labels[0])] = tokenized_labels[0]\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True)\n",
    "            outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            ''' backwards pass '''\n",
    "            loss.sum().backward()\n",
    "            # optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            print(f\"step: {global_step}\")\n",
    "            \n",
    "            print(\"Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\")\n",
    "            print(loss)\n",
    "            break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5.save_pretrained(\"BIG_PENIS_PREVAILS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0][0:27]\n",
    "# len(tokenized_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Inference with custom model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import OPTForCausalLM\n",
    "\n",
    "MODEL_VERSION_NAME   = \"opt_yt_pretrain_adamW_1e-3_iter69000\"\n",
    "TOKENIZER_NAME       = \"facebook/opt-125m\"\n",
    "\n",
    "# BASE_DIR            = '/scratch/bbki/kastanday/whisper'\n",
    "BASE_DIR            = '/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train'\n",
    "MODEL_SAVE_PATH     = f'{BASE_DIR}/MODEL_CHECKPOINTS/{MODEL_VERSION_NAME}'\n",
    "\n",
    "model     = OPTForCausalLM.from_pretrained(MODEL_SAVE_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>IN THIS VIDEO WE WILL A A A A A A A A A A A A A A\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"Hey guys welcome back to the channel, this time\"\n",
    "# PROMPT = \"I REALLY LIKE EATING ASS\"\n",
    "# PROMPT = \"TODAY WE ARE GOING TO\"\n",
    "PROMPT = \"IN THIS VIDEO WE WILL\"\n",
    "# PROMPT = \"THE MOST COMMON WORD IN THE ENGLISH LANGUAGE\"\n",
    "\n",
    "\n",
    "# inference\n",
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying GPT-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT2Config.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1123 2 3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_caption = '1123 2 3 4 5 6 7'.split(\" \")\n",
    "import itertools\n",
    "first_half_of_whisper_caption  = list(itertools.islice(scene_caption, 0, len(scene_caption)//2))\n",
    "second_half_of_whisper_caption = list(itertools.islice(scene_caption, len(scene_caption)//2, None))\n",
    "\" \".join(first_half_of_whisper_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '5', '6', '7']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_half_of_whisper_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1123', '2', '3'], ['4', '5', '6'], ['7']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(scene_caption)//2\n",
    "parts = [scene_caption[i:i+n] for i in range(0, len(scene_caption), n)]\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_has_everything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b69eb772e953fa94c7e6a13e20e5676ab72e6ff0f5efef62f5cddb3889daa1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
