{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Transforms to Video datasets\n",
    "\n",
    "Good demo here (using Kinetics):\n",
    "https://pytorch.org/vision/stable/auto_examples/plot_video_api.html#sphx-glr-auto-examples-plot-video-api-py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/Custom_PyTorch_Dataset.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/Custom_PyTorch_Dataset.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/Custom_PyTorch_Dataset.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/Custom_PyTorch_Dataset.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m \u001b[39mimport\u001b[39;00m io, transform\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/Custom_PyTorch_Dataset.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/Custom_PyTorch_Dataset.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "torch.manual_seed(69)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPT_Dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset transformations (using Compose)\n",
    "\n",
    "### Transformations used in X-CLIP\n",
    "\n",
    "We could probably steal their implementation, too: https://github.com/microsoft/VideoX/blob/master/X-CLIP/datasets/rand_augment.py\n",
    "\n",
    "```python\n",
    "_RAND_CHOICE_WEIGHTS_0 = {\n",
    "    \"Rotate\": 0.3,\n",
    "    \"ShearX\": 0.2,\n",
    "    \"ShearY\": 0.2,\n",
    "    \"TranslateXRel\": 0.1,\n",
    "    \"TranslateYRel\": 0.1,\n",
    "    \"Color\": 0.025,\n",
    "    \"Sharpness\": 0.025,\n",
    "    \"AutoContrast\": 0.025,\n",
    "    \"Solarize\": 0.005,\n",
    "    \"SolarizeAdd\": 0.005,\n",
    "    \"Contrast\": 0.005,\n",
    "    \"Brightness\": 0.005,\n",
    "    \"Equalize\": 0.005,\n",
    "    \"Posterize\": 0,\n",
    "    \"Invert\": 0,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchscript (torch.jit.script) an optimizing JIT runtime compiler for PyTorch. \n",
    "# Compiled to C++, faster. I've read data augmentation is CPU-intensive, so this might help.\n",
    "transforms = torch.nn.Sequential(\n",
    "    transforms.CenterCrop(10),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    CenterCrop(size=(10, 10))\n",
       "    PILToTensor()\n",
       "    ConvertImageDtype()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "video_transforms = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = VPT_Dataset(\"./dataset\", epoch_size=None, transform=video_transforms)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=12)\n",
    "data = {\"video\": [], 'start': [], 'end': [], 'tensorsize': []}\n",
    "for batch in loader:\n",
    "    for i in range(len(batch['path'])):\n",
    "        data['video'].append(batch['path'][i])\n",
    "        data['start'].append(batch['start'][i].item())\n",
    "        data['end'].append(batch['end'][i].item())\n",
    "        data['tensorsize'].append(batch['video'][i].size())\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0497ffde1df318912604fec7dc437cf596dddf2ca6f4de129c07bc0ec5975633"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
