{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping for real run. \n",
    "\n",
    "Todo: \n",
    "* Define `max_source_length` and `max_target_length` for the model (otherwise truncated).\n",
    "padding token should be replaced with -100, which is the 'ignore_index' of `CrossEntorpyLoss` in PT and TF. For Flax, use `decoder_attention_mask`. \n",
    "Attention_mask. ensures madding tokens of inputs are ignored. \n",
    "\n",
    "* Install apex. \"model will automatically use apex.normalization.FusedRMSNorm instead of T5LayerNorm.\" The former uses an optimized fused kernel which is several times faster than the latter.\n",
    "\n",
    "A note on model sizes: \n",
    "T5-11B (original, not v1.1) weights in float32 are 45.2GB. \n",
    "See this post for using huggingface endpoints on SINGLE GPU for cheap inference: https://www.philschmid.de/deploy-t5-11b\n",
    "Uses mixed precision and sharding, and LLM.int8(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config, AutoModelWithLMHead\n",
    "import accelerate\n",
    "import wandb\n",
    "# !wandb login  -- reactivate later\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODEL SELECTION\n",
    "\n",
    "T5 V1.1 --  https://huggingface.co/docs/transformers/model_doc/t5v1.1 && https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511\n",
    "small - base - large - 3b/xl - 11b/xxl\n",
    "\n",
    "OG: t5-small\n",
    "\n",
    "'google/t5-base-lm-adapt' # largest on my server (without float16)\n",
    "'google/t5-xl-lm-adapt'\n",
    "'''\n",
    "\n",
    "# MODEL_SIZE = \"t5-base\"\n",
    "MODEL_NAME = \"google/t5-small-lm-adapt\"\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) — Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MODEL QUANTIZATION: fp16 '''\n",
    "\n",
    "# XL worked on my server (tested both CPU and GPU).\n",
    "# MODEL_NAME = \"google/t5-base-lm-adapt\"\n",
    "# t5 = AutoModelWithLMHead.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) — Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental.\n",
    "\n",
    "''' int8 -- my GPU doesn't support it.. '''\n",
    "# t5 = AutoModelWithLMHead.from_pretrained(MODEL_NAME, load_in_8bit=True, device_map='auto', low_cpu_mem_usage=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' PREP EMBEDDING INPUTS '''\n",
    "# shape = (batch_size, 'words', embedding_dim) -- here 'words' == each of our embeddings, like clip and language.\n",
    "# one_input_shape = [6, 512, 512]\n",
    "one_input_shape = [1, 512, 512]\n",
    "\n",
    "decoder_input_embeds_arr = np.random.rand( *one_input_shape ).astype(np.float32) # need fp32\n",
    "decoder_input_embeds_arr = torch.from_numpy(decoder_input_embeds_arr).to(device)\n",
    "input_embeds_arr = np.random.rand( *one_input_shape ).astype(np.float32)\n",
    "input_embeds_arr = torch.from_numpy(input_embeds_arr).to(device)\n",
    "attn_mask_arr = np.ones( one_input_shape )\n",
    "attn_mask_arr = torch.from_numpy(attn_mask_arr).to(device)\n",
    "\n",
    "print(decoder_input_embeds_arr.shape)\n",
    "print(input_embeds_arr.shape)\n",
    "print(attn_mask_arr.shape)\n",
    "\n",
    "''' Decoder gets the tokenized caption. Shape is (batch_size, max_caption_length). Use padding to make it fit. '''\n",
    "# WORKING example, but easier with numpy.\n",
    "# import torch.nn.functional as F\n",
    "# decoder_input_ids = tokenizer(\"This is the target output sentence, aka the video caption. I like tacos because they are so delicious.\", return_tensors=\"pt\").input_ids.to(device)\n",
    "# decoder_input_ids = F.pad(decoder_input_ids, (0, (512-decoder_input_ids.shape[1])), value=tokenizer.pad_token_id)\n",
    "# print(decoder_input_ids.shape)\n",
    "# decoder_input_ids\n",
    "decoder_input_ids = torch.from_numpy(np.random.randint(1, 10_000, size=(one_input_shape[0], one_input_shape[2]))).to(device)\n",
    "print(decoder_input_ids.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function\n",
    "\n",
    "T5 forward() docs: https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5ForConditionalGeneration.forward\n",
    "\n",
    "Todo: investigate difference between decoder `decoder_input_ids` and `lm_labels`.\n",
    "For example: \n",
    "```\n",
    "outputs = t5(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
    "```\n",
    "\n",
    "I think `loss.sum()` is for multi-iteration loss. I was inadverdently using it 6 batches.\n",
    "https://discuss.pytorch.org/t/loss-backward-raises-error-grad-can-be-implicitly-created-only-for-scalar-outputs/12152 \n",
    "loss.backward() # T5 RuntimeError: grad can be implicitly created only for scalar outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5.train()\n",
    "# outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_input_ids=decoder_input_ids, decoder_attention_mask=attn_mask_arr)\n",
    "loss = outputs[0]\n",
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' backwards pass '''\n",
    "optimizer = torch.optim.Adam(params =  t5.parameters(), lr=1e-4)\n",
    "optimizer.zero_grad()\n",
    "loss.sum().backward()\n",
    "optimizer.step()\n",
    "print(\"✅ Successful training iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' minimal train '''\n",
    "t5.train()\n",
    "\n",
    "loss = outputs[0]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=1e-4)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
    "# The model is put into train mode and then we enumerate over the training loader and passed to the defined network \n",
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%10 == 0:\n",
    "            wandb.log({\"Training Loss\": loss.item()})\n",
    "\n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # xm.optimizer_step(optimizer)  # FOR TPU\n",
    "        # xm.mark_step()                # FOR TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # WandB – Initialize a new run\n",
    "    wandb.init(project=\"transformers_tutorials_summarization\")\n",
    "\n",
    "    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
    "    # Defining some key variables that will be used later on in the training  \n",
    "    config = wandb.config          # Initialize config\n",
    "    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
    "    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
    "    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n",
    "    config.VAL_EPOCHS = 1 \n",
    "    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "    config.SEED = 42               # random seed (default: 42)\n",
    "    config.MAX_LEN = 512\n",
    "    config.SUMMARY_LEN = 150 \n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(config.SEED) # pytorch random seed\n",
    "    np.random.seed(config.SEED) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    \n",
    "\n",
    "    # Importing and Pre-Processing the domain data\n",
    "    # Selecting the needed columns only. \n",
    "    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
    "    df = pd.read_csv('./data/news_summary.csv',encoding='latin-1')\n",
    "    df = df[['text','ctext']]\n",
    "    df.ctext = 'summarize: ' + df.ctext\n",
    "    print(df.head())\n",
    "\n",
    "    \n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
    "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    print(\"FULL Dataset: {}\".format(df.shape))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        'batch_size': config.TRAIN_BATCH_SIZE,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    val_params = {\n",
    "        'batch_size': config.VALID_BATCH_SIZE,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "    \n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    # Log metrics with wandb\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    # Training loop\n",
    "    print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "    for epoch in range(config.TRAIN_EPOCHS):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "\n",
    "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "    # Saving the dataframe as predictions.csv\n",
    "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "    for epoch in range(config.VAL_EPOCHS):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv('./models/predictions.csv')\n",
    "        print('Output Files generated for review')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08d608f700504c6b03e88c868d0cc9b143978899209b13a888b26c423352d24d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
