{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Flan T5\n",
    "\n",
    "Google just released new SOTA LLMs on huggingface (better than OPT and Bloom). \n",
    "It's instruction-finetuned via reinforcement learning. This might be the best open-source LLM for our system, and should replace OPT.\n",
    "\n",
    "* Paper: https://arxiv.org/abs/2210.11416\n",
    "* huggingface: https://huggingface.co/docs/transformers/model_doc/flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/flan-t5-large were not used when initializing T5EncoderModel: ['decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'lm_head.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0033,  0.0014,  0.0020,  ...,  0.0083, -0.0006,  0.0019],\n",
       "         [-0.0797,  0.0751,  0.0628,  ..., -0.0573,  0.1196,  0.0057]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5EncoderModel.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"One\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model(input_ids=input_ids, )\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<list_iterator object at 0x7f088e676b00>\n",
      "hi\n",
      "be\n",
      "<list_iterator object at 0x7f088e5ccfa0>\n",
      "oh\n",
      "my\n"
     ]
    }
   ],
   "source": [
    "import more_itertools\n",
    "files = ['hi', 'be', 'oh', 'my']\n",
    "batches = list(more_itertools.divide(2, files))\n",
    "batches\n",
    "for b in batches:\n",
    "  print(b)\n",
    "  for iner in b:\n",
    "    print(iner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[1, 2] i64 μ=278.000 σ=391.737 [[555, 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor[1, 2, 1024] f16 n=2048 x∈[-0.254, 0.297] μ=0.000 σ=0.043 grad ToCopyBackward0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "print(input_ids)\n",
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage_ssd/whisper_results_parallel_15 loaded successfully.\n",
      "Dataset(path='/mnt/storage_ssd/whisper_results_parallel_15', tensors=['caption', 'segment_metadata', 'video_filename', 'video_filepath'])\n",
      "\n",
      "      tensor        htype     shape     dtype  compression\n",
      "     -------       -------   -------   -------  ------- \n",
      "     caption        text    (5346, 1)    str     None   \n",
      " segment_metadata   json    (5346, 1)    str      lz4   \n",
      "  video_filename    text    (5346, 1)    str     None   \n",
      "  video_filepath    text    (5346, 1)    str     None   \n"
     ]
    }
   ],
   "source": [
    "import deeplake as dl\n",
    "BATCH_NAME          = 'parallel_15'\n",
    "# WHISPER_RESULTS_DATASET_PATH        = f'/mnt/storage_ssd/v1_whisper_results_{BATCH_NAME}'\n",
    "WHISPER_RESULTS_DATASET_PATH        = f'/mnt/storage_ssd/whisper_results_{BATCH_NAME}'\n",
    "\n",
    "ds = dl.load(WHISPER_RESULTS_DATASET_PATH)\n",
    "ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38,\n",
       " 30,\n",
       " 33,\n",
       " 34,\n",
       " 31,\n",
       " 30,\n",
       " 34,\n",
       " 36,\n",
       " 31,\n",
       " 29,\n",
       " 40,\n",
       " 41,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 30,\n",
       " 31,\n",
       " 28,\n",
       " 34,\n",
       " 35,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 38,\n",
       " 30,\n",
       " 32,\n",
       " 39,\n",
       " 34,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 33,\n",
       " 37,\n",
       " 34,\n",
       " 27,\n",
       " 29,\n",
       " 35,\n",
       " 34,\n",
       " 40,\n",
       " 31,\n",
       " 33,\n",
       " 40,\n",
       " 31,\n",
       " 36,\n",
       " 35,\n",
       " 32,\n",
       " 34,\n",
       " 28,\n",
       " 29,\n",
       " 29,\n",
       " 27,\n",
       " 26,\n",
       " 36,\n",
       " 36,\n",
       " 34,\n",
       " 33,\n",
       " 32,\n",
       " 36,\n",
       " 36,\n",
       " 34,\n",
       " 35,\n",
       " 33,\n",
       " 32,\n",
       " 37,\n",
       " 31,\n",
       " 35,\n",
       " 37,\n",
       " 27,\n",
       " 25,\n",
       " 27,\n",
       " 23,\n",
       " 31,\n",
       " 31,\n",
       " 27,\n",
       " 29,\n",
       " 31,\n",
       " 28,\n",
       " 31,\n",
       " 30,\n",
       " 32,\n",
       " 29,\n",
       " 31,\n",
       " 33,\n",
       " 46,\n",
       " 30,\n",
       " 30,\n",
       " 40,\n",
       " 33,\n",
       " 42,\n",
       " 44,\n",
       " 38,\n",
       " 40,\n",
       " 37,\n",
       " 33,\n",
       " 32,\n",
       " 33,\n",
       " 35,\n",
       " 35,\n",
       " 37,\n",
       " 37,\n",
       " 36,\n",
       " 37,\n",
       " 44,\n",
       " 38,\n",
       " 43,\n",
       " 41,\n",
       " 43,\n",
       " 32,\n",
       " 40,\n",
       " 37,\n",
       " 32,\n",
       " 42,\n",
       " 34,\n",
       " 40,\n",
       " 35,\n",
       " 38,\n",
       " 31,\n",
       " 44,\n",
       " 31,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 29,\n",
       " 33,\n",
       " 31,\n",
       " 31,\n",
       " 27,\n",
       " 32,\n",
       " 31,\n",
       " 27,\n",
       " 32,\n",
       " 28,\n",
       " 29,\n",
       " 32,\n",
       " 29,\n",
       " 34,\n",
       " 40,\n",
       " 37,\n",
       " 32,\n",
       " 28,\n",
       " 27,\n",
       " 34,\n",
       " 28,\n",
       " 28,\n",
       " 31,\n",
       " 30,\n",
       " 31,\n",
       " 31,\n",
       " 33,\n",
       " 31,\n",
       " 34,\n",
       " 32,\n",
       " 31,\n",
       " 33,\n",
       " 36,\n",
       " 41,\n",
       " 27,\n",
       " 28,\n",
       " 30,\n",
       " 34,\n",
       " 29,\n",
       " 31,\n",
       " 25,\n",
       " 31,\n",
       " 32,\n",
       " 29,\n",
       " 25,\n",
       " 27,\n",
       " 32,\n",
       " 24,\n",
       " 30,\n",
       " 30,\n",
       " 29,\n",
       " 28,\n",
       " 24,\n",
       " 33,\n",
       " 36,\n",
       " 24,\n",
       " 28,\n",
       " 26,\n",
       " 31,\n",
       " 26,\n",
       " 30,\n",
       " 31,\n",
       " 28,\n",
       " 27,\n",
       " 26,\n",
       " 24,\n",
       " 34,\n",
       " 26,\n",
       " 27,\n",
       " 29,\n",
       " 34,\n",
       " 40,\n",
       " 38,\n",
       " 30,\n",
       " 33,\n",
       " 25,\n",
       " 30,\n",
       " 32,\n",
       " 25,\n",
       " 34,\n",
       " 31,\n",
       " 30,\n",
       " 32,\n",
       " 33,\n",
       " 27,\n",
       " 33,\n",
       " 26,\n",
       " 32,\n",
       " 31,\n",
       " 32,\n",
       " 29,\n",
       " 26,\n",
       " 30,\n",
       " 29,\n",
       " 29,\n",
       " 27,\n",
       " 31,\n",
       " 25,\n",
       " 28,\n",
       " 29,\n",
       " 31,\n",
       " 29,\n",
       " 29,\n",
       " 33,\n",
       " 30,\n",
       " 28,\n",
       " 34,\n",
       " 30,\n",
       " 30,\n",
       " 23,\n",
       " 25,\n",
       " 26,\n",
       " 34,\n",
       " 30,\n",
       " 31,\n",
       " 29,\n",
       " 26,\n",
       " 23,\n",
       " 30,\n",
       " 25,\n",
       " 27,\n",
       " 36,\n",
       " 31,\n",
       " 24,\n",
       " 37,\n",
       " 36,\n",
       " 28,\n",
       " 37,\n",
       " 29,\n",
       " 26,\n",
       " 30,\n",
       " 25,\n",
       " 23,\n",
       " 30,\n",
       " 27,\n",
       " 30,\n",
       " 30,\n",
       " 29,\n",
       " 27,\n",
       " 21,\n",
       " 27,\n",
       " 25,\n",
       " 31,\n",
       " 26,\n",
       " 28,\n",
       " 31,\n",
       " 23,\n",
       " 28,\n",
       " 29,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 35,\n",
       " 24,\n",
       " 36,\n",
       " 36,\n",
       " 38,\n",
       " 29,\n",
       " 31,\n",
       " 36,\n",
       " 36,\n",
       " 23,\n",
       " 32,\n",
       " 30,\n",
       " 36,\n",
       " 44,\n",
       " 28,\n",
       " 31,\n",
       " 34,\n",
       " 30,\n",
       " 24,\n",
       " 30,\n",
       " 28,\n",
       " 28,\n",
       " 24,\n",
       " 39,\n",
       " 27,\n",
       " 32,\n",
       " 35,\n",
       " 28,\n",
       " 34,\n",
       " 34,\n",
       " 35,\n",
       " 31,\n",
       " 30,\n",
       " 31,\n",
       " 37,\n",
       " 31,\n",
       " 27,\n",
       " 30,\n",
       " 30,\n",
       " 33,\n",
       " 32,\n",
       " 42,\n",
       " 42,\n",
       " 33,\n",
       " 34,\n",
       " 28,\n",
       " 30,\n",
       " 28,\n",
       " 39,\n",
       " 32,\n",
       " 40,\n",
       " 35,\n",
       " 31,\n",
       " 36,\n",
       " 43,\n",
       " 44,\n",
       " 38,\n",
       " 33,\n",
       " 43,\n",
       " 45,\n",
       " 37,\n",
       " 37,\n",
       " 42,\n",
       " 41,\n",
       " 36,\n",
       " 33,\n",
       " 48,\n",
       " 40,\n",
       " 31,\n",
       " 29,\n",
       " 33,\n",
       " 33,\n",
       " 29,\n",
       " 34,\n",
       " 39,\n",
       " 34,\n",
       " 36,\n",
       " 37,\n",
       " 35,\n",
       " 38,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 32,\n",
       " 29,\n",
       " 25,\n",
       " 32,\n",
       " 29,\n",
       " 29,\n",
       " 33,\n",
       " 28,\n",
       " 35,\n",
       " 29,\n",
       " 33,\n",
       " 25,\n",
       " 35,\n",
       " 47,\n",
       " 31,\n",
       " 28,\n",
       " 37,\n",
       " 34,\n",
       " 33,\n",
       " 37,\n",
       " 38,\n",
       " 29,\n",
       " 36,\n",
       " 33,\n",
       " 34,\n",
       " 39,\n",
       " 34,\n",
       " 47,\n",
       " 32,\n",
       " 26,\n",
       " 29,\n",
       " 35,\n",
       " 51,\n",
       " 39,\n",
       " 38,\n",
       " 32,\n",
       " 37,\n",
       " 35,\n",
       " 30,\n",
       " 31,\n",
       " 41,\n",
       " 28,\n",
       " 38,\n",
       " 39,\n",
       " 47,\n",
       " 38,\n",
       " 32,\n",
       " 51,\n",
       " 35,\n",
       " 25,\n",
       " 41,\n",
       " 32,\n",
       " 29,\n",
       " 27,\n",
       " 26,\n",
       " 34,\n",
       " 29,\n",
       " 29,\n",
       " 40,\n",
       " 36,\n",
       " 29,\n",
       " 33,\n",
       " 34,\n",
       " 29,\n",
       " 34,\n",
       " 29,\n",
       " 29,\n",
       " 32,\n",
       " 30,\n",
       " 35,\n",
       " 33,\n",
       " 33,\n",
       " 39,\n",
       " 37,\n",
       " 33,\n",
       " 32,\n",
       " 27,\n",
       " 35,\n",
       " 36,\n",
       " 34,\n",
       " 40,\n",
       " 37,\n",
       " 38,\n",
       " 35,\n",
       " 29,\n",
       " 34,\n",
       " 37,\n",
       " 34,\n",
       " 34,\n",
       " 30,\n",
       " 28,\n",
       " 33,\n",
       " 25,\n",
       " 33,\n",
       " 34,\n",
       " 39,\n",
       " 31,\n",
       " 36,\n",
       " 33,\n",
       " 36,\n",
       " 41,\n",
       " 33,\n",
       " 40,\n",
       " 26,\n",
       " 36,\n",
       " 37,\n",
       " 30,\n",
       " 32,\n",
       " 31,\n",
       " 35,\n",
       " 28,\n",
       " 32,\n",
       " 31,\n",
       " 38,\n",
       " 34,\n",
       " 35,\n",
       " 33,\n",
       " 37,\n",
       " 45,\n",
       " 36,\n",
       " 30,\n",
       " 44,\n",
       " 31,\n",
       " 36,\n",
       " 42,\n",
       " 38,\n",
       " 42,\n",
       " 30,\n",
       " 25,\n",
       " 40,\n",
       " 36,\n",
       " 30,\n",
       " 32,\n",
       " 31,\n",
       " 40,\n",
       " 28,\n",
       " 35,\n",
       " 46,\n",
       " 36,\n",
       " 33,\n",
       " 30,\n",
       " 38,\n",
       " 36,\n",
       " 28,\n",
       " 26,\n",
       " 32,\n",
       " 42,\n",
       " 30,\n",
       " 29,\n",
       " 38,\n",
       " 31,\n",
       " 26,\n",
       " 28,\n",
       " 30,\n",
       " 31,\n",
       " 29,\n",
       " 41,\n",
       " 41,\n",
       " 28,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 29,\n",
       " 39,\n",
       " 32,\n",
       " 36,\n",
       " 33,\n",
       " 29,\n",
       " 33,\n",
       " 27,\n",
       " 39,\n",
       " 38,\n",
       " 32,\n",
       " 34,\n",
       " 36,\n",
       " 33,\n",
       " 32,\n",
       " 34,\n",
       " 36,\n",
       " 31,\n",
       " 32,\n",
       " 35,\n",
       " 28,\n",
       " 37,\n",
       " 35,\n",
       " 32,\n",
       " 29,\n",
       " 33,\n",
       " 33,\n",
       " 28,\n",
       " 28,\n",
       " 31,\n",
       " 32,\n",
       " 37,\n",
       " 28,\n",
       " 34,\n",
       " 41,\n",
       " 40,\n",
       " 36,\n",
       " 39,\n",
       " 32,\n",
       " 39,\n",
       " 30,\n",
       " 38,\n",
       " 35,\n",
       " 40,\n",
       " 39,\n",
       " 33,\n",
       " 33,\n",
       " 35,\n",
       " 42,\n",
       " 29,\n",
       " 43,\n",
       " 38,\n",
       " 35,\n",
       " 29,\n",
       " 32,\n",
       " 29,\n",
       " 38,\n",
       " 45,\n",
       " 31,\n",
       " 35,\n",
       " 38,\n",
       " 35,\n",
       " 32,\n",
       " 36,\n",
       " 36,\n",
       " 32,\n",
       " 39,\n",
       " 32,\n",
       " 29,\n",
       " 44,\n",
       " 44,\n",
       " 27,\n",
       " 28,\n",
       " 35,\n",
       " 34,\n",
       " 33,\n",
       " 42,\n",
       " 37,\n",
       " 36,\n",
       " 43,\n",
       " 40,\n",
       " 40,\n",
       " 37,\n",
       " 31,\n",
       " 34,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 34,\n",
       " 32,\n",
       " 30,\n",
       " 29,\n",
       " 29,\n",
       " 25,\n",
       " 28,\n",
       " 28,\n",
       " 30,\n",
       " 29,\n",
       " 35,\n",
       " 29,\n",
       " 29,\n",
       " 30,\n",
       " 38,\n",
       " 42,\n",
       " 36,\n",
       " 43,\n",
       " 39,\n",
       " 34,\n",
       " 41,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 30,\n",
       " 27,\n",
       " 32,\n",
       " 41,\n",
       " 32,\n",
       " 39,\n",
       " 30,\n",
       " 33,\n",
       " 32,\n",
       " 32,\n",
       " 37,\n",
       " 40,\n",
       " 28,\n",
       " 38,\n",
       " 30,\n",
       " 34,\n",
       " 41,\n",
       " 40,\n",
       " 42,\n",
       " 32,\n",
       " 39,\n",
       " 30,\n",
       " 38,\n",
       " 30,\n",
       " 32,\n",
       " 39,\n",
       " 39,\n",
       " 36,\n",
       " 42,\n",
       " 39,\n",
       " 29,\n",
       " 29,\n",
       " 31,\n",
       " 40,\n",
       " 29,\n",
       " 32,\n",
       " 31,\n",
       " 29,\n",
       " 39,\n",
       " 34,\n",
       " 37,\n",
       " 30,\n",
       " 25,\n",
       " 25,\n",
       " 33,\n",
       " 34,\n",
       " 29,\n",
       " 29,\n",
       " 24,\n",
       " 30,\n",
       " 28,\n",
       " 33,\n",
       " 25,\n",
       " 32,\n",
       " 35,\n",
       " 34,\n",
       " 33,\n",
       " 31,\n",
       " 29,\n",
       " 28,\n",
       " 31,\n",
       " 37,\n",
       " 26,\n",
       " 25,\n",
       " 24,\n",
       " 37,\n",
       " 30,\n",
       " 37,\n",
       " 34,\n",
       " 33,\n",
       " 32,\n",
       " 34,\n",
       " 29,\n",
       " 35,\n",
       " 38,\n",
       " 28,\n",
       " 33,\n",
       " 35,\n",
       " 32,\n",
       " 27,\n",
       " 30,\n",
       " 31,\n",
       " 30,\n",
       " 33,\n",
       " 31,\n",
       " 29,\n",
       " 26,\n",
       " 33,\n",
       " 34,\n",
       " 30,\n",
       " 33,\n",
       " 28,\n",
       " 33,\n",
       " 31,\n",
       " 33,\n",
       " 31,\n",
       " 33,\n",
       " 32,\n",
       " 29,\n",
       " 35,\n",
       " 26,\n",
       " 32,\n",
       " 30,\n",
       " 34,\n",
       " 30,\n",
       " 33,\n",
       " 29,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 30,\n",
       " 30,\n",
       " 31,\n",
       " 36,\n",
       " 31,\n",
       " 38,\n",
       " 36,\n",
       " 30,\n",
       " 33,\n",
       " 32,\n",
       " 35,\n",
       " 36,\n",
       " 28,\n",
       " 38,\n",
       " 26,\n",
       " 30,\n",
       " 27,\n",
       " 29,\n",
       " 31,\n",
       " 33,\n",
       " 35,\n",
       " 28,\n",
       " 27,\n",
       " 31,\n",
       " 27,\n",
       " 37,\n",
       " 32,\n",
       " 40,\n",
       " 32,\n",
       " 27,\n",
       " 27,\n",
       " 33,\n",
       " 34,\n",
       " 23,\n",
       " 27,\n",
       " 29,\n",
       " 35,\n",
       " 31,\n",
       " 32,\n",
       " 28,\n",
       " 34,\n",
       " 31,\n",
       " 47,\n",
       " 39,\n",
       " 24,\n",
       " 32,\n",
       " 27,\n",
       " 29,\n",
       " 33,\n",
       " 33,\n",
       " 34,\n",
       " 33,\n",
       " 29,\n",
       " 35,\n",
       " 34,\n",
       " 29,\n",
       " 31,\n",
       " 28,\n",
       " 31,\n",
       " 31,\n",
       " 33,\n",
       " 30,\n",
       " 29,\n",
       " 23,\n",
       " 30,\n",
       " 29,\n",
       " 35,\n",
       " 28,\n",
       " 40,\n",
       " 33,\n",
       " 30,\n",
       " 36,\n",
       " 30,\n",
       " 33,\n",
       " 23,\n",
       " 29,\n",
       " 33,\n",
       " 32,\n",
       " 30,\n",
       " 31,\n",
       " 36,\n",
       " 31,\n",
       " 35,\n",
       " 38,\n",
       " 34,\n",
       " 34,\n",
       " 31,\n",
       " 33,\n",
       " 38,\n",
       " 30,\n",
       " 29,\n",
       " 32,\n",
       " 33,\n",
       " 35,\n",
       " 27,\n",
       " 26,\n",
       " 25,\n",
       " 29,\n",
       " 28,\n",
       " 24,\n",
       " 31,\n",
       " 33,\n",
       " 30,\n",
       " 27,\n",
       " 31,\n",
       " 29,\n",
       " 30,\n",
       " 27,\n",
       " 27,\n",
       " 38,\n",
       " 35,\n",
       " 34,\n",
       " 34,\n",
       " 33,\n",
       " 36,\n",
       " 55,\n",
       " 52,\n",
       " 28,\n",
       " 36,\n",
       " 29,\n",
       " 32,\n",
       " 36,\n",
       " 36,\n",
       " 55,\n",
       " 32,\n",
       " 37,\n",
       " 33,\n",
       " 31,\n",
       " 29,\n",
       " 33,\n",
       " 44,\n",
       " 28,\n",
       " 36,\n",
       " 39,\n",
       " 34,\n",
       " 35,\n",
       " 31,\n",
       " 25,\n",
       " 29,\n",
       " 31,\n",
       " 32,\n",
       " 37,\n",
       " 28,\n",
       " 29,\n",
       " 32,\n",
       " 31,\n",
       " 36,\n",
       " 39,\n",
       " 27,\n",
       " 39,\n",
       " 30,\n",
       " 32,\n",
       " 26,\n",
       " 26,\n",
       " 24,\n",
       " 29,\n",
       " 29,\n",
       " 28,\n",
       " 37,\n",
       " 28,\n",
       " 34,\n",
       " 32,\n",
       " 30,\n",
       " 32,\n",
       " 32,\n",
       " 35,\n",
       " 33,\n",
       " 30,\n",
       " 38,\n",
       " 34,\n",
       " 29,\n",
       " 33,\n",
       " 32,\n",
       " 31,\n",
       " 34,\n",
       " 38,\n",
       " 33,\n",
       " 31,\n",
       " 26,\n",
       " 27,\n",
       " 29,\n",
       " 34,\n",
       " 33,\n",
       " 34,\n",
       " 32,\n",
       " 31,\n",
       " 34,\n",
       " 41,\n",
       " 31,\n",
       " 31,\n",
       " 34,\n",
       " 26,\n",
       " 39,\n",
       " 32,\n",
       " 29,\n",
       " 35,\n",
       " 33,\n",
       " 25,\n",
       " 35,\n",
       " 28,\n",
       " 26,\n",
       " 33,\n",
       " 36,\n",
       " 33,\n",
       " 34,\n",
       " 29,\n",
       " 39,\n",
       " 33,\n",
       " 30,\n",
       " 30,\n",
       " 29,\n",
       " 35,\n",
       " 29,\n",
       " 34,\n",
       " 32,\n",
       " 28,\n",
       " 30,\n",
       " 32,\n",
       " 33,\n",
       " 28,\n",
       " 30,\n",
       " 26,\n",
       " 34,\n",
       " 29,\n",
       " 35,\n",
       " 24,\n",
       " 30,\n",
       " 26,\n",
       " 29,\n",
       " 34,\n",
       " 33,\n",
       " 31,\n",
       " 44,\n",
       " 30,\n",
       " 31,\n",
       " 34,\n",
       " 30,\n",
       " 32,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 34,\n",
       " 29,\n",
       " 31,\n",
       " 34,\n",
       " 29,\n",
       " 39,\n",
       " 35,\n",
       " 37,\n",
       " 28,\n",
       " 32,\n",
       " 27,\n",
       " 32,\n",
       " 29,\n",
       " 27,\n",
       " 36,\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index = 16748-800\n",
    "all_tokenized = []\n",
    "for i, sample in enumerate(ds):\n",
    "  # if i < start_index:\n",
    "  #   continue\n",
    "  # print(sample.caption.data())\n",
    "  # print(sample.segment_metadata.data())\n",
    "  import traceback\n",
    "  try:\n",
    "    if sample.caption.data()['value']:\n",
    "      # print(i)\n",
    "      # print(sample.caption.data()['value'])\n",
    "      input_ids = tokenizer(\n",
    "          sample.caption.data()['value'], return_tensors=\"pt\"\n",
    "      ).input_ids\n",
    "      all_tokenized.append(input_ids.shape[1])\n",
    "  except Exception as e:\n",
    "    # print(f\"Error {e}\")\n",
    "    # print(traceback.print_exc())\n",
    "    break\n",
    "  # if i > start_index:\n",
    "  #   break\n",
    "all_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PIPETTE PUMP ONE TWENTY-TWO HUNDRED-MICROLITER PIPETTER ONE ONE HUNDRED-ONE THOUSAND-MICROLITER PIPETTER ONE EIGHT-CHANNEL TEN-ONE HUNDRED-MICROLITER'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[2672].caption.data()['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 57] i64 x∈[1, 28889] μ=6.584e+03 σ=7.789e+03"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\n",
    "          ds[1576].caption.data()['value'], return_tensors=\"pt\"\n",
    "      ).input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 32.398615787504674\n",
      "max 70\n",
      "argmax 2672\n",
      "min 20\n",
      "median 32.0\n",
      "std 4.776280983863671\n"
     ]
    }
   ],
   "source": [
    "all_tokenized\n",
    "\n",
    "# take mean of list all_tokenized\n",
    "import numpy as np\n",
    "print(\"mean\", np.mean(all_tokenized))\n",
    "# take max of list all_tokenized\n",
    "print(\"max\", np.max(all_tokenized))\n",
    "\n",
    "print(\"argmax\", np.argmax(all_tokenized))\n",
    "\n",
    "# take min of list all_tokenized\n",
    "print(\"min\", np.min(all_tokenized))\n",
    "\n",
    "# take median of list all_tokenized\n",
    "print(\"median\", np.median(all_tokenized))\n",
    "\n",
    "# take std of list all_tokenized\n",
    "print(\"std\", np.std(all_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m T5Tokenizer, T5EncoderModel\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgoogle/flan-t5-large\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m T5EncoderModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mgoogle/flan-t5-large\u001b[39;49m\u001b[39m\"\u001b[39;49m, device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16)\n\u001b[1;32m      5\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mStudies have been shown that owning a dog is good for you\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\u001b[39m.\u001b[39minput_ids  \u001b[39m# Batch size 1\u001b[39;00m\n\u001b[1;32m      8\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39minput_ids, )\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/v3_modern_torch/lib/python3.10/site-packages/transformers/modeling_utils.py:2230\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[1;32m   2228\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2229\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[0;32m-> 2230\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[1;32m   2232\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[39m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[1;32m   2237\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/v3_modern_torch/lib/python3.10/site-packages/transformers/modeling_utils.py:399\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_load_file(checkpoint_file)\n\u001b[1;32m    398\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    400\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    401\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/v3_modern_torch/lib/python3.10/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/v3_modern_torch/lib/python3.10/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/v3_modern_torch/lib/python3.10/site-packages/torch/serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1100\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1101\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1103\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/v3_modern_torch/lib/python3.10/site-packages/torch/serialization.py:1079\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1077\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1079\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39muntyped()\n\u001b[1;32m   1080\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m   1083\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1084\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5EncoderModel.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model(input_ids=input_ids, )\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "CONTEXT = \"{Example: A Two-Bit Gray Code Counter}Let's begin with a two-bit Gray code counter with no inputs.As we mentioned in Notes Set 2.1, a Gray code is a cycle over allbit patterns of a certain length in which consecutive patterns differin exactly one bit.For simplicity, our first few examples are based on counters anduse the internal stateof the FSM as the output values.  You should already knowhow to design combinational logic for the outputs if it were necessary.The inputs to a counter, if any, are typically limited to functionssuch as starting and stopping the counter, controlling the counting direction, and resetting the counter to a particular state.A fully-specified transition diagram for a two-bit Gray code counter appears below.With no inputs, the states simply form a loop, withthe counter moving from one state to the next each cycle.Each state in the diagram is marked with the internal state value S_1S_0 (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are always equal for this counter.Based on the transition diagram, we can fill in the K-maps for the next-state values S_1^+ and S_0^+ as shown to the right of thetransition diagram, then derive algebraic expressions in the usual way to obtainS_1^+=S_0 and S_0^+={{S_1}}.We then use the next-state logic to develop the implementationshown on the far right, completing our first counter design.\"\n",
    "PROMPT = \"Please answer this person's question accurately, clearly and concicely. Context: \"\n",
    "QUESTION = \"Question: What are the inputs and outputs of a Gray code counter? \"\n",
    "input_text = PROMPT + CONTEXT + QUESTION + \"Answer: \"\n",
    "\n",
    "article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n",
    "summary = \"Weiter Verhandlung in Syrien.\"\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "labels = tokenizer(text_target=summary, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=1024)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model(input_ids=input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3_modern_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84b0bb361bf95f708369aefede1d9770a5bab44518d07867f01f5b63015e6494"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
