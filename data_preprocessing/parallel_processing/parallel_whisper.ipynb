{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import argparse\n",
    "import jsonlines\n",
    "import json\n",
    "import time\n",
    "import deeplake as dl\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage_ssd/whisper_results_parallel_15 loaded successfully.\n",
      "Dataset(path='/mnt/storage_ssd/whisper_results_parallel_15', tensors=['caption', 'segment_metadata', 'video_filename', 'video_filepath'])\n",
      "\n",
      "      tensor        htype     shape      dtype  compression\n",
      "     -------       -------   -------    -------  ------- \n",
      "     caption        text    (17213, 1)    str     None   \n",
      " segment_metadata   json    (16748, 1)    str      lz4   \n",
      "  video_filename    text    (17213, 1)    str     None   \n",
      "  video_filepath    text    (17213, 1)    str     None   \n"
     ]
    }
   ],
   "source": [
    "WHISPER_RESULTS_DATASET_PATH = f'/mnt/storage_ssd/whisper_results_parallel_15'\n",
    "ds = dl.load(WHISPER_RESULTS_DATASET_PATH)\n",
    "ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The length of tensors in the dataset is different. The len(ds) returns the length of the smallest tensor in the dataset. If you want the length of the longest tensor in the dataset use ds.max_len.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'end': 5.017125,\n",
      " 'segment_index': 0,\n",
      " 'segment_word_list': [{'end': 1.3043749999999998,\n",
      "                        'start': 1.184,\n",
      "                        'word': 'HEY'},\n",
      "                       {'end': 1.6255, 'start': 1.3245, 'word': 'GUYS'},\n",
      "                       {'end': 1.7860624999999999,\n",
      "                        'start': 1.7058125,\n",
      "                        'word': \"IT'S\"},\n",
      "                       {'end': 2.026875, 'start': 1.806125, 'word': 'THUG'},\n",
      "                       {'end': 2.56875, 'start': 2.0670625, 'word': 'SNIPER'},\n",
      "                       {'end': 2.8697500000000002,\n",
      "                        'start': 2.749375,\n",
      "                        'word': 'AND'},\n",
      "                       {'end': 2.93, 'start': 2.889875, 'word': 'ME'},\n",
      "                       {'end': 3.7929375, 'start': 3.73275, 'word': 'AND'},\n",
      "                       {'end': 3.9535, 'start': 3.833125, 'word': 'MY'},\n",
      "                       {'end': 4.2344375, 'start': 3.9735625, 'word': 'BUDDY'},\n",
      "                       {'end': 4.354875000000001,\n",
      "                        'start': 4.274625,\n",
      "                        'word': 'HAVE'},\n",
      "                       {'end': 4.4953125, 'start': 4.3749375, 'word': 'BEEN'},\n",
      "                       {'end': 4.75625, 'start': 4.5154375, 'word': 'GETTING'},\n",
      "                       {'end': 4.8164375, 'start': 4.796375, 'word': 'A'},\n",
      "                       {'end': 5.017125, 'start': 4.856625, 'word': 'LOT'}],\n",
      " 'start': 1.184,\n",
      " 'total_segments': 24}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "for sample in ds:\n",
    "  pprint.pprint(sample.segment_metadata.data()['value'])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_DS_PATH = '/mnt/storage_ssd/v0_for_whisper_p15'\n",
    "DATASET_PATH        = '/mnt/storage_ssd/v0_for_whisper_parallel_15'\n",
    "BASE_DIR            = '/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/'\n",
    "# BASE_DIR            = '/mnt/storage_ssd/'\n",
    "BATCH_NAME          = 'parallel_15'\n",
    "REMOTE_WHISPER_FILE = f'{BASE_DIR}/{BATCH_NAME}_whisper_output.jsonl'\n",
    "REMOTE_CLIP_DIR     = f'{BASE_DIR}/{BATCH_NAME}_clip_output'\n",
    "REMOTE_SCENE_FILE   = f'{BASE_DIR}/{BATCH_NAME}_scene_output.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating/overwriting new dataset, sleeping 3 seconds to allow you to cancel.\n",
      "At Path: /mnt/storage_ssd/v0_for_whisper_p15\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "  ''' Create dataset '''\n",
    "  if False and os.path.exists(DATASET_PATH):\n",
    "    ds = dl.load(DATASET_PATH)\n",
    "    print(ds.summary())\n",
    "    all_stems = ds.video_stem.data()['value']\n",
    "    already_completed_stems = set(all_stems)\n",
    "    print(\"already completed: \", len(already_completed_stems))\n",
    "  else:\n",
    "    print(f\"Creating/overwriting new dataset, sleeping 3 seconds to allow you to cancel.\\nAt Path: {DATASET_PATH}\")  \n",
    "    # time.sleep(3)\n",
    "    ds = dl.empty(DATASET_PATH, overwrite=True)\n",
    "    with ds:\n",
    "      # ds.create_tensor('video_metadata', htype='json', sample_compression='lz4') # compression: https://docs.deeplake.ai/en/latest/Compressions.html\n",
    "      ds.create_tensor('video_name', htype='text', dtype=str, sample_compression=None)\n",
    "      ds.create_tensor('video_filepath', htype='text', dtype=str, sample_compression=None)\n",
    "      # ds.create_tensor('segment_frames', htype='image', dtype=np.uint8, sample_compression='jpg')\n",
    "      # ds.create_tensor('text_caption_embeddings', htype='image', dtype=np.float16, sample_compression='lz4') # CLIP produces FP16 embeddings.\n",
    "      # ds.create_tensor('frame_embeddings', htype='image', dtype=np.float16, sample_compression='lz4') # compression: https://docs.deeplake.ai/en/latest/Compressions.html\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Globbing paths from /mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/parallel_15/*\n",
      "/mnt/storage_ssd/v0_for_whisper_p15 loaded successfully.\n",
      "Adding to dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:20<00:00, 1229.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def add_raw_videos_to_dataset():\n",
    "  print(f\"Globbing paths from {os.path.join(BASE_DIR, BATCH_NAME, '*')}\")\n",
    "  raw_video_paths = glob.glob(os.path.join(BASE_DIR, BATCH_NAME, '*'), recursive = True)\n",
    "  ds = dl.load(DATASET_PATH)\n",
    "  print(\"Adding to dataset\")\n",
    "  counter = 0\n",
    "  with ds:\n",
    "    for video_path in tqdm.tqdm(raw_video_paths):\n",
    "      video_name = str(pathlib.Path(video_path).name)\n",
    "      # no groups \n",
    "      ds.append({\n",
    "        'video_name': video_name,\n",
    "        'video_filepath': video_path,\n",
    "      })\n",
    "      # one group per video\n",
    "      # ds.create_group(video_name)\n",
    "      # ds[video_name].create_tensor('video_metadata', htype='json', sample_compression='lz4') # compression: https://docs.deeplake.ai/en/latest/Compressions.html\n",
    "      # ds[video_name].create_tensor('video_name', htype='text', dtype=str, sample_compression=None)\n",
    "      # ds[video_name].append({\n",
    "      #   'video_name': video_name,\n",
    "      #   'video_metadata': metadata,\n",
    "      # })\n",
    "      # ds[video_name].video_name.append(video_name)\n",
    "      # ds[video_name].video_metadata.append(metadata)\n",
    "      # if counter > 100:\n",
    "        # break\n",
    "      counter += 1\n",
    "\n",
    "add_raw_videos_to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying dataset: 100%|██████████| 21/21 [00:00<00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage_ssd/v0_for_whisper_parallel_15 loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(path='/mnt/storage_ssd/v0_for_whisper_parallel_15', tensors=['video_filepath', 'video_name'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dl.deepcopy(OLD_DS_PATH, DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage_ssd/whisper_results_parallel_15 loaded successfully.\n",
      "Dataset(path='/mnt/storage_ssd/whisper_results_parallel_15', tensors=['caption', 'segment_metadata', 'video_filename', 'video_filepath'])\n",
      "\n",
      "      tensor        htype    shape    dtype  compression\n",
      "     -------       -------  -------  -------  ------- \n",
      "     caption        text    (5, 1)     str     None   \n",
      " segment_metadata   json    (1, 1)     str      lz4   \n",
      "  video_filename    text    (5, 1)     str     None   \n",
      "  video_filepath    text    (5, 1)     str     None   \n"
     ]
    }
   ],
   "source": [
    "l = f'/mnt/storage_ssd/whisper_results_{BATCH_NAME}'\n",
    "ds = dl.load(l)\n",
    "ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/parallel_15/jCciuZa8yXI_The Fumble_38323_Kevin Durant CAUGHT Hitting On Hottie At OKC Game & FINALLY Opens Up About BURNER Accounts!.webm'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = ds.video_filepath.data()['value']\n",
    "paths[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage_ssd/whisper_results_parallel_15 loaded successfully.\n",
      "Dataset(path='/mnt/storage_ssd/whisper_results_parallel_15', tensors=['caption', 'segment_metadata', 'video_filename', 'video_filepath'])\n",
      "\n",
      "      tensor        htype    shape    dtype  compression\n",
      "     -------       -------  -------  -------  ------- \n",
      "     caption        text    (41, 1)    str     None   \n",
      " segment_metadata   json    (41, 1)    str      lz4   \n",
      "  video_filename    text    (41, 1)    str     None   \n",
      "  video_filepath    text    (41, 1)    str     None   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds = dl.load(f'/mnt/storage_ssd/whisper_results_{BATCH_NAME}')\n",
    "ds.summary()\n",
    "completed_videos = set()\n",
    "# Get all completed vids for this batch iteratively (change to non-iter later)\n",
    "for index, i in enumerate(ds):\n",
    "  # print(index)\n",
    "  # print(i.video_filename.data())\n",
    "  # print(i.caption.data())\n",
    "  # i.video_filename.data()['value']\n",
    "  try:\n",
    "    completed_videos.add(i.video_filepath.data()['value'])\n",
    "  except:\n",
    "    print(index)\n",
    "# files = set(files) - completed_videos\n",
    "# files = list(files)\n",
    "len(completed_videos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_NAME = 'parallel_15'\n",
    "test_db = f'/mnt/storage_ssd/whisper_results_{BATCH_NAME}'\n",
    "dl.delete(test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage_ssd/v0_for_whisper_parallel_15 loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "ds = dl.load(DATASET_PATH)\n",
    "# ds.summary()\n",
    "files = ds.video_filepath.data()['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(files) - completed_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ds.video_filepath.data()['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23292"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(files) - set(files[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering -- Number of files: 23302\n"
     ]
    }
   ],
   "source": [
    "files = [ str(file) for file in files if not str(file).endswith( ('.txt','.vtt', 'json') ) ]\n",
    "print(\"After filtering -- Number of files:\", len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 0, 'b': 2}, {'a': 1, 'b': 2}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts = [{\"a\": 0}, {\"a\": 1}]\n",
    "num_dicts = len(dicts)\n",
    "for dic in dicts:\n",
    "  dic[\"b\"] = num_dicts\n",
    "dicts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_has_everything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b69eb772e953fa94c7e6a13e20e5676ab72e6ff0f5efef62f5cddb3889daa1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
