{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/custom_huggingface/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "# DEBUGGING \n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"parallel_15\"\n",
    "REMOTE_WHISPER_FILE = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_whisper_output.jsonl'\n",
    "REMOTE_CLIP_DIR  = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_clip_output'\n",
    "REMOTE_SCENE_FILE = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_scene_output.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate clip\n",
    "import clip\n",
    "\n",
    "MODEL_SIZE = 'ViT-L/14@336px'  # Best models are (1st) ViT-L/14@336px and (2nd) ViT-L/14. I don't recommend going lower.  \n",
    "clip_instance, clip_preprocess = clip.load(MODEL_SIZE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, OPTForCausalLM\n",
    "t5 = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "t5_tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "learning_rate       = 1e-4  # also good: 3e-4\n",
    "optimizer = torch.optim.AdamW(params =  t5.parameters(), lr=learning_rate) # Typically, 1e-4 and 3e-4 work well for most problems\n",
    "\n",
    "# prompt = \"Hey, are you consciours? Can you talk to me?\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate\n",
    "# generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "# tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForCausalLM, RobertaTokenizer\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "t5 = RobertaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=False).to(device) # float16, True\n",
    "t5_tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config, AutoModelWithLMHead\n",
    "\n",
    "'''\n",
    "MODEL SELECTION\n",
    "\n",
    "T5 V1.1 --  https://huggingface.co/docs/transformers/model_doc/t5v1.1 && https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511\n",
    "small - base - large - 3b/xl - 11b/xxl\n",
    "\n",
    "OG: t5-small\n",
    "\n",
    "'google/t5-base-lm-adapt' # largest on my server (without float16)\n",
    "'google/t5-xl-lm-adapt'\n",
    "\n",
    "google/t5-v1_1-large\n",
    "'''\n",
    "\n",
    "# MODEL_SIZE = \"t5-base\"\n",
    "MODEL_NAME = \"google/t5-v1_1-base\"\n",
    "# MODEL_NAME = \"google/t5-base-lm-adapt\"\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=False).to(device) # float16, True\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) â€” Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental.\n",
    "learning_rate       = 1e-4  # also good: 3e-4\n",
    "optimizer = torch.optim.AdamW(params =  t5.parameters(), lr=learning_rate) # Typically, 1e-4 and 3e-4 work well for most problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.linear.Linear"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t5.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cockpit ML Debugger\n",
    "# https://cockpit.readthedocs.io/en/latest/examples/01_basic_fmnist.html\n",
    "from backpack import extend\n",
    "from cockpit import Cockpit, CockpitPlotter\n",
    "from cockpit.utils.configuration import configuration\n",
    "\n",
    "t5 = extend(t5)\n",
    "\n",
    "individual_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Create Cockpit and a plotter\n",
    "cockpit = Cockpit(t5.parameters(), quantities=configuration(\"full\"))\n",
    "plotter = CockpitPlotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' just for trying the debugger cockpit'''\n",
    "\n",
    "\n",
    "# Iterate through the batch\n",
    "clip_15 = os.listdir(REMOTE_CLIP_DIR)\n",
    "\n",
    "# Initialize embeddings\n",
    "one_input_shape = [1, 768, 768]\n",
    "att_mask_shape = [1, 768]\n",
    "embed_shape = [1, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "attn_mask_arr[0][2] = 1 # no clip\n",
    "\n",
    "t5.train()\n",
    "\n",
    "global_step = 0 \n",
    "\n",
    "with jsonlines.open(REMOTE_SCENE_FILE, 'r') as scene_reader:\n",
    "    # Zipping the scene graph with the clip + whisper embeddings\n",
    "    \n",
    "    # itr over videos\n",
    "    for scene_seg_list, clip_npz_path in tqdm.tqdm(zip(scene_reader, glob.glob(os.path.join(REMOTE_CLIP_DIR, '*'), recursive = True))):\n",
    "        try:\n",
    "            np_loaded = np.load(clip_npz_path, allow_pickle=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load compressed numpy: {e}\")\n",
    "            continue\n",
    "        object_list_of_str = []\n",
    "        scene_seg_list = json.loads(scene_seg_list)\n",
    "        \n",
    "        # iterate over segments\n",
    "        for segment_index in range(np_loaded['arr_0'].item()['total_segments']):\n",
    "            # print(np_loaded[f'arr_{segment_index}'].item()['captions'])\n",
    "            frame_embedding       = np_loaded[f'arr_{segment_index}'].item()['frame_embeddings']\n",
    "            caption_embedding     = np_loaded[f'arr_{segment_index}'].item()['text_caption_embeddings']\n",
    "            whisper_text_captions = np_loaded[f'arr_{segment_index}'].item()['captions']\n",
    "            \n",
    "            frame_embedding       = torch.from_numpy(frame_embedding.reshape((768,))).to(device)\n",
    "            caption_embedding     = torch.from_numpy(caption_embedding).to(device)\n",
    "\n",
    "            # Update embedding array\n",
    "            input_embeds_arr[0][0] = frame_embedding\n",
    "            input_embeds_arr[0][1] = caption_embedding\n",
    "            \n",
    "            # print(\"Input shapes:\")\n",
    "            # print(caption_embedding)\n",
    "            print(frame_embedding)\n",
    "            labels = t5_tokenizer(whisper_text_captions, return_tensors=\"pt\").input_ids.to(device)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "            outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            print(\"loss\")\n",
    "            print(loss)\n",
    "            logits = outputs[1]\n",
    "            # print(\"logits\")\n",
    "            # print(logits)\n",
    "            individual_losses = individual_loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            # print(\"individual_losses\")\n",
    "            # print(individual_losses)\n",
    "            # loss = \n",
    "            for item in outputs:\n",
    "                print(item)\n",
    "            ''' backwards pass '''\n",
    "            # loss.sum().backward()\n",
    "            # backward pass\n",
    "            with cockpit(\n",
    "                global_step,\n",
    "                info={\n",
    "                    \"batch_size\": one_input_shape[0],\n",
    "                    \"individual_losses\": individual_losses,\n",
    "                    \"loss\": loss,\n",
    "                    \"optimizer\": optimizer,\n",
    "                },\n",
    "            ):\n",
    "                loss.backward(create_graph=cockpit.create_graph(global_step))\n",
    "            \n",
    "            # optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            print(f\"step: {global_step}\")\n",
    "            plotter.plot(cockpit)\n",
    "            \n",
    "            print(\"Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\")\n",
    "            print(loss)\n",
    "            break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaConfig\n",
    "# OPTConfig.from_pretrained(\"roberta-base\")\n",
    "from transformers import OPTConfig\n",
    "OPTConfig.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' More complete '''\n",
    "\n",
    "# Initialize embeddings\n",
    "# one_input_shape = [1, 768, 768]\n",
    "# att_mask_shape = [1, 768]\n",
    "# embed_shape = [1, 768]\n",
    "one_input_shape = [1, 2048, 768]  # torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)\n",
    "att_mask_shape = [1, 2048]        # \n",
    "embed_shape = [1, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape, device=device)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape, device=device)\n",
    "labels    = torch.zeros(att_mask_shape, dtype=int, device=device)\n",
    "print(labels.dtype)\n",
    "labels[:len(labels)] = -100\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "# attn_mask_arr[0][2] = 1 # no clip\n",
    "\n",
    "t5.train()\n",
    "\n",
    "global_step = 0 \n",
    "\n",
    "with jsonlines.open(REMOTE_SCENE_FILE, 'r') as scene_reader:\n",
    "    # Zipping the scene graph with the clip + whisper embeddings\n",
    "    # itr over videos\n",
    "    for scene_seg_list, clip_npz_path in tqdm.tqdm(zip(scene_reader, glob.glob(os.path.join(REMOTE_CLIP_DIR, '*'), recursive = True))):\n",
    "        try:\n",
    "            np_loaded = np.load(clip_npz_path, allow_pickle=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load compressed numpy: {e}\")\n",
    "            continue\n",
    "        object_list_of_str = []\n",
    "        scene_seg_list = json.loads(scene_seg_list)\n",
    "        \n",
    "        # iterate over segments\n",
    "        for segment_index in range(np_loaded['arr_0'].item()['total_segments']):\n",
    "            frame_embedding       = np_loaded[f'arr_{segment_index}'].item()['frame_embeddings']\n",
    "            caption_embedding     = np_loaded[f'arr_{segment_index}'].item()['text_caption_embeddings']\n",
    "            whisper_text_captions = np_loaded[f'arr_{segment_index}'].item()['captions']\n",
    "            \n",
    "            frame_embedding       = torch.from_numpy(frame_embedding.reshape((768,))).to(device)\n",
    "            caption_embedding     = torch.from_numpy(caption_embedding).to(device)\n",
    "\n",
    "            scene_caption = scene_seg_list[segment_index]\n",
    "            scene_caption = clip.tokenize(scene_caption).to(device)\n",
    "            with torch.inference_mode(): # even faster than no_grad()\n",
    "                scene_embedding = clip_instance.encode_text(scene_caption)\n",
    "            scene_embedding = scene_embedding.reshape((768,))\n",
    "\n",
    "            # Update embedding array\n",
    "            input_embeds_arr[0][0] = frame_embedding\n",
    "            input_embeds_arr[0][1] = caption_embedding\n",
    "            input_embeds_arr[0][2] = scene_embedding\n",
    "            \n",
    "            print(\"Input shapes:\")\n",
    "            print(input_embeds_arr.shape)\n",
    "            print(scene_embedding.shape)\n",
    "            print(caption_embedding.shape)\n",
    "            print(frame_embedding.shape)\n",
    "            tokenized_labels = t5_tokenizer(whisper_text_captions, return_tensors=\"pt\").input_ids.to(device)\n",
    "            print(\"tokenized_labels\")\n",
    "            print(tokenized_labels)\n",
    "            print(\"labels\")\n",
    "            print(labels)\n",
    "            print(labels.dtype)\n",
    "            labels[0][0:len(tokenized_labels[0])] = tokenized_labels[0]\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "            outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True, return_last_hidden_state=True)\n",
    "            print(outputs['last_hidden_state'].shape)\n",
    "            \n",
    "            results = t5.lm_head(outputs['last_hidden_state'])\n",
    "            print(results)\n",
    "            # loss = outputs[0]\n",
    "            # ''' backwards pass '''\n",
    "            # loss.sum().backward()\n",
    "            # # optimizer step\n",
    "            # optimizer.zero_grad()\n",
    "            # optimizer.step()\n",
    "            # global_step += 1\n",
    "            print(f\"step: {global_step}\")\n",
    "            \n",
    "            print(\"Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\")\n",
    "            print(loss)\n",
    "            break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "torch.Size([1, 768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "tokenized_labels\n",
      "tensor[1, 33] i64 xâˆˆ[1, 25000] Î¼=6.060e+03 Ïƒ=8.433e+03\n",
      "labels\n",
      "tensor[1, 768] i64 xâˆˆ[-100, -100] Î¼=-100.000 Ïƒ=0.\n",
      "torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:07, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m outputs \u001b[39m=\u001b[39m t5\u001b[39m.\u001b[39mforward(inputs_embeds\u001b[39m=\u001b[39minput_embeds_arr, attention_mask\u001b[39m=\u001b[39mattn_mask_arr, labels\u001b[39m=\u001b[39mlabels, return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs[\u001b[39m'\u001b[39;49m\u001b[39mlast_hidden_state\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m results \u001b[39m=\u001b[39m t5\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m'\u001b[39m\u001b[39mlast_hidden_state\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/utils/generic.py:271\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(k, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    270\u001b[0m     inner_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m (k, v) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m--> 271\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_dict[k]\n\u001b[1;32m    272\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_tuple()[k]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "''' More complete -- NO CLIP (for convienence) '''\n",
    "\n",
    "# Initialize embeddings\n",
    "one_input_shape = [1, 768, 768]\n",
    "att_mask_shape = [1, 768]\n",
    "embed_shape = [1, 768]\n",
    "# one_input_shape = [1, 2048, 768]  # torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)\n",
    "# att_mask_shape = [1, 2048]        # \n",
    "# embed_shape = [1, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape, device=device)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape, device=device)\n",
    "labels    = torch.zeros(att_mask_shape, dtype=int, device=device)\n",
    "print(labels.dtype)\n",
    "labels[:len(labels)] = -100\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "# attn_mask_arr[0][2] = 1 # no clip\n",
    "\n",
    "t5.train()\n",
    "\n",
    "global_step = 0 \n",
    "\n",
    "with jsonlines.open(REMOTE_SCENE_FILE, 'r') as scene_reader:\n",
    "    # Zipping the scene graph with the clip + whisper embeddings\n",
    "    # itr over videos\n",
    "    for scene_seg_list, clip_npz_path in tqdm.tqdm(zip(scene_reader, glob.glob(os.path.join(REMOTE_CLIP_DIR, '*'), recursive = True))):\n",
    "        try:\n",
    "            np_loaded = np.load(clip_npz_path, allow_pickle=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load compressed numpy: {e}\")\n",
    "            continue\n",
    "        object_list_of_str = []\n",
    "        scene_seg_list = json.loads(scene_seg_list)\n",
    "        \n",
    "        # iterate over segments\n",
    "        for segment_index in range(np_loaded['arr_0'].item()['total_segments']):\n",
    "            frame_embedding       = np_loaded[f'arr_{segment_index}'].item()['frame_embeddings']\n",
    "            caption_embedding     = np_loaded[f'arr_{segment_index}'].item()['text_caption_embeddings']\n",
    "            whisper_text_captions = np_loaded[f'arr_{segment_index}'].item()['captions']\n",
    "            \n",
    "            frame_embedding       = torch.from_numpy(frame_embedding.reshape((768,))).to(device)\n",
    "            caption_embedding     = torch.from_numpy(caption_embedding).to(device)\n",
    "\n",
    "            scene_caption = scene_seg_list[segment_index]\n",
    "            # scene_caption = clip.tokenize(scene_caption).to(device)\n",
    "            # with torch.inference_mode(): # even faster than no_grad()\n",
    "                # scene_embedding = clip_instance.encode_text(scene_caption)\n",
    "            # scene_embedding = scene_embedding.reshape((768,))\n",
    "\n",
    "            # Update embedding array\n",
    "            input_embeds_arr[0][0] = frame_embedding\n",
    "            input_embeds_arr[0][1] = caption_embedding\n",
    "            # input_embeds_arr[0][2] = scene_embedding\n",
    "            \n",
    "            print(\"Input shapes:\")\n",
    "            print(input_embeds_arr.shape)\n",
    "            # print(scene_embedding.shape)\n",
    "            print(caption_embedding.shape)\n",
    "            print(frame_embedding.shape)\n",
    "            tokenized_labels = t5_tokenizer(whisper_text_captions, return_tensors=\"pt\").input_ids.to(device)\n",
    "            print(\"tokenized_labels\")\n",
    "            print(tokenized_labels)\n",
    "            print(\"labels\")\n",
    "            print(labels)\n",
    "            print(labels.dtype)\n",
    "            labels[0][0:len(tokenized_labels[0])] = tokenized_labels[0]\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "            \n",
    "            decoder_input_ids = t5.forward()\n",
    "            output_ids = t5.generate(attention_mask=attn_mask_arr, decoder_input_ids=decoder_input_ids, inputs_embeds=input_embeds_arr, max_length=100, num_beams=4)\n",
    "\n",
    "            outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True, output_hidden_states=True)\n",
    "            print(outputs['last_hidden_state'].shape)\n",
    "            \n",
    "            results = t5.lm_head(outputs['last_hidden_state'])\n",
    "            print(results)\n",
    "            # loss = outputs[0]\n",
    "            # ''' backwards pass '''\n",
    "            # loss.sum().backward()\n",
    "            # # optimizer step\n",
    "            # optimizer.zero_grad()\n",
    "            # optimizer.step()\n",
    "            # global_step += 1\n",
    "            print(f\"step: {global_step}\")\n",
    "            \n",
    "            print(\"Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\")\n",
    "            print(loss)\n",
    "            break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 33] i64 xâˆˆ[1, 25000] Î¼=6.060e+03 Ïƒ=8.433e+03"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<lovely_tensors.repr_plt.PlotProxy at 0x7fca3bdeacd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = t5.lm_head(outputs['encoder_last_hidden_state'][0][0], shared_embedding=t5.shared)\n",
    "res.plt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'T5ForConditionalGeneration' object has no attribute 'get_output_layer_with_bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m t5\u001b[39m.\u001b[39;49mget_output_layer_with_bias()\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/custom_huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'T5ForConditionalGeneration' object has no attribute 'get_output_layer_with_bias'"
     ]
    }
   ],
   "source": [
    "t5.get_output_layer_with_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 768, 768] n=589824 xâˆˆ[-3.743, 3.480] Î¼=0.008 Ïƒ=0.221 grad MulBackward0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['encoder_last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = outputs['encoder_last_hidden_state']\n",
    "output_ids = t5.generate(attention_mask=attn_mask_arr, inputs_embeds=input_embeds_arr, max_length=768, num_beams=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyedge  beCy\n"
     ]
    }
   ],
   "source": [
    "print(t5_tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    tensor[768] xâˆˆ[-0.641, 0.664] Î¼=0.004 Ïƒ=0.206 grad SelectBackward0\n",
      "1    tensor[768] xâˆˆ[-0.819, 1.895] Î¼=0.005 Ïƒ=0.262 grad SelectBackward0\n",
      "2    tensor[768] xâˆˆ[-1.212, 2.202] Î¼=0.010 Ïƒ=0.257 grad SelectBackward0\n",
      "3    tensor[768] xâˆˆ[-1.842, 1.446] Î¼=0.011 Ïƒ=0.268 grad SelectBackward0\n",
      "4    tensor[768] xâˆˆ[-0.780, 0.905] Î¼=0.011 Ïƒ=0.253 grad SelectBackward0\n",
      "5    tensor[768] xâˆˆ[-0.409, 0.526] Î¼=0.006 Ïƒ=0.054 grad SelectBackward0\n",
      "6    tensor[768] xâˆˆ[-2.017, 2.062] Î¼=0.012 Ïƒ=0.269 grad SelectBackward0\n",
      "7    tensor[768] xâˆˆ[-1.222, 2.248] Î¼=0.013 Ïƒ=0.253 grad SelectBackward0\n",
      "8    tensor[768] xâˆˆ[-0.950, 0.782] Î¼=-0.006 Ïƒ=0.121 grad SelectBackward0\n",
      "9    tensor[768] xâˆˆ[-0.368, 0.582] Î¼=0.007 Ïƒ=0.076 grad SelectBackward0\n",
      "10    tensor[768] xâˆˆ[-1.840, 1.745] Î¼=0.003 Ïƒ=0.276 grad SelectBackward0\n",
      "11    tensor[768] xâˆˆ[-1.417, 1.333] Î¼=0.007 Ïƒ=0.256 grad SelectBackward0\n",
      "12    tensor[768] xâˆˆ[-0.878, 0.971] Î¼=-0.002 Ïƒ=0.154 grad SelectBackward0\n",
      "13    tensor[768] xâˆˆ[-1.169, 0.972] Î¼=0.014 Ïƒ=0.153 grad SelectBackward0\n",
      "14    tensor[768] xâˆˆ[-0.350, 0.489] Î¼=0.004 Ïƒ=0.048 grad SelectBackward0\n",
      "15    tensor[768] xâˆˆ[-0.439, 0.621] Î¼=0.003 Ïƒ=0.067 grad SelectBackward0\n",
      "16    tensor[768] xâˆˆ[-0.457, 0.248] Î¼=0.003 Ïƒ=0.062 grad SelectBackward0\n",
      "17    tensor[768] xâˆˆ[-0.952, 1.939] Î¼=0.014 Ïƒ=0.265 grad SelectBackward0\n",
      "18    tensor[768] xâˆˆ[-0.833, 2.797] Î¼=-0.005 Ïƒ=0.274 grad SelectBackward0\n",
      "19    tensor[768] xâˆˆ[-1.093, 3.173] Î¼=0.000 Ïƒ=0.278 grad SelectBackward0\n",
      "20    tensor[768] xâˆˆ[-1.327, 1.258] Î¼=0.005 Ïƒ=0.241 grad SelectBackward0\n",
      "21    tensor[768] xâˆˆ[-1.198, 1.670] Î¼=0.016 Ïƒ=0.232 grad SelectBackward0\n",
      "22    tensor[768] xâˆˆ[-0.730, 2.439] Î¼=0.010 Ïƒ=0.268 grad SelectBackward0\n",
      "23    tensor[768] xâˆˆ[-1.519, 1.539] Î¼=-0.003 Ïƒ=0.267 grad SelectBackward0\n",
      "24    tensor[768] xâˆˆ[-1.906, 1.837] Î¼=0.008 Ïƒ=0.269 grad SelectBackward0\n",
      "25    tensor[768] xâˆˆ[-0.623, 0.449] Î¼=0.007 Ïƒ=0.084 grad SelectBackward0\n",
      "26    tensor[768] xâˆˆ[-1.716, 2.238] Î¼=-0.009 Ïƒ=0.272 grad SelectBackward0\n",
      "27    tensor[768] xâˆˆ[-1.423, 2.154] Î¼=0.001 Ïƒ=0.269 grad SelectBackward0\n",
      "28    tensor[768] xâˆˆ[-1.078, 2.254] Î¼=0.003 Ïƒ=0.269 grad SelectBackward0\n",
      "29    tensor[768] xâˆˆ[-0.412, 0.445] Î¼=0.006 Ïƒ=0.051 grad SelectBackward0\n",
      "30    tensor[768] xâˆˆ[-1.106, 2.037] Î¼=0.013 Ïƒ=0.265 grad SelectBackward0\n",
      "31    tensor[768] xâˆˆ[-0.822, 2.528] Î¼=0.008 Ïƒ=0.257 grad SelectBackward0\n",
      "32    tensor[768] xâˆˆ[-1.766, 2.890] Î¼=0.001 Ïƒ=0.279 grad SelectBackward0\n",
      "33    tensor[768] xâˆˆ[-0.297, 0.514] Î¼=0.006 Ïƒ=0.053 grad SelectBackward0\n",
      "34    tensor[768] xâˆˆ[-1.095, 2.929] Î¼=0.004 Ïƒ=0.265 grad SelectBackward0\n",
      "35    tensor[768] xâˆˆ[-1.724, 2.031] Î¼=0.005 Ïƒ=0.253 grad SelectBackward0\n",
      "36    tensor[768] xâˆˆ[-0.387, 0.540] Î¼=0.006 Ïƒ=0.057 grad SelectBackward0\n",
      "37    tensor[768] xâˆˆ[-0.478, 0.520] Î¼=0.001 Ïƒ=0.044 grad SelectBackward0\n",
      "38    tensor[768] xâˆˆ[-0.332, 0.539] Î¼=0.009 Ïƒ=0.061 grad SelectBackward0\n",
      "39    tensor[768] xâˆˆ[-1.236, 1.905] Î¼=0.006 Ïƒ=0.249 grad SelectBackward0\n"
     ]
    }
   ],
   "source": [
    "outputs['encoder_last_hidden_state']\n",
    "\n",
    "for i in range(40):\n",
    "  print(i, \"  \",outputs['encoder_last_hidden_state'][0][i])\n",
    "  # display(outputs['encoder_last_hidden_state'][0][i].plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5.save_pretrained(\"BIG_PENIS_PREVAILS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0][0:27]\n",
    "# len(tokenized_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Inference with custom model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/custom_huggingface/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FINE-TUNED T5\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import OPTForCausalLM\n",
    "\n",
    "# MODEL_VERSION_NAME   = \"opt_yt_pretrain_adamW_1e-3_iter69000\"\n",
    "# TOKENIZER_NAME       = \"facebook/opt-125m\"\n",
    "\n",
    "# BASE_DIR            = '/scratch/bbki/kastanday/whisper'\n",
    "# BASE_DIR            = '/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train'\n",
    "# MODEL_SAVE_PATH     = f'{BASE_DIR}/MODEL_CHECKPOINTS/{MODEL_VERSION_NAME}'\n",
    "\n",
    "TOKENIZER_NAME       = \"google/t5-v1_1-base\"\n",
    "t5     = AutoModelWithLMHead.from_pretrained(\"/mnt/storage_hdd/thesis/MODEL_CHECKPOINTS/T5_labels_are_half_batch_15_adamW_iter622500\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate clip\n",
    "import clip\n",
    "\n",
    "MODEL_SIZE = 'ViT-L/14@336px'  # Best models are (1st) ViT-L/14@336px and (2nd) ViT-L/14. I don't recommend going lower.  \n",
    "clip_instance, clip_preprocess = clip.load(MODEL_SIZE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage_ssd/dummy_clip_results_parallel_15 loaded successfully.\n",
      "Dataset(path='/mnt/storage_ssd/dummy_clip_results_parallel_15', tensors=['caption', 'clip_hidden_states', 'pooled_clip_embedding', 'segment_metadata', 'video_filename', 'video_filepath'])\n",
      "\n",
      "        tensor           htype       shape        dtype  compression\n",
      "        -------         -------     -------      -------  ------- \n",
      "        caption          text      (17185, 1)      str     None   \n",
      "  clip_hidden_states     image   (68740, 1:768)  float32    lz4   \n",
      " pooled_clip_embedding   image   (68740, 1:768)  float32    lz4   \n",
      "   segment_metadata      json      (16748, 1)      str      lz4   \n",
      "    video_filename       text      (17185, 1)      str     None   \n",
      "    video_filepath       text      (17185, 1)      str     None   \n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get standard image and embed with CLIP.\n",
    "import deeplake as dl\n",
    "clip_dataset_path = f'/mnt/storage_ssd/dummy_clip_results_parallel_15'\n",
    "ds = dl.load(clip_dataset_path)\n",
    "print(ds.summary())\n",
    "\n",
    "frame_embedding = ds.pooled_clip_embedding[0].numpy()\n",
    "frame_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tie, bottle on counter, bottle on counter\n"
     ]
    }
   ],
   "source": [
    "# PROMPT = \"Hey guys welcome back to the channel, \"\n",
    "# PROMPT = \"I REALLY LIKE EATING ASS\"\n",
    "# PROMPT = \"TODAY WE ARE GOING TO\"\n",
    "PROMPT = \"IN THIS VIDEO WE WILL\"\n",
    "# PROMPT = \"THE MOST COMMON WORD IN THE ENGLISH LANGUAGE\"\n",
    "# PROMPT = \"IN THIS VIDEO WE WILL\"\n",
    "# PROMPT = \"I LOVE VIDEO GAMES BECAUSE I LOVE KILLING\"\n",
    "PROMPT = \"The secret to cooking a good burrito is\"\n",
    "\n",
    "\n",
    "# OLD/naive inference\n",
    "# input_ids = tokenizer(PROMPT, return_tensors=\"pt\").input_ids\n",
    "# outputs = model.generate(input_ids)\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "\n",
    "# Initialize embeddings\n",
    "one_input_shape = [1, 768, 768]\n",
    "att_mask_shape = [1, 768]\n",
    "input_embeds_arr = torch.zeros(one_input_shape, device=device)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape, device=device)\n",
    "# labels    = torch.zeros(att_mask_shape, dtype=int, device=device)\n",
    "# labels[:] = -100\n",
    "\n",
    "# device = 'cpu'\n",
    "caption_tokenized = clip.tokenize(PROMPT).to(device)\n",
    "with torch.inference_mode():\n",
    "    caption_embedding = clip_instance.encode_text(caption_tokenized)\n",
    "caption_embedding = caption_embedding.reshape((768,))\n",
    "\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "attn_mask_arr[0][2] = 1\n",
    "input_embeds_arr[0][0] = torch.tensor(frame_embedding)\n",
    "input_embeds_arr[0][1] = caption_embedding\n",
    "input_embeds_arr[0][2] = caption_embedding\n",
    "\n",
    "# TODO: generate attn_mask_arr, input_embeds_arr\n",
    "\n",
    "output_ids = t5.generate(attention_mask=attn_mask_arr, inputs_embeds=input_embeds_arr, max_length=768, num_beams=10)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage_ssd/dummy_clip_results_parallel_15 loaded successfully.\n",
      "Dataset(path='/mnt/storage_ssd/dummy_clip_results_parallel_15', tensors=['caption', 'clip_hidden_states', 'pooled_clip_embedding', 'segment_metadata', 'video_filename', 'video_filepath'])\n",
      "\n",
      "        tensor           htype       shape        dtype  compression\n",
      "        -------         -------     -------      -------  ------- \n",
      "        caption          text      (17185, 1)      str     None   \n",
      "  clip_hidden_states     image   (68740, 1:768)  float32    lz4   \n",
      " pooled_clip_embedding   image   (68740, 1:768)  float32    lz4   \n",
      "   segment_metadata      json      (16748, 1)      str      lz4   \n",
      "    video_filename       text      (17185, 1)      str     None   \n",
      "    video_filepath       text      (17185, 1)      str     None   \n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying GPT-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT2Config.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1123 2 3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_caption = '1123 2 3 4 5 6 7'.split(\" \")\n",
    "import itertools\n",
    "first_half_of_whisper_caption  = list(itertools.islice(scene_caption, 0, len(scene_caption)//2))\n",
    "second_half_of_whisper_caption = list(itertools.islice(scene_caption, len(scene_caption)//2, None))\n",
    "\" \".join(first_half_of_whisper_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '5', '6', '7']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_half_of_whisper_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1123', '2', '3'], ['4', '5', '6'], ['7']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(scene_caption)//2\n",
    "parts = [scene_caption[i:i+n] for i in range(0, len(scene_caption), n)]\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08d608f700504c6b03e88c868d0cc9b143978899209b13a888b26c423352d24d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
