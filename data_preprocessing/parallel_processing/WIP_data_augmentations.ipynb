{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset transformations (using Compose)\n",
    "\n",
    "### Transformations used in X-CLIP\n",
    "\n",
    "We could probably steal their implementation, too: https://github.com/microsoft/VideoX/blob/master/X-CLIP/datasets/rand_augment.py\n",
    "\n",
    "```python\n",
    "_RAND_CHOICE_WEIGHTS_0 = {\n",
    "    \"Rotate\": 0.3,\n",
    "    \"ShearX\": 0.2,\n",
    "    \"ShearY\": 0.2,\n",
    "    \"TranslateXRel\": 0.1,\n",
    "    \"TranslateYRel\": 0.1,\n",
    "    \"Color\": 0.025,\n",
    "    \"Sharpness\": 0.025,\n",
    "    \"AutoContrast\": 0.025,\n",
    "    \"Solarize\": 0.005,\n",
    "    \"SolarizeAdd\": 0.005,\n",
    "    \"Contrast\": 0.005,\n",
    "    \"Brightness\": 0.005,\n",
    "    \"Equalize\": 0.005,\n",
    "    \"Posterize\": 0,\n",
    "    \"Invert\": 0,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchscript (torch.jit.script) an optimizing JIT runtime compiler for PyTorch. \n",
    "# Compiled to C++, faster. I've read data augmentation is CPU-intensive, so this might help.\n",
    "\n",
    "# üìú ‚≠êÔ∏è Docs on all transforms: https://pytorch.org/vision/stable/transforms.html \n",
    "\n",
    "# Kastan's suggestions (helped by Copilot): \n",
    "# RandomRotation(degrees[, interpolation, ‚Ä¶])\n",
    "# RandomHorizontalFlip(p=0.5)\n",
    "# RandomVerticalFlip(p=0.5)\n",
    "# RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)\n",
    "# RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')\n",
    "# RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)\n",
    "# RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3, fill=0)\n",
    "# RandomApply(transforms, p=0.5)\n",
    "# RandomChoice(transforms)\n",
    "# RandomOrder(transforms) \n",
    "# ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)\n",
    "# Grayscale(num_output_channels=1)\n",
    "# Pad(padding, fill=0, padding_mode='constant')\n",
    "# LinearTransformation(transformation_matrix, mean_vector)\n",
    "# Normalize(mean, std, inplace=False)\n",
    "# Resize(size, interpolation=2)\n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    transforms.CenterCrop(10),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method....\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "video_transforms = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "612b182cb4c3e0acfd877acc6c10f43d075b0ae43380d6b249d2d2b5490153b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
