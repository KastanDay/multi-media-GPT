{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_and_db_index = [{'timestamp': 2.08878125, 'db_index': 0}, {'timestamp': 5.965406250000001, 'db_index': 1}, {'timestamp': 9.1594375, 'db_index': 2}, {'timestamp': 12.2341875, 'db_index': 3}, {'timestamp': 15.900781250000001, 'db_index': 4}, {'timestamp': 19.405625, 'db_index': 5}, {'timestamp': 22.3776875, 'db_index': 6}, {'timestamp': 26.35184375, 'db_index': 7}, {'timestamp': 31.088250000000002, 'db_index': 8}, {'timestamp': 34.802343750000006, 'db_index': 9}, {'timestamp': 38.1968125, 'db_index': 10}, {'timestamp': 41.30928125, 'db_index': 11}, {'timestamp': 45.142875000000004, 'db_index': 12}, {'timestamp': 49.017875000000004, 'db_index': 13}, {'timestamp': 52.29309375, 'db_index': 14}, {'timestamp': 56.47096875, 'db_index': 15}, {'timestamp': 60.86890625, 'db_index': 16}, {'timestamp': 64.52559375, 'db_index': 17}, {'timestamp': 67.6818125, 'db_index': 18}, {'timestamp': 70.47671875, 'db_index': 19}, {'timestamp': 74.21378125, 'db_index': 20}, {'timestamp': 78.57075, 'db_index': 21}, {'timestamp': 83.53734374999999, 'db_index': 22}, {'timestamp': 89.20384375, 'db_index': 23}, {'timestamp': 94.87990625, 'db_index': 24}, {'timestamp': 99.28521875, 'db_index': 25}, {'timestamp': 103.20015624999999, 'db_index': 26}, {'timestamp': 108.41871875000001, 'db_index': 27}, {'timestamp': 112.5041875, 'db_index': 28}, {'timestamp': 116.4690625, 'db_index': 29}, {'timestamp': 120.89615624999999, 'db_index': 30}, {'timestamp': 125.48046875, 'db_index': 31}, {'timestamp': 130.49409375, 'db_index': 32}, {'timestamp': 135.28009375, 'db_index': 33}, {'timestamp': 140.44878125, 'db_index': 34}, {'timestamp': 145.0458125, 'db_index': 35}, {'timestamp': 148.79084375, 'db_index': 36}, {'timestamp': 152.91546875, 'db_index': 37}, {'timestamp': 158.01359374999998, 'db_index': 38}, {'timestamp': 162.261125, 'db_index': 39}, {'timestamp': 167.1078125, 'db_index': 40}, {'timestamp': 171.66015625, 'db_index': 41}, {'timestamp': 177.22643749999997, 'db_index': 42}, {'timestamp': 181.68203125, 'db_index': 43}, {'timestamp': 185.64665624999998, 'db_index': 44}, {'timestamp': 189.49053125, 'db_index': 45}, {'timestamp': 192.64140625, 'db_index': 46}, {'timestamp': 195.96331249999997, 'db_index': 47}, {'timestamp': 200.5388125, 'db_index': 48}, {'timestamp': 204.56099999999998, 'db_index': 49}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.08878125,\n",
       " 5.965406250000001,\n",
       " 9.1594375,\n",
       " 12.2341875,\n",
       " 15.900781250000001,\n",
       " 19.405625,\n",
       " 22.3776875,\n",
       " 26.35184375,\n",
       " 31.088250000000002,\n",
       " 34.802343750000006,\n",
       " 38.1968125,\n",
       " 41.30928125,\n",
       " 45.142875000000004,\n",
       " 49.017875000000004,\n",
       " 52.29309375,\n",
       " 56.47096875,\n",
       " 60.86890625,\n",
       " 64.52559375,\n",
       " 67.6818125,\n",
       " 70.47671875,\n",
       " 74.21378125,\n",
       " 78.57075,\n",
       " 83.53734374999999,\n",
       " 89.20384375,\n",
       " 94.87990625,\n",
       " 99.28521875,\n",
       " 103.20015624999999,\n",
       " 108.41871875000001,\n",
       " 112.5041875,\n",
       " 116.4690625,\n",
       " 120.89615624999999,\n",
       " 125.48046875,\n",
       " 130.49409375,\n",
       " 135.28009375,\n",
       " 140.44878125,\n",
       " 145.0458125,\n",
       " 148.79084375,\n",
       " 152.91546875,\n",
       " 158.01359374999998,\n",
       " 162.261125,\n",
       " 167.1078125,\n",
       " 171.66015625,\n",
       " 177.22643749999997,\n",
       " 181.68203125,\n",
       " 185.64665624999998,\n",
       " 189.49053125,\n",
       " 192.64140625,\n",
       " 195.96331249999997,\n",
       " 200.5388125,\n",
       " 204.56099999999998]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[segment_dict['timestamp'] for segment_dict in time_and_db_index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset management (filtering, uploading, checkpointing)\n",
    "\n",
    "## Docs\n",
    "\n",
    "Main reference\n",
    "https://docs.deeplake.ai/en/latest/Datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage_ssd/whisper_results_parallel_15 loaded successfully.\n",
      "0.064 GB\n",
      "Dataset(path='/mnt/storage_ssd/whisper_results_parallel_15', tensors=['caption', 'segment_metadata', 'video_filename', 'video_filepath'])\n",
      "\n",
      "      tensor        htype     shape      dtype  compression\n",
      "     -------       -------   -------    -------  ------- \n",
      "     caption        text    (17055, 1)    str     None   \n",
      " segment_metadata   json    (16748, 1)    str      lz4   \n",
      "  video_filename    text    (17055, 1)    str     None   \n",
      "  video_filepath    text    (17055, 1)    str     None   \n"
     ]
    }
   ],
   "source": [
    "import deeplake\n",
    "dataset_name = '/mnt/storage_ssd/whisper_results_parallel_15'\n",
    "read_lake = deeplake.load(dataset_name)\n",
    "print(read_lake.size_approx() / 1e9, \"GB\")\n",
    "read_lake.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take first n samples\n",
    "read_lake = read_lake[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='/mnt/storage_ssd/whisper_results_parallel_15', index=Index([slice(None, 200, None)]), tensors=['caption', 'segment_metadata', 'video_filename', 'video_filepath'])\n",
      "\n",
      "      tensor        htype    shape     dtype  compression\n",
      "     -------       -------  -------   -------  ------- \n",
      "     caption        text    (200, 1)    str     None   \n",
      " segment_metadata   json    (200, 1)    str      lz4   \n",
      "  video_filename    text    (200, 1)    str     None   \n",
      "  video_filepath    text    (200, 1)    str     None   \n"
     ]
    }
   ],
   "source": [
    "read_lake.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'end': 5.017125,\n",
      " 'segment_index': 0,\n",
      " 'segment_word_list': [{'end': 1.3043749999999998,\n",
      "                        'start': 1.184,\n",
      "                        'word': 'HEY'},\n",
      "                       {'end': 1.6255, 'start': 1.3245, 'word': 'GUYS'},\n",
      "                       {'end': 1.7860624999999999,\n",
      "                        'start': 1.7058125,\n",
      "                        'word': \"IT'S\"},\n",
      "                       {'end': 2.026875, 'start': 1.806125, 'word': 'THUG'},\n",
      "                       {'end': 2.56875, 'start': 2.0670625, 'word': 'SNIPER'},\n",
      "                       {'end': 2.8697500000000002,\n",
      "                        'start': 2.749375,\n",
      "                        'word': 'AND'},\n",
      "                       {'end': 2.93, 'start': 2.889875, 'word': 'ME'},\n",
      "                       {'end': 3.7929375, 'start': 3.73275, 'word': 'AND'},\n",
      "                       {'end': 3.9535, 'start': 3.833125, 'word': 'MY'},\n",
      "                       {'end': 4.2344375, 'start': 3.9735625, 'word': 'BUDDY'},\n",
      "                       {'end': 4.354875000000001,\n",
      "                        'start': 4.274625,\n",
      "                        'word': 'HAVE'},\n",
      "                       {'end': 4.4953125, 'start': 4.3749375, 'word': 'BEEN'},\n",
      "                       {'end': 4.75625, 'start': 4.5154375, 'word': 'GETTING'},\n",
      "                       {'end': 4.8164375, 'start': 4.796375, 'word': 'A'},\n",
      "                       {'end': 5.017125, 'start': 4.856625, 'word': 'LOT'}],\n",
      " 'start': 1.184,\n",
      " 'total_segments': 24}\n",
      "'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/parallel_15/JccYTrBLmVI_ThugSniperx_109138_Part 1 How to Camo an Airsoft Gun.webm'\n"
     ]
    }
   ],
   "source": [
    "# data access\n",
    "import pprint\n",
    "for i, sample in enumerate(read_lake):\n",
    "  pprint.pprint(sample.segment_metadata.data()['value'])\n",
    "  pprint.pprint(sample.video_filepath.data()['value'])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to hub\n",
    "deeplake.copy(read_lake, dest=\"hub://kastan/p17_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "read_lake.add_creds_key('lBEly4ezopu3j54yvdehHYtLPQz6LJBzVx3Pmxpi')\n",
    "# print(os.environ.get('KAS_S3_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating copy transform: 40%|████      | 405963/1005115 [25:02<2:07:00"
     ]
    },
    {
     "ename": "TransformError",
     "evalue": "Cannot append to json tensor with Sample object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/transform/transform.py:217\u001b[0m, in \u001b[0;36mPipeline.eval\u001b[0;34m(self, data_in, ds_out, num_workers, scheduler, progressbar, skip_ok, check_lengths, pad_data_in, read_only_ok, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    218\u001b[0m         data_in,\n\u001b[1;32m    219\u001b[0m         target_ds,\n\u001b[1;32m    220\u001b[0m         compute_provider,\n\u001b[1;32m    221\u001b[0m         num_workers,\n\u001b[1;32m    222\u001b[0m         scheduler,\n\u001b[1;32m    223\u001b[0m         progressbar,\n\u001b[1;32m    224\u001b[0m         overwrite,\n\u001b[1;32m    225\u001b[0m         skip_ok,\n\u001b[1;32m    226\u001b[0m         read_only_ok \u001b[39mand\u001b[39;49;00m overwrite,\n\u001b[1;32m    227\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m     target_ds\u001b[39m.\u001b[39m_send_compute_progress(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprogress_end_args, status\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msuccess\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/transform/transform.py:340\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, data_in, target_ds, compute, num_workers, scheduler, progressbar, overwrite, skip_ok, read_only, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m         target_ds\u001b[39m.\u001b[39mdelete_tensor(tensor)\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m read_only:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/transform/transform.py:329\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, data_in, target_ds, compute, num_workers, scheduler, progressbar, overwrite, skip_ok, read_only, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m     desc \u001b[39m=\u001b[39m get_pbar_description(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunctions)\n\u001b[0;32m--> 329\u001b[0m     result \u001b[39m=\u001b[39m compute\u001b[39m.\u001b[39;49mmap_with_progressbar(\n\u001b[1;32m    330\u001b[0m         store_data_slice_with_pbar,\n\u001b[1;32m    331\u001b[0m         map_inp,\n\u001b[1;32m    332\u001b[0m         total_length\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(data_in),\n\u001b[1;32m    333\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/compute/serial.py:20\u001b[0m, in \u001b[0;36mSerialProvider.map_with_progressbar\u001b[0;34m(self, func, iterable, total_length, desc)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m func(pg_callback, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 20\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap(sub_func, iterable)\n\u001b[1;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/compute/serial.py:9\u001b[0m, in \u001b[0;36mSerialProvider.map\u001b[0;34m(self, func, iterable)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable):\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(func, iterable))\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/compute/serial.py:18\u001b[0m, in \u001b[0;36mSerialProvider.map_with_progressbar.<locals>.sub_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     progress_bar\u001b[39m.\u001b[39mupdate(value)\n\u001b[0;32m---> 18\u001b[0m \u001b[39mreturn\u001b[39;00m func(pg_callback, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/util/transform.py:134\u001b[0m, in \u001b[0;36mstore_data_slice_with_pbar\u001b[0;34m(pg_callback, transform_input)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m extend_only:\n\u001b[0;32m--> 134\u001b[0m     extend_data_slice(\n\u001b[1;32m    135\u001b[0m         data_slice,\n\u001b[1;32m    136\u001b[0m         pipeline,\n\u001b[1;32m    137\u001b[0m         all_chunk_engines,\n\u001b[1;32m    138\u001b[0m         group_index,\n\u001b[1;32m    139\u001b[0m         pg_callback,\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/util/transform.py:261\u001b[0m, in \u001b[0;36mextend_data_slice\u001b[0;34m(data_slice, pipeline, all_chunk_engines, group_index, pg_callback)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     chunk_engine\u001b[39m.\u001b[39;49mextend(\n\u001b[1;32m    262\u001b[0m         value\u001b[39m.\u001b[39;49mnumpy_compressed(),\n\u001b[1;32m    263\u001b[0m         link_callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    264\u001b[0m         pg_callback\u001b[39m=\u001b[39;49mpg_callback,\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    266\u001b[0m value\u001b[39m.\u001b[39mitems\u001b[39m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/chunk_engine.py:948\u001b[0m, in \u001b[0;36mChunkEngine.extend\u001b[0;34m(self, samples, progressbar, link_callback, pg_callback)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     verified_samples \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 948\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extend(samples, progressbar, pg_callback\u001b[39m=\u001b[39;49mpg_callback) \u001b[39mor\u001b[39;00m samples\n\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    950\u001b[0m     \u001b[39mif\u001b[39;00m link_callback:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/chunk_engine.py:902\u001b[0m, in \u001b[0;36mChunkEngine._extend\u001b[0;34m(self, samples, progressbar, pg_callback, update_commit_diff)\u001b[0m\n\u001b[1;32m    901\u001b[0m samples, verified_samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sanitize_samples(samples)\n\u001b[0;32m--> 902\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_samples_to_chunks(\n\u001b[1;32m    903\u001b[0m     samples,\n\u001b[1;32m    904\u001b[0m     start_chunk\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_appended_chunk(),\n\u001b[1;32m    905\u001b[0m     register\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    906\u001b[0m     progressbar\u001b[39m=\u001b[39;49mprogressbar,\n\u001b[1;32m    907\u001b[0m     update_commit_diff\u001b[39m=\u001b[39;49mupdate_commit_diff,\n\u001b[1;32m    908\u001b[0m     pg_callback\u001b[39m=\u001b[39;49mpg_callback,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m \u001b[39mreturn\u001b[39;00m verified_samples\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/chunk_engine.py:775\u001b[0m, in \u001b[0;36mChunkEngine._samples_to_chunks\u001b[0;34m(self, samples, start_chunk, register, update_commit_diff, update_tensor_meta, start_chunk_row, progressbar, register_creds, pg_callback)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(samples) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     num_samples_added \u001b[39m=\u001b[39m current_chunk\u001b[39m.\u001b[39;49mextend_if_has_space(\n\u001b[1;32m    776\u001b[0m         samples, update_tensor_meta\u001b[39m=\u001b[39;49mupdate_tensor_meta, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_args  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    777\u001b[0m     )  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    778\u001b[0m     \u001b[39mif\u001b[39;00m register_creds:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/chunk/sample_compressed_chunk.py:26\u001b[0m, in \u001b[0;36mSampleCompressedChunk.extend_if_has_space\u001b[0;34m(self, incoming_samples, update_tensor_meta, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m i, incoming_sample \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(incoming_samples):\n\u001b[0;32m---> 26\u001b[0m     serialized_sample, shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserialize_sample(incoming_sample, compr)\n\u001b[1;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/chunk/base_chunk.py:318\u001b[0m, in \u001b[0;36mBaseChunk.serialize_sample\u001b[0;34m(self, incoming_sample, sample_compression, chunk_compression, break_into_tiles, store_uncompressed_tiles)\u001b[0m\n\u001b[1;32m    317\u001b[0m         htype \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLinked\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensor_meta\u001b[39m.\u001b[39mis_link \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhtype\n\u001b[0;32m--> 318\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot append to \u001b[39m\u001b[39m{\u001b[39;00mhtype\u001b[39m}\u001b[39;00m\u001b[39m tensor with Sample object\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         )\n\u001b[1;32m    321\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot append to json tensor with Sample object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransformError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# s3_bucket = deeplake.empty(\"s3://vpt-deeplake/p17_test\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# upload to s3\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m deeplake\u001b[39m.\u001b[39;49mcopy(read_lake, dest\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39ms3://vpt-deeplake/final_clip_p15\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/api/dataset.py:740\u001b[0m, in \u001b[0;36mdataset.copy\u001b[0;34m(src, dest, tensors, overwrite, src_creds, token, dest_creds, num_workers, scheduler, progressbar, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m     src_ds\u001b[39m.\u001b[39mpath \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(src_ds\u001b[39m.\u001b[39mpath)\n\u001b[1;32m    738\u001b[0m dest \u001b[39m=\u001b[39m convert_pathlib_to_string_if_needed(dest)\n\u001b[0;32m--> 740\u001b[0m \u001b[39mreturn\u001b[39;00m src_ds\u001b[39m.\u001b[39;49mcopy(\n\u001b[1;32m    741\u001b[0m     dest,\n\u001b[1;32m    742\u001b[0m     tensors\u001b[39m=\u001b[39;49mtensors,\n\u001b[1;32m    743\u001b[0m     overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[1;32m    744\u001b[0m     creds\u001b[39m=\u001b[39;49mdest_creds,\n\u001b[1;32m    745\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    746\u001b[0m     num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    747\u001b[0m     scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m    748\u001b[0m     progressbar\u001b[39m=\u001b[39;49mprogressbar,\n\u001b[1;32m    749\u001b[0m )\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/dataset/dataset.py:3428\u001b[0m, in \u001b[0;36mDataset.copy\u001b[0;34m(self, dest, tensors, overwrite, creds, token, num_workers, scheduler, progressbar, public)\u001b[0m\n\u001b[1;32m   3396\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcopy\u001b[39m(\n\u001b[1;32m   3397\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3398\u001b[0m     dest: Union[\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPath],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3406\u001b[0m     public: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   3407\u001b[0m ):\n\u001b[1;32m   3408\u001b[0m     \u001b[39m\"\"\"Copies this dataset or dataset view to ``dest``. Version control history is not included.\u001b[39;00m\n\u001b[1;32m   3409\u001b[0m \n\u001b[1;32m   3410\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3426\u001b[0m \u001b[39m        DatasetHandlerError: If a dataset already exists at destination path and overwrite is False.\u001b[39;00m\n\u001b[1;32m   3427\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3428\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_copy(\n\u001b[1;32m   3429\u001b[0m         dest,\n\u001b[1;32m   3430\u001b[0m         tensors,\n\u001b[1;32m   3431\u001b[0m         overwrite,\n\u001b[1;32m   3432\u001b[0m         creds,\n\u001b[1;32m   3433\u001b[0m         token,\n\u001b[1;32m   3434\u001b[0m         num_workers,\n\u001b[1;32m   3435\u001b[0m         scheduler,\n\u001b[1;32m   3436\u001b[0m         progressbar,\n\u001b[1;32m   3437\u001b[0m         public,\n\u001b[1;32m   3438\u001b[0m     )\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/dataset/dataset.py:3359\u001b[0m, in \u001b[0;36mDataset._copy\u001b[0;34m(self, dest, tensors, overwrite, creds, token, num_workers, scheduler, progressbar, public, unlink, create_vds_index_tensor)\u001b[0m\n\u001b[1;32m   3357\u001b[0m     reset_index \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   3358\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3359\u001b[0m     deeplake\u001b[39m.\u001b[39;49mcompute(_copy_tensor, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcopy transform\u001b[39;49m\u001b[39m\"\u001b[39;49m)()\u001b[39m.\u001b[39;49meval(\n\u001b[1;32m   3360\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   3361\u001b[0m         dest_ds,\n\u001b[1;32m   3362\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m   3363\u001b[0m         scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m   3364\u001b[0m         progressbar\u001b[39m=\u001b[39;49mprogressbar,\n\u001b[1;32m   3365\u001b[0m         skip_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3366\u001b[0m         check_lengths\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   3367\u001b[0m         disable_label_sync\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3368\u001b[0m         extend_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3369\u001b[0m     )\n\u001b[1;32m   3371\u001b[0m     dest_ds\u001b[39m.\u001b[39mflush()\n\u001b[1;32m   3372\u001b[0m     \u001b[39mif\u001b[39;00m create_vds_index_tensor:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/transform/transform.py:90\u001b[0m, in \u001b[0;36mComputeFunction.eval\u001b[0;34m(self, data_in, ds_out, num_workers, scheduler, progressbar, skip_ok, check_lengths, pad_data_in, read_only_ok, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\"\"\"Evaluates the ComputeFunction on data_in to produce an output dataset ds_out.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m    UnsupportedSchedulerError: If the scheduler passed is not recognized. Supported values include: 'serial', 'threaded', 'processed' and 'ray'.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline([\u001b[39mself\u001b[39m])\n\u001b[0;32m---> 90\u001b[0m pipeline\u001b[39m.\u001b[39;49meval(\n\u001b[1;32m     91\u001b[0m     data_in,\n\u001b[1;32m     92\u001b[0m     ds_out,\n\u001b[1;32m     93\u001b[0m     num_workers,\n\u001b[1;32m     94\u001b[0m     scheduler,\n\u001b[1;32m     95\u001b[0m     progressbar,\n\u001b[1;32m     96\u001b[0m     skip_ok,\n\u001b[1;32m     97\u001b[0m     check_lengths,\n\u001b[1;32m     98\u001b[0m     pad_data_in,\n\u001b[1;32m     99\u001b[0m     read_only_ok,\n\u001b[1;32m    100\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    101\u001b[0m )\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/general_has_everything/lib/python3.9/site-packages/deeplake/core/transform/transform.py:232\u001b[0m, in \u001b[0;36mPipeline.eval\u001b[0;34m(self, data_in, ds_out, num_workers, scheduler, progressbar, skip_ok, check_lengths, pad_data_in, read_only_ok, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    231\u001b[0m     target_ds\u001b[39m.\u001b[39m_send_compute_progress(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprogress_end_args, status\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfailed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 232\u001b[0m     \u001b[39mraise\u001b[39;00m TransformError(e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     compute_provider\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mTransformError\u001b[0m: Cannot append to json tensor with Sample object"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating copy transform: 40%|████      | 405963/1005115 [25:15<2:07:00"
     ]
    }
   ],
   "source": [
    "# s3_bucket = deeplake.empty(\"s3://vpt-deeplake/p17_test\")\n",
    "# upload to s3\n",
    "deeplake.copy(read_lake, dest=\"s3://vpt-deeplake/final_clip_p15\")\n",
    "## Todo: parallel. num_workers=8, scheduler='processed' or 'threaded' or 'ray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_stems = set(read_lake.video_stem.data()['value'])\n",
    "# print(len(all_stems))\n",
    "# read_lake.query(\"select * where contains(video_stem, 'car')\")\n",
    "\n",
    "def filter_func(sample):\n",
    "  # print(sample.segment_metadata.data()['value'])\n",
    "  caption = sample.segment_metadata.data()['value']['captions']\n",
    "  if 'computer'.upper() in caption.upper():\n",
    "    # print(\"returning true\")\n",
    "    return True \n",
    "  elif 'kiss'.upper() in caption.upper():\n",
    "    return True\n",
    "  else:\n",
    "    # print('false')\n",
    "    return False\n",
    "\n",
    "filtered_dataset = read_lake.filter(filter_func, num_workers = 2, scheduler = 'threaded')\n",
    "# read_lake.filter(\"'computer' in segment_metadata.data()['value']['captions']\", num_workers = 10, scheduler = 'ray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "filtered_dataset.summary()\n",
    "for i, sample in enumerate(filtered_dataset):\n",
    "  pprint.pprint(sample.segment_metadata.data()['value'])\n",
    "  \n",
    "  if i > 20: \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_lake.segment_metadata[2].data()['value'][total_segments]\n",
    "# read_lake.text_caption_embeddings[2].data()['value']\n",
    "\n",
    "read_lake = filtered_dataset\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "for i in range(110):\n",
    "  # frame = read_lake.segment_frames[i].numpy()\n",
    "  # frame_rgb = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "  display(Image.fromarray(read_lake.segment_frames[i].numpy()))\n",
    "  print(read_lake.segment_metadata[i].data()['value']['captions'])\n",
    "  metadata = read_lake.segment_metadata[i].data()['value']\n",
    "  print(metadata['segment_index'], \"of\", metadata['total_segments'])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_has_everything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b69eb772e953fa94c7e6a13e20e5676ab72e6ff0f5efef62f5cddb3889daa1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
