{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Transforms to Video datasets\n",
    "\n",
    "Good demo here (using Kinetics):\n",
    "https://pytorch.org/vision/stable/auto_examples/plot_video_api.html#sphx-glr-auto-examples-plot-video-api-py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "torch.manual_seed(69)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPT_Dataset(Dataset):\n",
    "    \"\"\"TODO: Adapt this data class.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        # self.root_dir = root_dir\n",
    "        self.video_file = []\n",
    "        self.img_embeddings = []\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # todo: adapt this to our data\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset transformations (using Compose)\n",
    "\n",
    "### Transformations used in X-CLIP\n",
    "\n",
    "We could probably steal their implementation, too: https://github.com/microsoft/VideoX/blob/master/X-CLIP/datasets/rand_augment.py\n",
    "\n",
    "```python\n",
    "_RAND_CHOICE_WEIGHTS_0 = {\n",
    "    \"Rotate\": 0.3,\n",
    "    \"ShearX\": 0.2,\n",
    "    \"ShearY\": 0.2,\n",
    "    \"TranslateXRel\": 0.1,\n",
    "    \"TranslateYRel\": 0.1,\n",
    "    \"Color\": 0.025,\n",
    "    \"Sharpness\": 0.025,\n",
    "    \"AutoContrast\": 0.025,\n",
    "    \"Solarize\": 0.005,\n",
    "    \"SolarizeAdd\": 0.005,\n",
    "    \"Contrast\": 0.005,\n",
    "    \"Brightness\": 0.005,\n",
    "    \"Equalize\": 0.005,\n",
    "    \"Posterize\": 0,\n",
    "    \"Invert\": 0,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchscript (torch.jit.script) an optimizing JIT runtime compiler for PyTorch. \n",
    "# Compiled to C++, faster. I've read data augmentation is CPU-intensive, so this might help.\n",
    "transforms = torch.nn.Sequential(\n",
    "    transforms.CenterCrop(10),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    CenterCrop(size=(10, 10))\n",
       "    PILToTensor()\n",
       "    ConvertImageDtype()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "video_transforms = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = VPT_Dataset(\"./dataset\", epoch_size=None, transform=video_transforms)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=12)\n",
    "data = {\"video\": [], 'start': [], 'end': [], 'tensorsize': []}\n",
    "for batch in loader:\n",
    "    for i in range(len(batch['path'])):\n",
    "        data['video'].append(batch['path'][i])\n",
    "        data['start'].append(batch['start'][i].item())\n",
    "        data['end'].append(batch['end'][i].item())\n",
    "        data['tensorsize'].append(batch['video'][i].size())\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0497ffde1df318912604fec7dc437cf596dddf2ca6f4de129c07bc0ec5975633"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
