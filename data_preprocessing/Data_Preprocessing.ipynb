{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing + Loading for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### Imports\n",
    "import os \n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installs\n",
    "# ! pip install git+https://github.com/openai/CLIP.git\n",
    "# ! pip install cog\n",
    "# ! pip uninstall mmcv\n",
    "# ! pip install mmcv-full==1.4.3 -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.7.0/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Code to create each training sample and save each sample as a npy file.\n",
    "Npy file is a dict of the following format:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"youtube_name\": \"2ZokZgnjrYs\",                      \n",
    "  \"filename\": video_name,                      \n",
    "  \"youtube_id\": \"Hawaii Vlog\",                         \n",
    "  \"segment_length_seconds\": 15.0,\n",
    "  \"captions\": \"Daniel's chosen format here\",           \n",
    "  \"segment_start_time\":[<segment_start_times_list>],\n",
    "  \"segment_end_time\":[<segment_end_times_list>],     \n",
    "  \"frame_embeddings\":[<frame_embeddings_list>],        \n",
    "  \"audio_embeddings\":[<audio_embeddings_list>],        \n",
    "  \"text_caption_embeddings\":[<text_embeddings_list>],  \n",
    "  \"scene_graph_embeddings\":[<scene_graph_embeddings_list>],     \n",
    "  # \"labels\": [<labels_list>],                       \n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed Whisper File Format\n",
    "\n",
    "The preprocessed Whisper files are of the following format:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        'caption': \"some-caption-1\",\n",
    "        'start': caption_start_time,\n",
    "        'end': caption_end_time, \n",
    "        'segment_word_list': [{'word': 'some-word', 'start': word_start_time, 'end': word_end_time}, ...]\n",
    "    },\n",
    "    {\n",
    "        'caption': \"some-caption-2\",\n",
    "        'start': caption_start_time,\n",
    "        'end': caption_end_time, \n",
    "        'segment_word_list': [{'word': 'some-word', 'start': word_start_time, 'end': word_end_time}, ...]\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `DataPreprocessor` class, instantiate it with the data path to the folder containing .webm videos and the data path containing the Whisper extracted json files. Also, make sure you set the extension to `\".webm\"`. Then, simply call `process_using_audio_dir` with the desired output path to store the npy files. Each npy file corresponds to a single sample i.e. a single segment extracted from a video. The naming convention for the npy files is the following format: `{video_name}_segment{segment_number}.npy`. \n",
    "\n",
    "\n",
    "Example usage: \n",
    "\n",
    "\n",
    "```\n",
    "data_preprocessor = DataPreprocessor(video_data_path=\"/mnt/storage_ssd/yt1b_train_slice/\", audio_data_path=\"/mnt/storage_ssd/yt1b_train_slice_json/\", extension=\".webm\")\n",
    "data_preprocessor.process_using_audio_dir(\"./test_sample_generation\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor: \n",
    "    def __init__(self, video_data_path, audio_data_path, extension=\".mp4\", debug=True):\n",
    "        self.video_data_path = video_data_path\n",
    "        self.audio_data_path = audio_data_path\n",
    "        self.extension = extension\n",
    "        self.debug = debug\n",
    "\n",
    "        # Load the model\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Using {self.device}...\")\n",
    "\n",
    "        self.clip, self.clip_preprocess = clip.load('ViT-B/32', self.device)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Done setting up CLIP...\")\n",
    "\n",
    "        # self.scene_graph_predictor = Predictor()\n",
    "        # self.scene_graph_predictor.setup()\n",
    "\n",
    "\n",
    "    def get_frames_for_segments(self, video_name, segments):\n",
    "        if len(segments) == 0:\n",
    "            return None\n",
    "        \n",
    "        curr_segment_idx = 0\n",
    "        curr_segment_start = segments[0]['start']\n",
    "        curr_segment_end = segments[0]['end']\n",
    "\n",
    "        segment_frames = [[] for i in range(len(segments))]\n",
    "\n",
    "        assert os.path.exists(self.video_data_path+video_name+self.extension)\n",
    "\n",
    "        cap = cv2.VideoCapture(self.video_data_path+video_name+self.extension)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        frame_width = int(cap.get(3))\n",
    "        frame_height = int(cap.get(4))\n",
    "        \n",
    "        size = (frame_width, frame_height)\n",
    "\n",
    "        timestamps = [cap.get(cv2.CAP_PROP_POS_MSEC)]\n",
    "        curr_timestamp = 0.0\n",
    "\n",
    "        segments_done = False\n",
    "\n",
    "        while(cap.isOpened()):\n",
    "            frame_exists, curr_frame = cap.read()\n",
    "            \n",
    "            if frame_exists:\n",
    "                curr_timestamp = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "                timestamps.append(curr_timestamp)\n",
    "\n",
    "                if (curr_timestamp/1000.0) < curr_segment_start:\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    if (curr_timestamp/1000.0) < curr_segment_end:\n",
    "                        segment_frames[curr_segment_idx].append(curr_frame)\n",
    "                    else:\n",
    "                        curr_segment_start = segments[curr_segment_idx]['start']\n",
    "                        curr_segment_end = segments[curr_segment_idx]['end']\n",
    "                        while True:\n",
    "                            if ((curr_timestamp/1000.0) > curr_segment_start) and ((curr_timestamp/1000.0) < curr_segment_end):\n",
    "                                break\n",
    "\n",
    "                            if (curr_timestamp/1000.0) < curr_segment_start:\n",
    "                                break\n",
    "\n",
    "                            curr_segment_idx += 1\n",
    "                            if curr_segment_idx >= len(segments):\n",
    "                                # print(\"segments done hit\")\n",
    "                                segments_done = True\n",
    "                                break\n",
    "                        \n",
    "                            curr_segment_start = segments[curr_segment_idx]['start']\n",
    "                            curr_segment_end = segments[curr_segment_idx]['end']\n",
    "\n",
    "                        if segments_done:\n",
    "                            break\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        print(min(timestamps), max(timestamps))\n",
    "\n",
    "        return segment_frames\n",
    "        \n",
    "\n",
    "    def get_multimodal_features(self, video_name, segments):\n",
    "        segment_frames = self.get_frames_for_segments(video_name, segments)\n",
    "        scene_graph_features = []\n",
    "        clip_features = []\n",
    "        caption_features = []\n",
    "\n",
    "\n",
    "        # extract clip features for each segment  and extract text features for captions\n",
    "        for i, frames_list in enumerate(segment_frames):\n",
    "            if len(frames_list) == 0:\n",
    "                continue\n",
    "\n",
    "            # TODO: extract scene graph features for each segment\n",
    "            # scene_graph = self.scene_graph_predictor.predict(frames_list[0], num_rel=10)\n",
    "\n",
    "            middle_frame_idx = len(frames_list) // 2\n",
    "            sample_frame_idxs = [middle_frame_idx]\n",
    "\n",
    "            resized_frames = [Image.fromarray(cv2.resize(frames_list[frame_idx], dsize=(224, 224), interpolation=cv2.INTER_CUBIC)) for frame_idx in sample_frame_idxs]\n",
    "\n",
    "            image_input = torch.cat([self.clip_preprocess(frame).unsqueeze(0) for frame in resized_frames]).to(self.device)\n",
    "            text_inputs = torch.cat([clip.tokenize(segments[i]['caption'])]).to(self.device)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                image_features = self.clip.encode_image(image_input)\n",
    "                text_features = self.clip.encode_text(text_inputs)\n",
    "\n",
    "                clip_features.append(image_features)\n",
    "                caption_features.append(text_features)\n",
    "\n",
    "        return clip_features, scene_graph_features, caption_features, segment_frames\n",
    "\n",
    "\n",
    "    def construct_training_samples(self, video_name, output_path):\n",
    "        # initialize empty sample\n",
    "        whisper_segments = None\n",
    "        with open(self.audio_data_path+video_name+\".json\", \"r\") as whisper_f:\n",
    "            whisper_segments = json.load(whisper_f)\n",
    "\n",
    "        image_features, scene_graph_features, caption_features = self.get_multimodal_features(video_name, whisper_segments)\n",
    "\n",
    "        # assert len(image_features) == len(scene_graph_features) == len(caption_features) == len(whisper_segments)\n",
    "        \n",
    "        for i, (image_feature, scene_graph_feature, caption_feature, segment_frames) in enumerate(zip(image_features, scene_graph_features, caption_features, segment_frames)):\n",
    "            sample_dict = {\n",
    "                \"filename\": video_name,\n",
    "                \"segment_length\": whisper_segments[i]['end'] - whisper_segments[i]['start'],\n",
    "                \"captions\": whisper_segments[i]['caption'],\n",
    "                \"segment_start_time\": whisper_segments[i]['start'],\n",
    "                \"segment_end_time\": whisper_segments[i]['end'],\n",
    "                \"frame_embeddings\": image_feature,\n",
    "                \"text_caption_embeddings\": caption_feature,\n",
    "                # \"scene_graph_embeddings\": scene_graph_feature\n",
    "                \"segment_frames\": segment_frames\n",
    "            }\n",
    "\n",
    "            # TODO: Uncomment this code to start saving the files\n",
    "            np.save(os.path.join(output_path, f\"{video_name}_segment{i}\"), sample_dict)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Constructed training samples\")\n",
    "            \n",
    "    def process_using_audio_dir(self, output_path):\n",
    "        samples_not_found = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        if not os.path.exists(output_path):\n",
    "            # Create a new directory because it does not exist\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        all_whisper_files = os.listdir(self.audio_data_path)\n",
    "        for i in tqdm(range(len(all_whisper_files))):\n",
    "            f = all_whisper_files[i]\n",
    "            if f[-5:] == \".json\":\n",
    "                video_name = f[:-5]\n",
    "                # todo: use os.path.join()\n",
    "                if not os.path.exists(self.video_data_path+video_name+self.extension):\n",
    "                    samples_not_found += 1\n",
    "                else:\n",
    "                    self.construct_training_samples(video_name, output_path)\n",
    "\n",
    "                total_samples += 1\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"[WARNING] {samples_not_found}/{total_samples} are invalid\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda...\n",
      "Done setting up CLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2458 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 123040.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/2458 [00:03<08:46,  4.63it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# webm, mp4, avi, mkv\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m data_preprocessor \u001b[39m=\u001b[39m DataPreprocessor(video_data_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/parallel_0/\u001b[39m\u001b[39m\"\u001b[39m, audio_data_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/parallel_0_json/\u001b[39m\u001b[39m\"\u001b[39m, extension\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.mp4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m data_preprocessor\u001b[39m.\u001b[39;49mprocess_using_audio_dir(\u001b[39m\"\u001b[39;49m\u001b[39m./test_sample_generation\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb Cell 8\u001b[0m in \u001b[0;36mDataPreprocessor.process_using_audio_dir\u001b[0;34m(self, output_path)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=167'>168</a>\u001b[0m             samples_not_found \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=168'>169</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=169'>170</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct_training_samples(video_name, output_path)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=171'>172</a>\u001b[0m         total_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=173'>174</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug:\n",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb Cell 8\u001b[0m in \u001b[0;36mDataPreprocessor.construct_training_samples\u001b[0;34m(self, video_name, output_path)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=127'>128</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_data_path\u001b[39m+\u001b[39mvideo_name\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m whisper_f:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m     whisper_segments \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(whisper_f)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m image_features, scene_graph_features, caption_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_multimodal_features(video_name, whisper_segments)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m \u001b[39m# assert len(image_features) == len(scene_graph_features) == len(caption_features) == len(whisper_segments)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Data_Preprocessing.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (image_feature, scene_graph_feature, caption_feature, segment_frames) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(image_features, scene_graph_features, caption_features, segment_frames)):\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# webm, mp4, avi, mkv\n",
    "data_preprocessor = DataPreprocessor(video_data_path=\"/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/parallel_0/\", audio_data_path=\"/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/parallel_0_json/\", extension=\".mp4\")\n",
    "data_preprocessor.process_using_audio_dir(\"./test_sample_generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp_v2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "612b182cb4c3e0acfd877acc6c10f43d075b0ae43380d6b249d2d2b5490153b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
