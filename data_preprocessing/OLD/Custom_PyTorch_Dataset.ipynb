{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Transforms to Video datasets\n",
    "\n",
    "Good demo here (using Kinetics):\n",
    "https://pytorch.org/vision/stable/auto_examples/plot_video_api.html#sphx-glr-auto-examples-plot-video-api-py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "# torch.manual_seed(69)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "# import skvideo.io\n",
    "import time\n",
    "import random\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Imports \n",
    "# from lhotse import CutSet, RecordingSet, align_with_torchaudio, annotate_with_whisper\n",
    "# from tqdm import tqdma\n",
    "# from pprint import pprint\n",
    "# from dataclasses import asdict\n",
    "# import torch\n",
    "# from os import path\n",
    "# from pydub import AudioSegment\n",
    "# import sys\n",
    "# sys.path.append('../../video-pretrained-transformer/')\n",
    "from whisper_audio.CaptionPreprocessing import CaptionPreprocessing\n",
    "import subprocess\n",
    "import os\n",
    "# Pranav's CLIP Video Pre-Processing\n",
    "\n",
    "# We want to extract EXACTLY n frames from a video given a start and end time\n",
    "# We also want to use random sampling if possible (1-30 fps for now)\n",
    "# def get_frames(video_path, n_frames = 60, words = 15, threshold_words = 30)\n",
    "#   frame_rate = get_frame_rate(video_path)\n",
    "#   list_of_start_end_dict = CaptionPreprocessing(video_path)\n",
    "#   for start_end_dict in list_of_start_end_dict:\n",
    "#       start, end = start_end_dict['start'], start_end_dict['end']\n",
    "#       if end - start < n_frames/frame_rate:\n",
    "#           return ERROR\n",
    "#       Get random frames\n",
    "\n",
    "class YoutubeDataPreprocessor:\n",
    "    def __init__(self, data_path, extension=\".mp4\", debug=False):\n",
    "        self.data_path = data_path\n",
    "        self.extension = extension\n",
    "        self.debug = debug\n",
    "        self.caption_preprocessor = CaptionPreprocessing()\n",
    "\n",
    "    def write_resampled_video(self, video_reader, resample_name, frames_per_partition=64, num_partitions=5, frame_separation=3, extension=\".mp4\"):\n",
    "        num_frames, height, width, channels = video_reader.getShape()\n",
    "\n",
    "        num_frames_processed_per_partition = (frames_per_partition-1)*frame_separation+frames_per_partition\n",
    "        partitions = []\n",
    "        \n",
    "        all_frames = set([i for i in range(num_frames-num_frames_processed_per_partition-1)])\n",
    "\n",
    "        for partition in range(num_partitions):\n",
    "            retry = 0\n",
    "            no_partition_flag = False\n",
    "            while True:\n",
    "                if retry == 5:\n",
    "                    no_partition_flag = True\n",
    "                    break\n",
    "\n",
    "                # random_start_idx = random.chouce(0, num_frames-num_frames_processed_per_partition-1)\n",
    "\n",
    "                # TODO: ADD IN CHECK FOR NUMBER OF WORDS IN SEGMENT\n",
    "                random_start_idx = random.choice(list(all_frames))\n",
    "                if True:\n",
    "                    partition = set([i for i in range(random_start_idx, random_start_idx+num_frames_processed_per_partition)])\n",
    "                    all_frames = all_frames - partition\n",
    "                    break\n",
    "\n",
    "                retry += 1\n",
    "            \n",
    "            if no_partition_flag:\n",
    "                break\n",
    "\n",
    "            frames_in_partition = []\n",
    "            curr_frame = random_start_idx\n",
    "\n",
    "            frames_in_partition = list(range(random_start_idx, random_start_idx+num_frames_processed_per_partition, frame_separation+1))\n",
    "\n",
    "            assert len(frames_in_partition) == frames_per_partition\n",
    "            \n",
    "            partitions.append(frames_in_partition)\n",
    "        \n",
    "        if len(partitions) < num_partitions:\n",
    "            print(f\"[WARNING] {resample_name} has less than {num_partitions} (number of partitions: {len(partitions)})\")\n",
    "\n",
    "        partitions = sorted(partitions, key=itemgetter(0))\n",
    "\n",
    "        writer = None\n",
    "        curr_partition_end = 0\n",
    "        curr_frame = 0\n",
    "        frames_to_extract = []\n",
    "        partition_num = 0\n",
    "\n",
    "        curr_partition = 0\n",
    "        writer = skvideo.io.FFmpegWriter(f\"{resample_name}_{curr_partition}{extension}\")\n",
    "\n",
    "        for frame in video_reader.nextFrame():\n",
    "            if curr_frame >= partitions[curr_partition][-1]:\n",
    "                if writer is not None:\n",
    "                    # print(\"closedd....\")\n",
    "                    writer.close()\n",
    "                    writer = None\n",
    "\n",
    "                curr_partition += 1\n",
    "                writer = skvideo.io.FFmpegWriter(f\"{resample_name}_{curr_partition}{extension}\")\n",
    "\n",
    "                if self.debug:\n",
    "                    print(f\"{resample_name}_{curr_partition}{extension}\")\n",
    "\n",
    "            if curr_partition >= len(partitions):\n",
    "                # print(\"broken\")\n",
    "                break\n",
    "            \n",
    "            if curr_frame in partitions[curr_partition]:\n",
    "                # print(\"hit\")\n",
    "                writer.writeFrame(frame)\n",
    "                \n",
    "            curr_frame += 1\n",
    "\n",
    "\n",
    "        # if writer is not None:\n",
    "        #     writer.close()\n",
    "        \n",
    "        return\n",
    "\n",
    "    def process_video_old(self, video_name):\n",
    "        video_reader = skvideo.io.FFmpegReader(video_name)\n",
    "        start = time.time()\n",
    "        self.write_resampled_video(video_reader, f\"./test\", frames_per_partition=64, num_partitions=5, frame_separation=3, extension=\".mp4\")\n",
    "        end = time.time() - start\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[INFO] video took {end} s for resampling\")\n",
    "\n",
    "        video_reader.close()\n",
    "\n",
    "    # def get_frames(video_path, n_frames = 60, words = 15, threshold_words = 30)\n",
    "    #   frame_rate = get_frame_rate(video_path)\n",
    "    #   list_of_start_end_dict = CaptionPreprocessing(video_path)\n",
    "    #   for start_end_dict in list_of_start_end_dict:\n",
    "    #       start, end = start_end_dict['start'], start_end_dict['end']\n",
    "    #       if end - start < n_frames/frame_rate:\n",
    "    #           return ERROR\n",
    "    #       Get random frames\n",
    "\n",
    "    def get_frame_segment(self, video_name, segment, segment_start, segment_end):\n",
    "        ''' start here '''\n",
    "        print(video_name)\n",
    "        cap = cv2.VideoCapture(video_name)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        frame_width = int(cap.get(3))\n",
    "        frame_height = int(cap.get(4))\n",
    "        \n",
    "        size = (frame_width, frame_height)\n",
    "   \n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        result = cv2.VideoWriter('./segment_video.mp4', fourcc, fps, size)\n",
    "\n",
    "        timestamps = [cap.get(cv2.CAP_PROP_POS_MSEC)]\n",
    "        curr_timestamp = 0.0\n",
    "\n",
    "        frames = []\n",
    "        cnt = 0\n",
    "\n",
    "        while(cap.isOpened()):\n",
    "            cnt += 1\n",
    "            # print(\"function called\")\n",
    "            frame_exists, curr_frame = cap.read()\n",
    "            \n",
    "            if frame_exists:\n",
    "                curr_timestamp = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "                timestamps.append(curr_timestamp)\n",
    "\n",
    "                # ensure curr_frame is within curr_segment\n",
    "                if (curr_timestamp/1000.0) > segment_start and (curr_timestamp/1000.0) < segment_end:\n",
    "                    cv2.imwrite(f\"./segment_videos/frame_{cnt}.png\", curr_frame)\n",
    "                    result.write(curr_frame)\n",
    "\n",
    "                    # todo: make python list of CV2 frames per segment.\n",
    "                    # todo: Add clip model for embeddings. \n",
    "                    \n",
    "                    # result.write(curr_frame)\n",
    "                    # cv2.imwrite(f\"frame_{curr_timestamp}\", curr_frame)\n",
    "                    # cv2.imshow(\"test\", curr_frame)\n",
    "                    # cv2.waitKey(0)\n",
    "                    # cv2.destroyAllWindows()\n",
    "            else:\n",
    "                break\n",
    "          {\n",
    "            \"youtube_name\": \"2ZokZgnjrYs\",                      \n",
    "            \"filename\": video_name,                      \n",
    "            \"youtube_id\": \"Hawaii Vlog\",                         \n",
    "            \"segment_length_seconds\": 15.0,\n",
    "            \"captions\": \"Daniel's chosen format here\",           \n",
    "            \"segment_start_time\":[<segment_start_times_list>],\n",
    "            \"segment_end_time\":[<segment_end_times_list>],     \n",
    "            \"frame_embeddings\":[<frame_embeddings_list>],        \n",
    "            \"audio_embeddings\":[<audio_embeddings_list>],        \n",
    "            \"text_caption_embeddings\":[<text_embeddings_list>],  \n",
    "            \"scene_graph_embeddings\":[<scene_graph_embeddings_list>],     \n",
    "            # \"labels\": [<labels_list>],                       \n",
    "        }\n",
    "\n",
    "        cap.release()\n",
    "        result.release()\n",
    "        print(min(timestamps), max(timestamps))\n",
    "        \n",
    "\n",
    "\n",
    "    def process_video(self, video_name):\n",
    "        self.caption_preprocessor.process_mp4(video_name)\n",
    "        \n",
    "        # return cuts_aligned\n",
    "        segment_timestamps = self.caption_preprocessor.get_segments_thresholded()\n",
    "        # print(segment_timestamps)\n",
    "        \n",
    "        for segment_timestamp in segment_timestamps:\n",
    "            self.get_frame_segment(video_name, segment_timestamp, segment_timestamp['start'], segment_timestamp['end'])\n",
    "\n",
    "\n",
    "       \n",
    "        return segment_timestamps\n",
    "    \n",
    "\n",
    "    def process_video_dataset(self):\n",
    "        for f in os.listdir(self.data_path):\n",
    "            if f.endswith(self.extension):\n",
    "                self.process_video(os.path.join(self.data_path, f))\n",
    "            else:\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"../../data/massive_youtube_data\"\n",
    "path = \"/home/kastan/thesis/data/simple_test_data\"\n",
    "data_preprocessor = YoutubeDataPreprocessor(path)\n",
    "test_file = '/home/kastan/thesis/data/simple_test_data/rick_roll.mp4'\n",
    "# test_file = \"/home/kastan/thesis/data/whisper_directory/Rick Astley - Never Gonna Give You Up (Official Music Video).mp4\"\n",
    "\n",
    "# import cProfile\n",
    "# cProfile.run('data_preprocessor.process_video(test_file)')\n",
    "\n",
    "result = data_preprocessor.process_video(test_file)\n",
    "# print(result)\n",
    "# data_preprocessor.process_video(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning audio files (*.wav): 1it [00:00, 889.38it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "We don't support forced alignment of cuts with overlapping supervisions (cut ID: 'rick_roll')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# test_file = \"/home/kastan/thesis/data/whisper_directory/Rick Astley - Never Gonna Give You Up (Official Music Video).mp4\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# import cProfile\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# cProfile.run('data_preprocessor.process_video(test_file)')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m result \u001b[39m=\u001b[39m data_preprocessor\u001b[39m.\u001b[39;49mprocess_video(test_file)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print(result)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# data_preprocessor.process_video(test_file)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(i)\n",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb Cell 5\u001b[0m in \u001b[0;36mYoutubeDataPreprocessor.process_video\u001b[0;34m(self, video_name)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=182'>183</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcaption_preprocessor\u001b[39m.\u001b[39mprocess_mp4(video_name)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=185'>186</a>\u001b[0m \u001b[39m# return cuts_aligned\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=186'>187</a>\u001b[0m segment_timestamps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcaption_preprocessor\u001b[39m.\u001b[39;49mget_segments_thresholded()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=187'>188</a>\u001b[0m \u001b[39m# print(segment_timestamps)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=188'>189</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=189'>190</a>\u001b[0m \u001b[39m# self.get_frame_segment(video_name, segment_timestamps[0]['start'], segment_timestamps[0]['end'])\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=193'>194</a>\u001b[0m \u001b[39mreturn\u001b[39;00m segment_timestamps\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/data_preprocessing/whisper_audio/CaptionPreprocessing.py:78\u001b[0m, in \u001b[0;36mCaptionPreprocessing.get_segments_thresholded\u001b[0;34m(self, time, threshold)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m time_dict_list\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcut:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcut \u001b[39m=\u001b[39m get_cut(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmp3_path)\n\u001b[1;32m     80\u001b[0m time_dict_list \u001b[39m=\u001b[39m to_time_dict()\n\u001b[1;32m     81\u001b[0m curr_dict_list \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/data_preprocessing/whisper_audio/CaptionPreprocessing.py:57\u001b[0m, in \u001b[0;36mCaptionPreprocessing.get_segments_thresholded.<locals>.get_cut\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     55\u001b[0m     cuts_aligned \u001b[39m=\u001b[39m align_with_torchaudio(cuts)\n\u001b[1;32m     56\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mfor\u001b[39;00m cut \u001b[39min\u001b[39;00m cuts_aligned:\n\u001b[1;32m     58\u001b[0m         \u001b[39mreturn\u001b[39;00m asdict(cut)\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/lhotse/workflows/forced_alignment.py:60\u001b[0m, in \u001b[0;36malign_with_torchaudio\u001b[0;34m(cuts, bundle_name, device, normalize_text)\u001b[0m\n\u001b[1;32m     57\u001b[0m discard_symbols \u001b[39m=\u001b[39m _make_discard_symbols_regex(labels)\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m cut \u001b[39min\u001b[39;00m cuts:\n\u001b[0;32m---> 60\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m cut\u001b[39m.\u001b[39mhas_overlapping_supervisions, (\n\u001b[1;32m     61\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt support forced alignment of cuts with overlapping supervisions \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(cut ID: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcut\u001b[39m.\u001b[39mid\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     65\u001b[0m     \u001b[39mfor\u001b[39;00m idx, subcut \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(cut\u001b[39m.\u001b[39mtrim_to_supervisions(keep_overlapping\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)):\n\u001b[1;32m     66\u001b[0m         sup \u001b[39m=\u001b[39m subcut\u001b[39m.\u001b[39msupervisions[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: We don't support forced alignment of cuts with overlapping supervisions (cut ID: 'rick_roll')"
     ]
    }
   ],
   "source": [
    "# Danny test cell\n",
    "for i in range(30):\n",
    "    # path = \"../../data/massive_youtube_data\"\n",
    "    \n",
    "    path = \"/home/kastan/thesis/data/simple_test_data\"\n",
    "    data_preprocessor = YoutubeDataPreprocessor(path)\n",
    "    test_file = '/home/kastan/thesis/data/simple_test_data/rick_roll.mp4'\n",
    "    # test_file = \"/home/kastan/thesis/data/whisper_directory/Rick Astley - Never Gonna Give You Up (Official Music Video).mp4\"\n",
    "\n",
    "    # import cProfile\n",
    "    # cProfile.run('data_preprocessor.process_video(test_file)')\n",
    "    result = data_preprocessor.process_video(test_file)\n",
    "    # print(result)\n",
    "    # data_preprocessor.process_video(test_file)\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kastan/thesis/data/simple_test_data/rick_roll.mp4\n",
      "0.0 211560.0\n"
     ]
    }
   ],
   "source": [
    "data_preprocessor = YoutubeDataPreprocessor(path)\n",
    "data_preprocessor.get_frame_segment('/home/kastan/thesis/data/simple_test_data/rick_roll.mp4', result[0]['start'], result[0]['end'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:(Pranav, Kastan) Define Dataset class\n",
    "\n",
    "This doesn't work yet, it's just examples from the docs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPT_Dataset(Dataset):\n",
    "    \"\"\"TODO: Adapt this data class.\n",
    "    This doesn't work yet, it's just examples from the docs!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        # TODO: Adapt this\n",
    "        Args: \n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        # self.root_dir = root_dir\n",
    "        self.video_file = []\n",
    "        self.img_embeddings = []\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # todo: adapt this to our data\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset transformations (using Compose)\n",
    "\n",
    "### Transformations used in X-CLIP\n",
    "\n",
    "We could probably steal their implementation, too: https://github.com/microsoft/VideoX/blob/master/X-CLIP/datasets/rand_augment.py\n",
    "\n",
    "```python\n",
    "_RAND_CHOICE_WEIGHTS_0 = {\n",
    "    \"Rotate\": 0.3,\n",
    "    \"ShearX\": 0.2,\n",
    "    \"ShearY\": 0.2,\n",
    "    \"TranslateXRel\": 0.1,\n",
    "    \"TranslateYRel\": 0.1,\n",
    "    \"Color\": 0.025,\n",
    "    \"Sharpness\": 0.025,\n",
    "    \"AutoContrast\": 0.025,\n",
    "    \"Solarize\": 0.005,\n",
    "    \"SolarizeAdd\": 0.005,\n",
    "    \"Contrast\": 0.005,\n",
    "    \"Brightness\": 0.005,\n",
    "    \"Equalize\": 0.005,\n",
    "    \"Posterize\": 0,\n",
    "    \"Invert\": 0,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchscript (torch.jit.script) an optimizing JIT runtime compiler for PyTorch. \n",
    "# Compiled to C++, faster. I've read data augmentation is CPU-intensive, so this might help.\n",
    "\n",
    "# 📜 ⭐️ Docs on all transforms: https://pytorch.org/vision/stable/transforms.html \n",
    "\n",
    "# Kastan's suggestions (helped by Copilot): \n",
    "# RandomRotation(degrees[, interpolation, …])\n",
    "# RandomHorizontalFlip(p=0.5)\n",
    "# RandomVerticalFlip(p=0.5)\n",
    "# RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)\n",
    "# RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')\n",
    "# RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)\n",
    "# RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3, fill=0)\n",
    "# RandomApply(transforms, p=0.5)\n",
    "# RandomChoice(transforms)\n",
    "# RandomOrder(transforms) \n",
    "# ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)\n",
    "# Grayscale(num_output_channels=1)\n",
    "# Pad(padding, fill=0, padding_mode='constant')\n",
    "# LinearTransformation(transformation_matrix, mean_vector)\n",
    "# Normalize(mean, std, inplace=False)\n",
    "# Resize(size, interpolation=2)\n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    transforms.CenterCrop(10),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "video_transforms = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:(Pranav, Kastan) Instantiate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = VPT_Dataset(\"./dataset\", epoch_size=None, transform=video_transforms)\n",
    "\n",
    "\n",
    "# TODO: This is just inspiration from the docs, pytorch datasets. THIS DOESN'T WORK yet\n",
    "loader = DataLoader(dataset, batch_size=12)\n",
    "data = {\"video\": [], 'start': [], 'end': [], 'tensorsize': []}\n",
    "for batch in loader:\n",
    "    for i in range(len(batch['path'])):\n",
    "        data['video'].append(batch['path'][i])\n",
    "        data['start'].append(batch['start'][i].item())\n",
    "        data['end'].append(batch['end'][i].item())\n",
    "        data['tensorsize'].append(batch['video'][i].size())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing captionpreprocessing\n",
    "process = CaptionPreprocessing()\n",
    "process.load_mp4_to_wav(\"/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/test_4.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/test_4.wav\"\n",
    "dir = '/'.join(path.split(\"/\")[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.get_segments_thresholded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing videoreader \n",
    "import skvideo.io\n",
    "video_name = '/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/parallel_0/zoyplKPGXr8_Bill Shuttic_546_Help For Stroke 10x10x10 Challenge.webm'\n",
    "video_reader = skvideo.io.FFmpegReader(video_name)\n",
    "\n",
    "for frame in video_reader.nextFrame():\n",
    "    display(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/parallel_0/zoyplKPGXr8_Bill Shuttic_546_Help For Stroke 10x10x10 Challenge.webm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'cv2.VideoCapture' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(video_name)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m fps \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mget(cv2\u001b[39m.\u001b[39mCAP_PROP_FPS)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m cap:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/data_preprocessing/Custom_PyTorch_Dataset.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m   display(c)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'cv2.VideoCapture' object is not iterable"
     ]
    }
   ],
   "source": [
    "# test cv2 videoloader\n",
    "import cv2\n",
    "print(video_name)\n",
    "cap = cv2.VideoCapture(video_name)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp_v2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "612b182cb4c3e0acfd877acc6c10f43d075b0ae43380d6b249d2d2b5490153b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
