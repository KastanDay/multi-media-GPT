{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' upload to hub\n",
    "from parallel_processing import deeplake_driver as dl_driver\n",
    "# import deeplake as dl\n",
    "\n",
    "BATCH_NAME             = 'parallel_15'\n",
    "# RESULTS_DATASET_PATH   = f'/mnt/storage_ssd/clip_OUTPUT_results_{BATCH_NAME}'\n",
    "RESULTS_DATASET_PATH   = f'~/VPT/v4_CLIP_encode_results_{BATCH_NAME}' # overdrive\n",
    "\n",
    "dl_driver.upload_dataset_to_hub(RESULTS_DATASET_PATH)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdl\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\n\u001b[1;32m      3\u001b[0m BATCH_NAME             \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mparallel_15\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/__init__.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mplatform \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdarwin\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     16\u001b[0m     multiprocessing\u001b[39m.\u001b[39mset_start_method(\u001b[39m\"\u001b[39m\u001b[39mfork\u001b[39m\u001b[39m\"\u001b[39m, force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset \u001b[39mas\u001b[39;00m api_dataset\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mread\u001b[39;00m \u001b[39mimport\u001b[39;00m read\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlink\u001b[39;00m \u001b[39mimport\u001b[39;00m link\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/api/dataset.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict, Optional, Union, List\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munstructured\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkaggle\u001b[39;00m \u001b[39mimport\u001b[39;00m download_kaggle_dataset\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munstructured\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage_classification\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageClassification\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m DeepLakeBackendClient\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlog\u001b[39;00m \u001b[39mimport\u001b[39;00m logger\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/auto/unstructured/image_classification.py:13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m ingestion_summary\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     InvalidPathException,\n\u001b[1;32m     11\u001b[0m     TensorInvalidSampleShapeError,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m UnstructuredDataset\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/core/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/core/storage/__init__.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39ms3\u001b[39;00m \u001b[39mimport\u001b[39;00m S3Provider\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgoogle_drive\u001b[39;00m \u001b[39mimport\u001b[39;00m GDriveProvider\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmemory\u001b[39;00m \u001b[39mimport\u001b[39;00m MemoryProvider\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlocal\u001b[39;00m \u001b[39mimport\u001b[39;00m LocalProvider\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlru_cache\u001b[39;00m \u001b[39mimport\u001b[39;00m LRUCache\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/core/storage/memory.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Dict\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlru_cache\u001b[39;00m \u001b[39mimport\u001b[39;00m _get_nbytes\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprovider\u001b[39;00m \u001b[39mimport\u001b[39;00m StorageProvider\n\u001b[1;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMemoryProvider\u001b[39;00m(StorageProvider):\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/core/storage/lru_cache.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpartial_reader\u001b[39;00m \u001b[39mimport\u001b[39;00m PartialReader\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeeplake_memory_object\u001b[39;00m \u001b[39mimport\u001b[39;00m DeepLakeMemoryObject\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchunk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_chunk\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseChunk\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Dict, Optional, Union\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprovider\u001b[39;00m \u001b[39mimport\u001b[39;00m StorageProvider\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/core/chunk/base_chunk.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfast_forwarding\u001b[39;00m \u001b[39mimport\u001b[39;00m ffw_chunk\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinked_sample\u001b[39;00m \u001b[39mimport\u001b[39;00m LinkedSample\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmeta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mencode\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbyte_positions\u001b[39;00m \u001b[39mimport\u001b[39;00m BytePositionsEncoder\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmeta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mencode\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mshape\u001b[39;00m \u001b[39mimport\u001b[39;00m ShapeEncoder\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmeta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensor_meta\u001b[39;00m \u001b[39mimport\u001b[39;00m TensorMeta\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/core/meta/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmeta\u001b[39;00m \u001b[39mimport\u001b[39;00m Meta\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdataset_meta\u001b[39;00m \u001b[39mimport\u001b[39;00m DatasetMeta\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtensor_meta\u001b[39;00m \u001b[39mimport\u001b[39;00m TensorMeta\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/core/meta/tensor_meta.py:16\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     TensorMetaInvalidHtype,\n\u001b[1;32m      8\u001b[0m     TensorMetaInvalidHtypeOverwriteValue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     InvalidTensorLinkError,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjson\u001b[39;00m \u001b[39mimport\u001b[39;00m validate_json_schema\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     REQUIRE_USER_SPECIFICATION,\n\u001b[1;32m     19\u001b[0m     UNSPECIFIED,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     COMPRESSION_ALIASES,\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/util/json.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbase64\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msample\u001b[39;00m \u001b[39mimport\u001b[39;00m Sample  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m      8\u001b[0m Schema \u001b[39m=\u001b[39m Any\n\u001b[1;32m     11\u001b[0m scalars \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbool\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdict\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSample\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/core/sample.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     compress_array,\n\u001b[1;32m      4\u001b[0m     decompress_array,\n\u001b[1;32m      5\u001b[0m     verify_compressed_file,\n\u001b[1;32m      6\u001b[0m     read_meta_from_compressed_file,\n\u001b[1;32m      7\u001b[0m     get_compression,\n\u001b[1;32m      8\u001b[0m     _open_video,\n\u001b[1;32m      9\u001b[0m     _read_metadata_from_vstream,\n\u001b[1;32m     10\u001b[0m     _read_audio_meta,\n\u001b[1;32m     11\u001b[0m     _read_3d_data_meta,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     get_compression_type,\n\u001b[1;32m     15\u001b[0m     AUDIO_COMPRESSION,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     MESH_COMPRESSION,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m UnableToReadFromUrlError\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/core/compression.py:9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     SampleCompressionError,\n\u001b[1;32m      5\u001b[0m     SampleDecompressionError,\n\u001b[1;32m      6\u001b[0m     UnsupportedCompressionError,\n\u001b[1;32m      7\u001b[0m     CorruptedSampleError,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mobject_3d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mread_3d_data\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     read_3d_data,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     get_compression_type,\n\u001b[1;32m     14\u001b[0m     BYTE_COMPRESSION,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     MESH_COMPRESSION,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Union, Tuple, Sequence, List, Optional, BinaryIO\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/util/object_3d/read_3d_data.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m UnsupportedExtensionError\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mobject_3d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpoint_cloud\u001b[39;00m \u001b[39mimport\u001b[39;00m PointCloudLas\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mobject_3d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmesh\u001b[39;00m \u001b[39mimport\u001b[39;00m MeshPly\n\u001b[1;32m     16\u001b[0m _POINT_CLOUD_EXTENSIONS_TO_CLASS \u001b[39m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m.las\u001b[39m\u001b[39m\"\u001b[39m: PointCloudLas,\n\u001b[1;32m     18\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m.ply\u001b[39m\u001b[39m\"\u001b[39m: MeshPly,\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_3d_data\u001b[39m(point_cloud_path):\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/util/object_3d/mesh.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpath\u001b[39;00m \u001b[39mimport\u001b[39;00m convert_pathlib_to_string_if_needed\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mobject_3d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mobject_base_3d\u001b[39;00m \u001b[39mimport\u001b[39;00m ObjectBase3D\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mobject_3d\u001b[39;00m \u001b[39mimport\u001b[39;00m mesh_parser, mesh_reader\n\u001b[1;32m     12\u001b[0m sys_byteorder \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m<\u001b[39m\u001b[39m\"\u001b[39m)[sys\u001b[39m.\u001b[39mbyteorder \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlittle\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     14\u001b[0m _valid_formats \u001b[39m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbinary_big_endian\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbinary_little_endian\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m<\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m }\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/util/object_3d/mesh_reader.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mobject_3d\u001b[39;00m \u001b[39mimport\u001b[39;00m ply_reader\n\u001b[1;32m      4\u001b[0m MESH_EXTENSION_TO_MESH_READER \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mply\u001b[39m\u001b[39m\"\u001b[39m: ply_reader\u001b[39m.\u001b[39mPlyReader}\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_mesh_reader\u001b[39m(ext):\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/util/object_3d/ply_reader.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DynamicTensorNumpyError  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mobject_3d\u001b[39;00m \u001b[39mimport\u001b[39;00m ply_readers\n\u001b[1;32m      4\u001b[0m PLY_FORMAT_TO_PLY_READER \u001b[39m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m\"\u001b[39m: ply_readers\u001b[39m.\u001b[39mPlyASCIIReader,\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mascii_with_normals\u001b[39m\u001b[39m\"\u001b[39m: ply_readers\u001b[39m.\u001b[39mPlyASCIIWithNormalsReader,\n\u001b[1;32m      7\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbinary_little_endian\u001b[39m\u001b[39m\"\u001b[39m: ply_readers\u001b[39m.\u001b[39mPlyBinReader,\n\u001b[1;32m      8\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbinary_big_endian\u001b[39m\u001b[39m\"\u001b[39m: ply_readers\u001b[39m.\u001b[39mPlyBinReader,\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPlyReader\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/transformers_rec2/lib/python3.8/site-packages/deeplake/util/object_3d/ply_readers.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m BytesIO, StringIO\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DynamicTensorNumpyError  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mobject_3d\u001b[39;00m \u001b[39mimport\u001b[39;00m ply_reader_base\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import deeplake as dl\n",
    "import tqdm\n",
    "BATCH_NAME             = 'parallel_15'\n",
    "# new_name   = f'/mnt/storage_ssd/dec_23_clip_results_{BATCH_NAME}'\n",
    "# new_name   = f'/mnt/storage_ssd/v3_text_encode_results_{BATCH_NAME}'\n",
    "# new_name   = f'/mnt/storage_hdd/thesis/yt_1b_dataset/backups/v3_text_encode_results_{BATCH_N/AME}'\n",
    "# new_name   = f'/mnt/storage_hdd/thesis/yt_1b_dataset/backups/no_compression_whisper_results_{BATCH_NAME}'\n",
    "# new_name   = f'/mnt/storage_hdd/thesis/yt_1b_dataset/backups/v3_text_encode_results_{BATCH_NAME}'\n",
    "# new_name   = f'/mnt/storage_ssd/dec_26_whisper_results_parallel_15'\n",
    "RESULTS_DATASET_PATH   = f'~/VPT/v4_CLIP_encode_results_{BATCH_NAME}' # overdrive\n",
    "# RESULTS_DATASET_PATH = f'/mnt/storage_ssd/v3_CLIP_encode_results_{BATCH_NAME}'  # ssd\n",
    "ds = dl.load(RESULTS_DATASET_PATH)\n",
    "print(ds.size_approx() / 1e9, \"GB\")\n",
    "ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segment_index': 0,\n",
       " 'total_segments': 490,\n",
       " 'captions': 'HELLO EVERYBODY AND WELCOME BACK TO MY SECOND CHANNEL I KNOW IT HAS BEEN A',\n",
       " 'start': 0.320875,\n",
       " 'end': 3.8108750000000002}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(ds.segment_metadata[0].data()['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "506889it [01:06, 7595.95it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m idx, sample \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(\u001b[39menumerate\u001b[39m(ds)):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# sample['text'] = sample['text'].decode('utf-8')\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     metadata \u001b[39m=\u001b[39m sample[\u001b[39m'\u001b[39;49m\u001b[39msegment_metadata\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mdata()[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m metadata:\n\u001b[1;32m      6\u001b[0m       \u001b[39mprint\u001b[39m(metadata)\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/tensor.py:850\u001b[0m, in \u001b[0;36mTensor.data\u001b[0;34m(self, aslist, fetch_chunks)\u001b[0m\n\u001b[1;32m    848\u001b[0m htype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_htype\n\u001b[1;32m    849\u001b[0m \u001b[39mif\u001b[39;00m htype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 850\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext(fetch_chunks\u001b[39m=\u001b[39;49mfetch_chunks)}\n\u001b[1;32m    851\u001b[0m \u001b[39mif\u001b[39;00m htype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    852\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdict(fetch_chunks\u001b[39m=\u001b[39mfetch_chunks)}\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/tensor.py:1209\u001b[0m, in \u001b[0;36mTensor.text\u001b[0;34m(self, fetch_chunks)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtext\u001b[39m(\u001b[39mself\u001b[39m, fetch_chunks: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1208\u001b[0m     \u001b[39m\"\"\"Return text data. Only applicable for tensors with 'text' base htype.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_value(\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m, fetch_chunks\u001b[39m=\u001b[39;49mfetch_chunks)\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/tensor.py:1203\u001b[0m, in \u001b[0;36mTensor._extract_value\u001b[0;34m(self, htype, fetch_chunks)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOnly supported for \u001b[39m\u001b[39m{\u001b[39;00mhtype\u001b[39m}\u001b[39;00m\u001b[39m tensors.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1202\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy(fetch_chunks\u001b[39m=\u001b[39;49mfetch_chunks)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1205\u001b[0m     \u001b[39mreturn\u001b[39;00m [sample[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy(aslist\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)]\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/tensor.py:730\u001b[0m, in \u001b[0;36mTensor.numpy\u001b[0;34m(self, aslist, fetch_chunks)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\n\u001b[1;32m    706\u001b[0m     \u001b[39mself\u001b[39m, aslist\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, fetch_chunks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    707\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[np\u001b[39m.\u001b[39mndarray, List[np\u001b[39m.\u001b[39mndarray]]:\n\u001b[1;32m    708\u001b[0m     \u001b[39m\"\"\"Computes the contents of the tensor in numpy format.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[39m        For tensors of htype ``polygon``, aslist is always ``True``.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 730\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_engine\u001b[39m.\u001b[39;49mnumpy(\n\u001b[1;32m    731\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex,\n\u001b[1;32m    732\u001b[0m         aslist\u001b[39m=\u001b[39;49maslist,\n\u001b[1;32m    733\u001b[0m         fetch_chunks\u001b[39m=\u001b[39;49mfetch_chunks \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_iteration,\n\u001b[1;32m    734\u001b[0m         pad_tensor\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_tensor,\n\u001b[1;32m    735\u001b[0m     )\n\u001b[1;32m    736\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpoint_cloud\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# TODO: refactor\u001b[39;00m\n\u001b[1;32m    737\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/chunk_engine.py:1587\u001b[0m, in \u001b[0;36mChunkEngine.numpy\u001b[0;34m(self, index, aslist, use_data_cache, fetch_chunks, pad_tensor)\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_link_ready()\n\u001b[1;32m   1586\u001b[0m fetch_chunks \u001b[39m=\u001b[39m fetch_chunks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_full_chunk(index)\n\u001b[0;32m-> 1587\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sequence_numpy \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_sequence \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy)(\n\u001b[1;32m   1588\u001b[0m     index, aslist, use_data_cache, fetch_chunks, pad_tensor\n\u001b[1;32m   1589\u001b[0m )\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/chunk_engine.py:1751\u001b[0m, in \u001b[0;36mChunkEngine._numpy\u001b[0;34m(self, index, aslist, use_data_cache, fetch_chunks, pad_tensor)\u001b[0m\n\u001b[1;32m   1749\u001b[0m samples \u001b[39m=\u001b[39m []\n\u001b[1;32m   1750\u001b[0m \u001b[39mfor\u001b[39;00m global_sample_index \u001b[39min\u001b[39;00m index\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindices(length):\n\u001b[0;32m-> 1751\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_single_sample(\n\u001b[1;32m   1752\u001b[0m         global_sample_index,\n\u001b[1;32m   1753\u001b[0m         index,\n\u001b[1;32m   1754\u001b[0m         fetch_chunks\u001b[39m=\u001b[39;49mfetch_chunks,\n\u001b[1;32m   1755\u001b[0m         pad_tensor\u001b[39m=\u001b[39;49mpad_tensor,\n\u001b[1;32m   1756\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     check_sample_shape(sample\u001b[39m.\u001b[39mshape, last_shape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey, index, aslist)\n\u001b[1;32m   1758\u001b[0m     last_shape \u001b[39m=\u001b[39m sample\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/chunk_engine.py:1705\u001b[0m, in \u001b[0;36mChunkEngine.get_single_sample\u001b[0;34m(self, global_sample_index, index, fetch_chunks, pad_tensor)\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[39mreturn\u001b[39;00m sample\n\u001b[1;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_tiled_sample(global_sample_index):\n\u001b[0;32m-> 1705\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_non_tiled_sample(\n\u001b[1;32m   1706\u001b[0m         global_sample_index, index, fetch_chunks\u001b[39m=\u001b[39;49mfetch_chunks\n\u001b[1;32m   1707\u001b[0m     )\n\u001b[1;32m   1708\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(index\u001b[39m.\u001b[39mvalues) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1709\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_tiled_sample(global_sample_index)\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/chunk_engine.py:1664\u001b[0m, in \u001b[0;36mChunkEngine.get_non_tiled_sample\u001b[0;34m(self, global_sample_index, index, fetch_chunks)\u001b[0m\n\u001b[1;32m   1662\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_video:\n\u001b[1;32m   1663\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_video_sample(global_sample_index, index)\n\u001b[0;32m-> 1664\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_basic_sample(\n\u001b[1;32m   1665\u001b[0m     global_sample_index, index, fetch_chunks\u001b[39m=\u001b[39;49mfetch_chunks\n\u001b[1;32m   1666\u001b[0m )\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/chunk_engine.py:1646\u001b[0m, in \u001b[0;36mChunkEngine.get_basic_sample\u001b[0;34m(self, global_sample_index, index, fetch_chunks)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_basic_sample\u001b[39m(\u001b[39mself\u001b[39m, global_sample_index, index, fetch_chunks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1645\u001b[0m     enc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_id_encoder\n\u001b[0;32m-> 1646\u001b[0m     chunk_id, row, worst_case_header_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_chunk_info(\n\u001b[1;32m   1647\u001b[0m         global_sample_index, fetch_chunks\n\u001b[1;32m   1648\u001b[0m     )\n\u001b[1;32m   1649\u001b[0m     local_sample_index \u001b[39m=\u001b[39m enc\u001b[39m.\u001b[39mtranslate_index_relative_to_chunks(global_sample_index)\n\u001b[1;32m   1650\u001b[0m     chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_chunk_from_chunk_id(\n\u001b[1;32m   1651\u001b[0m         chunk_id, partial_chunk_bytes\u001b[39m=\u001b[39mworst_case_header_size\n\u001b[1;32m   1652\u001b[0m     )\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/chunk_engine.py:1610\u001b[0m, in \u001b[0;36mChunkEngine.get_chunk_info\u001b[0;34m(self, global_sample_index, fetch_chunks)\u001b[0m\n\u001b[1;32m   1608\u001b[0m \u001b[39m\"\"\"Returns the chunk_id, row and worst case header size of chunk containing the given sample.\"\"\"\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m enc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_id_encoder\n\u001b[0;32m-> 1610\u001b[0m out \u001b[39m=\u001b[39m enc\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(global_sample_index, return_row_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1611\u001b[0m chunk_id, row \u001b[39m=\u001b[39m out[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], out[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[1;32m   1613\u001b[0m worst_case_header_size \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/meta/encode/chunk_id.py:253\u001b[0m, in \u001b[0;36mChunkIdEncoder.__getitem__\u001b[0;34m(self, local_sample_index, return_row_index)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mif\u001b[39;00m local_sample_index \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    251\u001b[0m     local_sample_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples\n\u001b[0;32m--> 253\u001b[0m row_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtranslate_index(local_sample_index)\n\u001b[1;32m    254\u001b[0m output: List[Any] \u001b[39m=\u001b[39m []\n\u001b[1;32m    255\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_derive_value(\n\u001b[1;32m    256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encoded[row_index], row_index, local_sample_index\n\u001b[1;32m    257\u001b[0m )\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/meta/encode/base_encoder.py:140\u001b[0m, in \u001b[0;36mEncoder.translate_index\u001b[0;34m(self, local_sample_index)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m local_sample_index \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    138\u001b[0m     local_sample_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples\n\u001b[0;32m--> 140\u001b[0m row_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_last_row(local_sample_index)\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m row_index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     row_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msearchsorted(\n\u001b[1;32m    143\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encoded[:, LAST_SEEN_INDEX_COLUMN], local_sample_index\n\u001b[1;32m    144\u001b[0m     )\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/meta/encode/base_encoder.py:24\u001b[0m, in \u001b[0;36mEncoder.check_last_row\u001b[0;34m(self, global_sample_index)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m\"\"\"Takes a look at self.last_row and tries to find chunk id without binary search by looking at the current and next row.\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encoded\n\u001b[0;32m---> 24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_row \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(arr) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_index_in_last_row(\n\u001b[1;32m     25\u001b[0m     arr, global_sample_index\n\u001b[1;32m     26\u001b[0m ):\n\u001b[1;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_row\n\u001b[1;32m     28\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_row \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(arr) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/deeplake/core/meta/encode/base_encoder.py:19\u001b[0m, in \u001b[0;36mEncoder.is_index_in_last_row\u001b[0;34m(self, arr, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m\"\"\"Checks if `index` is in the self.last_row of of encoder.\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_row\n\u001b[0;32m---> 19\u001b[0m \u001b[39mreturn\u001b[39;00m arr[row, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m index \u001b[39mand\u001b[39;00m (row \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m arr[row \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m index)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, sample in tqdm.tqdm(enumerate(ds)):\n",
    "    # sample['text'] = sample['text'].decode('utf-8')\n",
    "    metadata = sample['segment_metadata'].data()['value']\n",
    "    \n",
    "    if metadata:\n",
    "      print(metadata)\n",
    "      print(\"found something ^^^^^^^^^^\")\n",
    "      break\n",
    "    # if idx >= 500:\n",
    "      # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import deeplake as dl\n",
    "import tqdm\n",
    "dataset_name = '/mnt/storage_ssd/v2_text_encode_results_parallel_15'\n",
    "# dataset_name = '/mnt/storage_ssd/v2_whisper_results_parallel_15'\n",
    "# dataset_name = '/mnt/storage_ssd/problem_file_only_whisper_results_parallel_15'\n",
    "ds = deeplake.load(dataset_name)\n",
    "print(ds.size_approx() / 1e9, \"GB\")\n",
    "ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = '/mnt/storage_ssd/text_encode_results_parallel_15'\n",
    "ds = deeplake.load(dataset_name)\n",
    "print(ds.size_approx() / 1e9, \"GB\")\n",
    "ds.summary()\n",
    "print(type(ds.caption_embedding[0].data()['value']))\n",
    "print(ds.caption_embedding[0].data()['value'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a backup\n",
    "# deeplake.deepcopy(dataset_name_v2, '/mnt/storage_ssd/BACKUP_no_compression_whisper_results_parallel_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_ds[19422].caption_embedding.numpy()\n",
    "new_ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CONVERT dataset without compression'''\n",
    "import numpy as np\n",
    "old_ds_path = '/mnt/storage_ssd/text_encode_results_parallel_15'\n",
    "old_ds = deeplake.load(old_ds_path)\n",
    "\n",
    "new_ds_path = '/mnt/storage_ssd/fresh_text_encode_parallel_15'\n",
    "new_ds = deeplake.empty(new_ds_path, overwrite=True)\n",
    "\n",
    "with new_ds:\n",
    "  new_ds.create_tensor('caption', htype='text', dtype=str, sample_compression=None) # todo: change to chunk_compression\n",
    "  new_ds.create_tensor('video_filename', htype='text', dtype=str, sample_compression=None)\n",
    "  new_ds.create_tensor('video_filepath', htype='text', dtype=str, sample_compression=None)\n",
    "  new_ds.create_tensor('segment_metadata', htype='json', sample_compression=None)\n",
    "  new_ds.create_tensor('caption_embedding', htype='image', dtype=np.float16, sample_compression='lz4')\n",
    "\n",
    "  for idx, sample in tqdm.tqdm(enumerate(old_ds)):\n",
    "    try:\n",
    "      new_ds.caption.append( sample.caption.data()['value']   )\n",
    "      new_ds.video_filename.append( sample.video_filename.data()['value']   )\n",
    "      new_ds.video_filepath.append( sample.video_filepath.data()['value']   )\n",
    "      new_ds.segment_metadata.append( sample.segment_metadata.data()['value']   )\n",
    "      new_ds.caption_embedding.append( sample.caption_embedding.data()['value']   )\n",
    "    except Exception as e:\n",
    "      print(\"error at idx:\", idx, e)\n",
    "new_ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.tensors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = '/mnt/storage_ssd/v2_whisper_results_parallel_15'\n",
    "ds_copy = deeplake.empty(dataset_name, overwrite=True)\n",
    "ds = ds_copy_var_only\n",
    "ds.flush()\n",
    "ds.size_approx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = '/mnt/storage_ssd/whisper_results_parallel_15'\n",
    "loaded = deeplake.load(dataset_name)\n",
    "loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeplake.copy(ds_copy, '/mnt/storage_ssd/v3_whisper_results_parallel_15', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jus looking\n",
    "ds_copy_var_only = ds[:16667]\n",
    "ds_copy_var_only.summary()\n",
    "\n",
    "# metadata = sample.segment_metadata.data()['value']\n",
    "metadata = ds_copy_var_only[16667].segment_metadata.data()['value']\n",
    "print(metadata['segment_index'], \"of\", metadata['total_segments'], \"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete bad tensors\n",
    "\n",
    "last_good_index = 19183\n",
    "with new_ds: \n",
    "  for i in range((new_ds.max_len-1), last_good_index, -1):\n",
    "    print(i)\n",
    "    new_ds.caption.pop(i)\n",
    "    new_ds.video_filename.pop(i)\n",
    "    new_ds.video_filepath.pop(i)\n",
    "    new_ds.segment_metadata.pop(i)\n",
    "    try:\n",
    "      new_ds.caption_embedding.pop(i)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds_path = ds[:16705].save_view(path=\"/mnt/storage_ssd/V3_ONLY_VALID_whisper_results_parallel_15\", id=\"only_valid\")\n",
    "vds_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.summary())\n",
    "\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "# index_caption_pairs = []\n",
    "not_done = 0\n",
    "with ds:\n",
    "  for idx, sample in enumerate(ds):\n",
    "    # if not done\n",
    "    if not sample.caption_embedding.numpy().any():\n",
    "      print(idx, sample.caption_embedding.numpy())\n",
    "      not_done += 1\n",
    "    \n",
    "    # if sample.caption_embedding.numpy().shape == (0, 0):\n",
    "    #   index_caption_pairs.append( {'db_index': idx, 'caption': sample.caption.data()['value']} )\n",
    "# index_caption_pairs\n",
    "print(not_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = deeplake.empty('/mnt/storage_ssd/fresh_text_encode_parallel_15', overwrite=True)\n",
    "\n",
    "for idx, sample in enumerate(ds):\n",
    "  # find \n",
    "  try: \n",
    "    new_ds.caption_embedding.append( sample.caption_embedding.numpy() )\n",
    "    # if sample.caption_embedding.numpy().shape == (0, 0):\n",
    "    #   index_caption_pairs.append( {'db_index': idx, 'caption': sample.caption.data()['value']} )\n",
    "  except IndexError as e:\n",
    "    print(e)\n",
    "    # if there's an IndexError, then the caption_embedding is empty.\n",
    "    index_caption_pairs.append( {'db_index': idx, 'caption': sample.caption.data()['value']} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.caption_embedding[15214].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data access\n",
    "import pprint\n",
    "import traceback\n",
    "\n",
    "indexes_with_no_pcation_embedding = []\n",
    "for i in range(ds.max_len):\n",
    "  try: \n",
    "    if ds.caption_embedding[i].numpy().shape != (0, 0):\n",
    "      # print(i, \"✅ of\", ds.max_len)\n",
    "      pass\n",
    "    else:\n",
    "      print(i, \"❌ NONE of\", ds.max_len, \"shape: \", ds.caption_embedding[i].numpy().shape)\n",
    "      indexes_with_no_pcation_embedding.append(i)\n",
    "  except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indexes_with_no_pcation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake as dl\n",
    "import numpy as np\n",
    "junk = deeplake.empty('/tmp/hi', overwrite=True)\n",
    "junk.create_tensor('penis',   htype='image', dtype=np.float32, sample_compression=None)\n",
    "for i in range(10):\n",
    "  junk.penis.append(None)\n",
    "\n",
    "junk.penis[5] = np.float32(69)\n",
    "print(junk.summary())\n",
    "\n",
    "for sample in junk:\n",
    "  if sample.penis.numpy().shape == (0,):\n",
    "    print(\"it was none\")\n",
    "  else:\n",
    "    print(sample.penis.numpy().shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset management (filtering, uploading, checkpointing)\n",
    "\n",
    "## Docs\n",
    "\n",
    "Main reference\n",
    "https://docs.deeplake.ai/en/latest/Datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "dataset_name = '/mnt/storage_ssd/whisper_results_parallel_15'\n",
    "read_lake = deeplake.load(dataset_name)\n",
    "print(read_lake.size_approx() / 1e9, \"GB\")\n",
    "read_lake.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take first n samples\n",
    "read_lake = read_lake[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_lake.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data access\n",
    "import pprint\n",
    "for i, sample in enumerate(read_lake):\n",
    "  pprint.pprint(sample.segment_metadata.data()['value'])\n",
    "  pprint.pprint(sample.video_filepath.data()['value'])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to hub\n",
    "deeplake.copy(read_lake, dest=\"hub://kastan/p17_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "read_lake.add_creds_key('lBEly4ezopu3j54yvdehHYtLPQz6LJBzVx3Pmxpi')\n",
    "# print(os.environ.get('KAS_S3_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = deeplake.empty(\"s3://vpt-deeplake/p17_test\")\n",
    "# upload to s3\n",
    "deeplake.copy(read_lake, dest=\"s3://vpt-deeplake/final_clip_p15\")\n",
    "## Todo: parallel. num_workers=8, scheduler='processed' or 'threaded' or 'ray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_stems = set(read_lake.video_stem.data()['value'])\n",
    "# print(len(all_stems))\n",
    "# read_lake.query(\"select * where contains(video_stem, 'car')\")\n",
    "\n",
    "def filter_func(sample):\n",
    "  # print(sample.segment_metadata.data()['value'])\n",
    "  caption = sample.segment_metadata.data()['value']['captions']\n",
    "  if 'computer'.upper() in caption.upper():\n",
    "    # print(\"returning true\")\n",
    "    return True \n",
    "  elif 'kiss'.upper() in caption.upper():\n",
    "    return True\n",
    "  else:\n",
    "    # print('false')\n",
    "    return False\n",
    "\n",
    "filtered_dataset = read_lake.filter(filter_func, num_workers = 2, scheduler = 'threaded')\n",
    "# read_lake.filter(\"'computer' in segment_metadata.data()['value']['captions']\", num_workers = 10, scheduler = 'ray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "filtered_dataset.summary()\n",
    "for i, sample in enumerate(filtered_dataset):\n",
    "  pprint.pprint(sample.segment_metadata.data()['value'])\n",
    "  \n",
    "  if i > 20: \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_lake.segment_metadata[2].data()['value'][total_segments]\n",
    "# read_lake.text_caption_embeddings[2].data()['value']\n",
    "\n",
    "read_lake = filtered_dataset\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "for i in range(110):\n",
    "  # frame = read_lake.segment_frames[i].numpy()\n",
    "  # frame_rgb = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "  display(Image.fromarray(read_lake.segment_frames[i].numpy()))\n",
    "  print(read_lake.segment_metadata[i].data()['value']['captions'])\n",
    "  metadata = read_lake.segment_metadata[i].data()['value']\n",
    "  print(metadata['segment_index'], \"of\", metadata['total_segments'])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_rec2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9448da8ff7f0a960e3d99cb96042194f38d598dfc9428e7c8dc6e4b8517d2275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
