+ CONTAINER=/scratch/bbki/kastanday/apptainer_cache/whisper_latest.sif
+ echo 'This is best_cpu.slurm'
This is best_cpu.slurm
+ export RAY_worker_register_timeout_seconds=120
+ RAY_worker_register_timeout_seconds=120
++ scontrol show hostnames 'cn[093-094]'
+ nodes='cn093
cn094'
+ nodes_array=($nodes)
+ head_node=cn093
++ srun --nodes=1 --ntasks=1 -w cn093 hostname --ip-address
+ head_node_ip=172.28.22.156
+ printf '%s\n' cn093 cn094
+ [[ 172.28.22.156 == *\ * ]]
+ port=6379
+ ip_head=172.28.22.156:6379
+ export ip_head
+ echo 'IP Head: 172.28.22.156:6379'
IP Head: 172.28.22.156:6379
+ conda activate v3_full_viz_pipeline
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate v3_full_viz_pipeline
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate v3_full_viz_pipeline
++ __add_sys_prefix_to_path
++ '[' -n '' ']'
+++ dirname /scratch/bbki/kastanday/conda_envs/envs/nice_base/bin/conda
++ SYSP=/scratch/bbki/kastanday/conda_envs/envs/nice_base/bin
+++ dirname /scratch/bbki/kastanday/conda_envs/envs/nice_base/bin
++ SYSP=/scratch/bbki/kastanday/conda_envs/envs/nice_base
++ '[' -n '' ']'
++ PATH=/scratch/bbki/kastanday/conda_envs/envs/nice_base/bin:/usr/local/cuda/bin:/u/kastanday/.cargo/bin:/scratch/bbki/kastanday/conda_envs/envs/nice_base/bin:/scratch/bbki/kastanday/conda_envs/envs/nice_base/condabin:/u/kastanday/.local/bin:/u/kastanday/bin:/sw/spack/delta-2022-03/apps/cuda/11.6.1-gcc-11.2.0-vglutoe/bin:/sw/spack/delta-2022-03/apps/openmpi/4.1.2-gcc-11.2.0-37px7gc/bin:/sw/spack/delta-2022-03/apps/ucx/1.11.2-gcc-11.2.0-pymppfm/bin:/sw/spack/delta-2022-03/apps/gcc/11.2.0-gcc-8.4.1-fxgnsyr/bin:/sw/user/scripts:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/ddn/ime/bin:/opt/puppetlabs/bin:/opt/ddn/ime/bin:/opt/ddn/ime/bin:/opt/ddn/ime/bin
++ export PATH
++ /scratch/bbki/kastanday/conda_envs/envs/nice_base/bin/conda shell.posix activate v3_full_viz_pipeline
+ ask_conda='. "/scratch/bbki/kastanday/conda_envs/envs/nice_base/etc/conda/deactivate.d/cm-deactivate.sh"
PS1='\''(v3_full_viz_pipeline) '\''
export PATH='\''/usr/local/cuda/bin:/u/kastanday/.cargo/bin:/scratch/bbki/kastanday/conda_envs/envs/v3_full_viz_pipeline/bin:/scratch/bbki/kastanday/conda_envs/envs/nice_base/condabin:/u/kastanday/.local/bin:/u/kastanday/bin:/sw/spack/delta-2022-03/apps/cuda/11.6.1-gcc-11.2.0-vglutoe/bin:/sw/spack/delta-2022-03/apps/openmpi/4.1.2-gcc-11.2.0-37px7gc/bin:/sw/spack/delta-2022-03/apps/ucx/1.11.2-gcc-11.2.0-pymppfm/bin:/sw/spack/delta-2022-03/apps/gcc/11.2.0-gcc-8.4.1-fxgnsyr/bin:/sw/user/scripts:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/ddn/ime/bin:/opt/puppetlabs/bin:/opt/ddn/ime/bin:/opt/ddn/ime/bin:/opt/ddn/ime/bin'\''
export CONDA_PREFIX='\''/scratch/bbki/kastanday/conda_envs/envs/v3_full_viz_pipeline'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''v3_full_viz_pipeline'\''
export CONDA_PROMPT_MODIFIER='\''(v3_full_viz_pipeline) '\''
export CONDA_EXE='\''/scratch/bbki/kastanday/conda_envs/envs/nice_base/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/scratch/bbki/kastanday/conda_envs/envs/nice_base/bin/python'\''
export CONDA_PREFIX_1='\''/scratch/bbki/kastanday/conda_envs/envs/nice_base'\'''
+ eval '. "/scratch/bbki/kastanday/conda_envs/envs/nice_base/etc/conda/deactivate.d/cm-deactivate.sh"
PS1='\''(v3_full_viz_pipeline) '\''
export PATH='\''/usr/local/cuda/bin:/u/kastanday/.cargo/bin:/scratch/bbki/kastanday/conda_envs/envs/v3_full_viz_pipeline/bin:/scratch/bbki/kastanday/conda_envs/envs/nice_base/condabin:/u/kastanday/.local/bin:/u/kastanday/bin:/sw/spack/delta-2022-03/apps/cuda/11.6.1-gcc-11.2.0-vglutoe/bin:/sw/spack/delta-2022-03/apps/openmpi/4.1.2-gcc-11.2.0-37px7gc/bin:/sw/spack/delta-2022-03/apps/ucx/1.11.2-gcc-11.2.0-pymppfm/bin:/sw/spack/delta-2022-03/apps/gcc/11.2.0-gcc-8.4.1-fxgnsyr/bin:/sw/user/scripts:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/ddn/ime/bin:/opt/puppetlabs/bin:/opt/ddn/ime/bin:/opt/ddn/ime/bin:/opt/ddn/ime/bin'\''
export CONDA_PREFIX='\''/scratch/bbki/kastanday/conda_envs/envs/v3_full_viz_pipeline'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''v3_full_viz_pipeline'\''
export CONDA_PROMPT_MODIFIER='\''(v3_full_viz_pipeline) '\''
export CONDA_EXE='\''/scratch/bbki/kastanday/conda_envs/envs/nice_base/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/scratch/bbki/kastanday/conda_envs/envs/nice_base/bin/python'\''
export CONDA_PREFIX_1='\''/scratch/bbki/kastanday/conda_envs/envs/nice_base'\'''
++ . /scratch/bbki/kastanday/conda_envs/envs/nice_base/etc/conda/deactivate.d/cm-deactivate.sh
+++ '[' Conda = Conda ']'
+++ unset CMDOMAIN
+++ unset CMMGR
+++ '[' -z '' ']'
+++ export CMMGR=
+++ CMMGR=
+++ unset _CONDA_SET_CMMGR
++ PS1='(v3_full_viz_pipeline) '
++ export PATH=/usr/local/cuda/bin:/u/kastanday/.cargo/bin:/scratch/bbki/kastanday/conda_envs/envs/v3_full_viz_pipeline/bin:/scratch/bbki/kastanday/conda_envs/envs/nice_base/condabin:/u/kastanday/.local/bin:/u/kastanday/bin:/sw/spack/delta-2022-03/apps/cuda/11.6.1-gcc-11.2.0-vglutoe/bin:/sw/spack/delta-2022-03/apps/openmpi/4.1.2-gcc-11.2.0-37px7gc/bin:/sw/spack/delta-2022-03/apps/ucx/1.11.2-gcc-11.2.0-pymppfm/bin:/sw/spack/delta-2022-03/apps/gcc/11.2.0-gcc-8.4.1-fxgnsyr/bin:/sw/user/scripts:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/ddn/ime/bin:/opt/puppetlabs/bin:/opt/ddn/ime/bin:/opt/ddn/ime/bin:/opt/ddn/ime/bin
++ PATH=/usr/local/cuda/bin:/u/kastanday/.cargo/bin:/scratch/bbki/kastanday/conda_envs/envs/v3_full_viz_pipeline/bin:/scratch/bbki/kastanday/conda_envs/envs/nice_base/condabin:/u/kastanday/.local/bin:/u/kastanday/bin:/sw/spack/delta-2022-03/apps/cuda/11.6.1-gcc-11.2.0-vglutoe/bin:/sw/spack/delta-2022-03/apps/openmpi/4.1.2-gcc-11.2.0-37px7gc/bin:/sw/spack/delta-2022-03/apps/ucx/1.11.2-gcc-11.2.0-pymppfm/bin:/sw/spack/delta-2022-03/apps/gcc/11.2.0-gcc-8.4.1-fxgnsyr/bin:/sw/user/scripts:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/ddn/ime/bin:/opt/puppetlabs/bin:/opt/ddn/ime/bin:/opt/ddn/ime/bin:/opt/ddn/ime/bin
++ export CONDA_PREFIX=/scratch/bbki/kastanday/conda_envs/envs/v3_full_viz_pipeline
++ CONDA_PREFIX=/scratch/bbki/kastanday/conda_envs/envs/v3_full_viz_pipeline
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=v3_full_viz_pipeline
++ CONDA_DEFAULT_ENV=v3_full_viz_pipeline
++ export 'CONDA_PROMPT_MODIFIER=(v3_full_viz_pipeline) '
++ CONDA_PROMPT_MODIFIER='(v3_full_viz_pipeline) '
++ export CONDA_EXE=/scratch/bbki/kastanday/conda_envs/envs/nice_base/bin/conda
++ CONDA_EXE=/scratch/bbki/kastanday/conda_envs/envs/nice_base/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/scratch/bbki/kastanday/conda_envs/envs/nice_base/bin/python
++ CONDA_PYTHON_EXE=/scratch/bbki/kastanday/conda_envs/envs/nice_base/bin/python
++ export CONDA_PREFIX_1=/scratch/bbki/kastanday/conda_envs/envs/nice_base
++ CONDA_PREFIX_1=/scratch/bbki/kastanday/conda_envs/envs/nice_base
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ export RAY_max_pending_lease_requests_per_scheduling_category=2000
+ RAY_max_pending_lease_requests_per_scheduling_category=2000
+ echo 'Starting HEAD at cn093'
Starting HEAD at cn093
+ srun --nodes=1 --ntasks=1 -w cn093 ray stop
2022-11-12 17:52:19,741	INFO scripts.py:1048 -- Did not find any active Ray processes.
+ srun --nodes=1 --ntasks=1 -w cn093 singularity run --bind /scratch/bbki/kastanday:/scratch /scratch/bbki/kastanday/apptainer_cache/whisper_latest.sif /bin/bash

=============
== PyTorch ==
=============

NVIDIA Release 22.08 (build 42105213)
PyTorch Version 1.13.0a0+d321be6

Container image Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

Copyright (c) 2014-2022 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.
   Use the NVIDIA Container Toolkit to start this container with GPU support; see
   https://docs.nvidia.com/datacenter/cloud-native/ .

NOTE: Mellanox network driver detected, but NVIDIA peer memory driver not
      detected.  Multi-node communication performance may be reduced.

+ sleep 1
+ srun --nodes=1 --ntasks=1 -w cn093 ray start --head --node-ip-address=172.28.22.156 --port=6379 --dashboard-host 0.0.0.0 --log-color true --block
+ worker_num=1
+ (( i = 1 ))
+ (( i <= worker_num ))
+ node_i=cn094
+ srun --nodes=1 --ntasks=1 -w cn094 ray stop
2022-11-12 17:52:45,774	INFO services.py:1470 -- View the Ray dashboard at [1m[32mhttp://141.142.144.156:8265[39m[22m
2022-11-12 17:52:25,225	INFO usage_lib.py:297 -- Usage stats collection is disabled.
2022-11-12 17:52:25,225	INFO scripts.py:715 -- [37mLocal node IP[39m: [1m172.28.22.156[22m
2022-11-12 17:52:47,181	SUCC scripts.py:757 -- [32m--------------------[39m
2022-11-12 17:52:47,181	SUCC scripts.py:758 -- [32mRay runtime started.[39m
2022-11-12 17:52:47,183	SUCC scripts.py:759 -- [32m--------------------[39m
2022-11-12 17:52:47,184	INFO scripts.py:761 -- [36mNext steps[39m
2022-11-12 17:52:47,184	INFO scripts.py:762 -- To connect to this Ray runtime from another node, run
2022-11-12 17:52:47,184	INFO scripts.py:765 -- [1m  ray start --address='172.28.22.156:6379'[22m
2022-11-12 17:52:47,184	INFO scripts.py:770 -- Alternatively, use the following Python code:
2022-11-12 17:52:47,184	INFO scripts.py:772 -- [35mimport[39m[26m ray
2022-11-12 17:52:47,184	INFO scripts.py:776 -- ray[35m.[39m[26minit(address[35m=[39m[26m[33m'auto'[39m[26m, _node_ip_address[35m=[39m[26m[33m'172.28.22.156'[39m[26m)
2022-11-12 17:52:47,184	INFO scripts.py:788 -- To connect to this Ray runtime from outside of the cluster, for example to
2022-11-12 17:52:47,184	INFO scripts.py:792 -- connect to a remote cluster from your laptop directly, use the following
2022-11-12 17:52:47,184	INFO scripts.py:796 -- Python code:
2022-11-12 17:52:47,184	INFO scripts.py:798 -- [35mimport[39m[26m ray
2022-11-12 17:52:47,184	INFO scripts.py:799 -- ray[35m.[39m[26minit(address[35m=[39m[26m[33m'ray://<head_node_ip_address>:10001'[39m[26m)
2022-11-12 17:52:47,184	INFO scripts.py:808 -- [4mIf connection fails, check your firewall settings and network configuration.[24m
2022-11-12 17:52:47,184	INFO scripts.py:816 -- To terminate the Ray runtime, run
2022-11-12 17:52:47,184	INFO scripts.py:817 -- [1m  ray stop[22m
2022-11-12 17:52:47,184	INFO scripts.py:892 -- [36m[1m--block[22m[39m
2022-11-12 17:52:47,184	INFO scripts.py:893 -- This command will now block until terminated by a signal.
2022-11-12 17:52:47,184	INFO scripts.py:896 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly.
2022-11-12 17:53:03,204	INFO scripts.py:1048 -- Did not find any active Ray processes.
+ echo 'Starting WORKER 1 at cn094'
Starting WORKER 1 at cn094
+ srun --nodes=1 --ntasks=1 -w cn094 ulimit -n unlimited
/usr/bin/ulimit: line 2: ulimit: open files: cannot modify limit: Operation not permitted
srun: error: cn094: task 0: Exited with exit code 1
+ srun --nodes=1 --ntasks=1 -w cn094 export RAY_worker_register_timeout_seconds=120
slurmstepd: error: execve(): export: No such file or directory
srun: error: cn094: task 0: Exited with exit code 2
+ srun --nodes=1 --ntasks=1 -w cn094 singularity run --bind /scratch/bbki/kastanday:/scratch /scratch/bbki/kastanday/apptainer_cache/whisper_latest.sif /bin/bash

=============
== PyTorch ==
=============

NVIDIA Release 22.08 (build 42105213)
PyTorch Version 1.13.0a0+d321be6

Container image Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

Copyright (c) 2014-2022 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.
   Use the NVIDIA Container Toolkit to start this container with GPU support; see
   https://docs.nvidia.com/datacenter/cloud-native/ .

NOTE: Mellanox network driver detected, but NVIDIA peer memory driver not
      detected.  Multi-node communication performance may be reduced.

+ echo 'WORKER 1 has SLURM_CPUS_PER_TASK: '
WORKER 1 has SLURM_CPUS_PER_TASK: 
+ srun --nodes=1 --ntasks=1 -w cn094 ray start --address 172.28.22.156:6379 --log-color true --block
+ (( i++ ))
+ (( i <= worker_num ))
+ sleep infinity
[2022-11-12 17:53:13,799 I 2931155 2931155] global_state_accessor.cc:357: This node has an IP address of 172.28.22.157, while we can not found the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.
2022-11-12 17:53:10,844	INFO scripts.py:870 -- [37mLocal node IP[39m: [1m172.28.22.157[22m
2022-11-12 17:53:13,810	SUCC scripts.py:882 -- [32m--------------------[39m
2022-11-12 17:53:13,810	SUCC scripts.py:883 -- [32mRay runtime started.[39m
2022-11-12 17:53:13,810	SUCC scripts.py:884 -- [32m--------------------[39m
2022-11-12 17:53:13,810	INFO scripts.py:886 -- To terminate the Ray runtime, run
2022-11-12 17:53:13,810	INFO scripts.py:887 -- [1m  ray stop[22m
2022-11-12 17:53:13,810	INFO scripts.py:892 -- [36m[1m--block[22m[39m
2022-11-12 17:53:13,810	INFO scripts.py:893 -- This command will now block until terminated by a signal.
2022-11-12 17:53:13,810	INFO scripts.py:896 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly.
2022-11-12 18:12:00,340	ERR scripts.py:907 -- [31mSome Ray subprcesses exited unexpectedly:[39m
2022-11-12 18:12:00,340	ERR scripts.py:911 -- [31m[1mgcs_server[22m[26m[39m[0m[2m [exit code=0][22m[0m
2022-11-12 18:12:00,340	ERR scripts.py:911 -- [31m[1mray_client_server[22m[26m[39m[0m[2m [exit code=15][22m[0m
2022-11-12 18:12:00,341	ERR scripts.py:911 -- [31m[1mraylet[22m[26m[39m[0m[2m [exit code=-15][22m[0m
2022-11-12 18:12:00,341	ERR scripts.py:911 -- [31m[1mlog_monitor[22m[26m[39m[0m[2m [exit code=-15][22m[0m
2022-11-12 18:12:00,341	ERR scripts.py:919 -- [31mRemaining processes will be killed.[39m
srun: error: cn093: task 0: Exited with exit code 1
2022-11-12 18:12:16,032	ERR scripts.py:907 -- [31mSome Ray subprcesses exited unexpectedly:[39m
2022-11-12 18:12:16,032	ERR scripts.py:911 -- [31m[1mlog_monitor[22m[26m[39m[0m[2m [exit code=-15][22m[0m
2022-11-12 18:12:16,033	ERR scripts.py:919 -- [31mRemaining processes will be killed.[39m
srun: error: cn094: task 0: Exited with exit code 1
slurmstepd: error: *** JOB 1152330 ON cn093 CANCELLED AT 2022-11-12T21:05:49 DUE TO NODE FAILURE, SEE SLURMCTLD LOG FOR DETAILS ***
