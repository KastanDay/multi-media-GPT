{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping for real run. \n",
    "\n",
    "Todo: \n",
    "* Define `max_source_length` and `max_target_length` for the model (otherwise truncated).\n",
    "padding token should be replaced with -100, which is the 'ignore_index' of `CrossEntorpyLoss` in PT and TF. For Flax, use `decoder_attention_mask`. \n",
    "Attention_mask. ensures madding tokens of inputs are ignored. \n",
    "\n",
    "* Install apex. \"model will automatically use apex.normalization.FusedRMSNorm instead of T5LayerNorm.\" The former uses an optimized fused kernel which is several times faster than the latter.\n",
    "\n",
    "A note on model sizes: \n",
    "T5-11B (original, not v1.1) weights in float32 are 45.2GB. \n",
    "See this post for using huggingface endpoints on SINGLE GPU for cheap inference: https://www.philschmid.de/deploy-t5-11b\n",
    "Uses mixed precision and sharding, and LLM.int8(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel, T5Model, T5Config, AutoModelWithLMHead\n",
    "import accelerate\n",
    "# import wandb\n",
    "from tqdm import tqdm\n",
    "import lovely_tensors as lt\n",
    "import math\n",
    "from PIL import Image\n",
    "lt.monkey_patch()\n",
    "# !wandb login  -- reactivate later\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001\n",
      "1504\n"
     ]
    }
   ],
   "source": [
    "# Split VAL2014 into train and test datasets \n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for i, img_name in enumerate(os.listdir(\"/home/kastan/thesis/video-pretrained-transformer/vqa/val2014\")):\n",
    "    if i >= 39000:\n",
    "        test_set.append(os.path.join(\"/home/kastan/thesis/video-pretrained-transformer/vqa/val2014\", img_name))\n",
    "    elif i <= 5000:\n",
    "        train_set.append(os.path.join(\"/home/kastan/thesis/video-pretrained-transformer/vqa/val2014\", img_name))\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "Initializing clip and scene graph models...\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: \", device)\n",
    "\n",
    "# Initialize preprocessing models for collate_fn \n",
    "print(\"Initializing clip and scene graph models...\")\n",
    "clip_model, clip_preprocess = clip.load('ViT-L/14@336px', device)\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-base\", return_special_tokens_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_imgs = []\n",
    "    input_questions = []\n",
    "    answers = []\n",
    "    scene_strs = []\n",
    "    question_ids = []\n",
    "    for elt in batch:\n",
    "        img, question, answer, scene_graph_str, question_id = elt\n",
    "        input_imgs.append(clip_preprocess(Image.fromarray(img)).unsqueeze(0))\n",
    "        input_questions.append(question)\n",
    "        answers.append(answer)\n",
    "        scene_strs.append(scene_graph_str)\n",
    "        question_ids.append(question_id)\n",
    "\n",
    "    image_input = torch.cat(input_imgs).to(device)\n",
    "    text_input = clip.tokenize(input_questions, truncate=True).to(device)\n",
    "    sg_input = clip.tokenize(scene_strs, truncate=True).to(device)\n",
    "\n",
    "    with torch.inference_mode(): # even faster than no_grad()\n",
    "        # image_features = torch.unsqueeze(clip_model.encode_image(image_input), dim=1)\n",
    "        # text_features = torch.unsqueeze(clip_model.encode_text(text_input), dim=1)\n",
    "        # sg_features = torch.unsqueeze(clip_model.encode_text(sg_input), dim=1)\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "        sg_features = clip_model.encode_text(sg_input)\n",
    "\n",
    "    # labels = t5_tokenizer(answers, padding=\"longest\", max_length=128, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    # labels[labels == t5_tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # print(\"answers: \", answers)\n",
    "    labels = t5_tokenizer(answers, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # print(\"labels: \", labels)\n",
    "\n",
    "    return image_features, text_features, sg_features, labels, question_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing train dataset...\n",
      "Stored annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214354/214354 [00:12<00:00, 17220.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing question to answer dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26297/26297 [00:00<00:00, 2049379.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed question to answer dictionary...\n",
      "Processing scene graph json...\n",
      "Created img name to scene graph mapping...\n",
      "Stored questions...\n",
      "Created question ID to question mapping...\n",
      "train dataset contains 26297 annotations\n",
      "Constructing test dataset...\n",
      "Stored annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214354/214354 [00:04<00:00, 51146.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing question to answer dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8134/8134 [00:00<00:00, 2225035.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed question to answer dictionary...\n",
      "Processing scene graph json...\n",
      "Created img name to scene graph mapping...\n",
      "Stored questions...\n",
      "Created question ID to question mapping...\n",
      "test dataset contains 8134 annotations\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class VQA(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations, img_path, pseudo_mode=\"val\", mode=\"train\", img_set=None):\n",
    "        self.annotations = [] \n",
    "        self.questions = None\n",
    "        self.img_path = img_path\n",
    "        self.qid_to_question = {}\n",
    "        self.mode = mode\n",
    "        self.pseudo_mode = pseudo_mode\n",
    "\n",
    "        self.img_to_scene_graph = {}\n",
    "        self.question_to_answer = {}\n",
    "\n",
    "        print(f\"Constructing {self.mode} dataset...\")\n",
    "        with open(annotations[f\"{self.mode}_annotations\"]) as f:\n",
    "            all_annotations = json.load(f)['annotations']\n",
    "            print(\"Stored annotations...\")\n",
    "\n",
    "            for annotation in tqdm(all_annotations):\n",
    "                image_id = annotation[\"image_id\"]\n",
    "                padded_image_id = \"\".join((12 - len(str(image_id)))*[\"0\"]) + str(image_id)\n",
    "                img_path = os.path.join(self.img_path, f\"COCO_{self.pseudo_mode}2014_{padded_image_id}.jpg\")\n",
    "\n",
    "                if img_set is not None:\n",
    "                    if img_path in img_set:\n",
    "                        self.annotations.append(annotation)\n",
    "                else:\n",
    "                    self.annotations.append(annotation)\n",
    "\n",
    "        self.construct_question_to_answer_dict()\n",
    "\n",
    "        with open(annotations[f\"scene_graph_json\"]) as f:\n",
    "            print(\"Processing scene graph json...\")\n",
    "            for obj in f:\n",
    "                json_obj = json.loads(eval(eval(obj)))\n",
    "                if str(json_obj[\"input_img_path\"]) not in self.img_to_scene_graph:\n",
    "                    self.img_to_scene_graph[str(json_obj[\"input_img_path\"])] = str(json_obj[\"scene_graph_string\"])\n",
    "                else:\n",
    "                    print(\"Duplicate scene graphs exist!\")\n",
    "\n",
    "            print(\"Created img name to scene graph mapping...\")\n",
    "\n",
    "       \n",
    "        with open(annotations[f\"{self.mode}_questions\"]) as f:\n",
    "            self.questions = json.load(f)[\"questions\"]\n",
    "            print(f\"Stored questions...\")\n",
    "            for question in self.questions:\n",
    "                self.qid_to_question[question[\"question_id\"]] = question[\"question\"]\n",
    "            print(\"Created question ID to question mapping...\")\n",
    "            \n",
    "        \n",
    "\n",
    "        print(f\"{self.mode} dataset contains {len(self.annotations)} annotations\")\n",
    "\n",
    "    \n",
    "    def construct_question_to_answer_dict(self):\n",
    "        print(\"Constructing question to answer dictionary...\")\n",
    "        for annotation in tqdm(self.annotations):\n",
    "            self.question_to_answer[annotation[\"question_id\"]] = annotation\n",
    "            \n",
    "        print(\"Constructed question to answer dictionary...\")\n",
    "\n",
    "    def get_question_to_answer_dict(self):\n",
    "        return self.question_to_answer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        curr_annotation = self.annotations[idx]\n",
    "\n",
    "        question_id = curr_annotation[\"question_id\"]\n",
    "        image_id = curr_annotation[\"image_id\"]\n",
    "        answers = curr_annotation[\"answers\"]\n",
    "\n",
    "        # Don't need these for now but may need them for future ablations\n",
    "        # question_type = curr_annotation[\"question_type\"]\n",
    "        # answer_type = curr_annotation[\"answers\"]\n",
    "        # multiple_choice_answer = curr_annotation[\"multiple_choice_answer\"]\n",
    "\n",
    "        # VQA has multiple possible answers, can modify this later to use other answers\n",
    "        answer_choice = answers[0][\"answer\"]\n",
    "        \n",
    "        img = None\n",
    "\n",
    "        question = self.qid_to_question.get(question_id, None)\n",
    "\n",
    "        assert question is not None\n",
    "\n",
    "        padded_image_id = \"\".join((12 - len(str(image_id)))*[\"0\"]) + str(image_id)\n",
    "\n",
    "        img_path = os.path.join(self.img_path, f\"COCO_{self.pseudo_mode}2014_{padded_image_id}.jpg\")\n",
    "\n",
    "        assert os.path.exists(img_path)\n",
    "            \n",
    "        img = np.asarray(Image.open(img_path))\n",
    "\n",
    "        scene_graph_str = self.img_to_scene_graph.get(img_path, None)\n",
    "       \n",
    "        assert scene_graph_str is not None\n",
    "        \n",
    "        return img, question, answer_choice, scene_graph_str, question_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "# we use the original val set as our test set since we have the annotations available (test annotations not available)\n",
    "# this will be useful for error analysis later on\n",
    "\n",
    "annotations = {\n",
    "    \"train_questions\": \"/home/kastan/thesis/video-pretrained-transformer/vqa/v2_OpenEnded_mscoco_val2014_questions.json\",\n",
    "    \"test_questions\": \"/home/kastan/thesis/video-pretrained-transformer/vqa/v2_OpenEnded_mscoco_val2014_questions.json\",\n",
    "    \"train_annotations\": \"/home/kastan/thesis/video-pretrained-transformer/vqa/v2_mscoco_val2014_annotations.json\",\n",
    "    \"test_annotations\": \"/home/kastan/thesis/video-pretrained-transformer/vqa/v2_mscoco_val2014_annotations.json\",\n",
    "    \"scene_graph_json\": \"/home/kastan/thesis/video-pretrained-transformer/vqa/val_scene_graph.json\"\n",
    "}\n",
    "\n",
    "train_dataset = VQA(annotations, \"/home/kastan/thesis/video-pretrained-transformer/vqa/val2014\", mode=\"train\", img_set=train_set)\n",
    "test_dataset = VQA(annotations, \"/home/kastan/thesis/video-pretrained-transformer/vqa/val2014\", mode=\"test\", img_set=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing t5 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26297 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.92 GiB total capacity; 2.79 GiB already allocated; 8.50 MiB free; 2.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 53\u001b[0m\n\u001b[1;32m     48\u001b[0m input_embeds_arr[\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, :] \u001b[39m=\u001b[39m scene_graph_embed\n\u001b[1;32m     50\u001b[0m \u001b[39m# labels = t5_tokenizer(\"hi my name is pranav\", return_tensors=\"pt\").input_ids.to(device)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m# print(\"labels shaep: \", labels.shape)\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m outputs \u001b[39m=\u001b[39m t5\u001b[39m.\u001b[39;49mforward(inputs_embeds\u001b[39m=\u001b[39;49minput_embeds_arr, attention_mask\u001b[39m=\u001b[39;49mattn_mask_arr, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[1;32m     55\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     57\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39misnan(loss)\u001b[39m.\u001b[39many()\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1611\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1608\u001b[0m \u001b[39m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1610\u001b[0m     \u001b[39m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1611\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1612\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1613\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1614\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1615\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1616\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1617\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1618\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1620\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1621\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1622\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1623\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1624\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1625\u001b[0m     )\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1040\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1028\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1029\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1039\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1040\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1041\u001b[0m         hidden_states,\n\u001b[1;32m   1042\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1043\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1044\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1045\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1046\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1047\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1048\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1049\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1050\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1051\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1052\u001b[0m     )\n\u001b[1;32m   1054\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:673\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m0\u001b[39;49m](\n\u001b[1;32m    674\u001b[0m     hidden_states,\n\u001b[1;32m    675\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    676\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    677\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    678\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    679\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    680\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    681\u001b[0m )\n\u001b[1;32m    682\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    683\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:579\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    569\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    570\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m ):\n\u001b[1;32m    578\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 579\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSelfAttention(\n\u001b[1;32m    580\u001b[0m         normed_hidden_states,\n\u001b[1;32m    581\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    582\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    583\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    584\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    585\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    586\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    587\u001b[0m     )\n\u001b[1;32m    588\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[1;32m    589\u001b[0m     outputs \u001b[39m=\u001b[39m (hidden_states,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:504\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39m# get key/value states\u001b[39;00m\n\u001b[1;32m    501\u001b[0m key_states \u001b[39m=\u001b[39m project(\n\u001b[1;32m    502\u001b[0m     hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk, key_value_states, past_key_value[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    503\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m value_states \u001b[39m=\u001b[39m project(\n\u001b[1;32m    505\u001b[0m     hidden_states, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv, key_value_states, past_key_value[\u001b[39m1\u001b[39;49m] \u001b[39mif\u001b[39;49;00m past_key_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    506\u001b[0m )\n\u001b[1;32m    508\u001b[0m \u001b[39m# compute scores\u001b[39;00m\n\u001b[1;32m    509\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(\n\u001b[1;32m    510\u001b[0m     query_states, key_states\u001b[39m.\u001b[39mtranspose(\u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    511\u001b[0m )  \u001b[39m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:481\u001b[0m, in \u001b[0;36mT5Attention.forward.<locals>.project\u001b[0;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39m\"\"\"projects hidden states correctly to key/query states\"\"\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m key_value_states \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     \u001b[39m# self-attn\u001b[39;00m\n\u001b[1;32m    480\u001b[0m     \u001b[39m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m     hidden_states \u001b[39m=\u001b[39m shape(proj_layer(hidden_states))\n\u001b[1;32m    482\u001b[0m \u001b[39melif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[39m# cross-attn\u001b[39;00m\n\u001b[1;32m    484\u001b[0m     \u001b[39m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     hidden_states \u001b[39m=\u001b[39m shape(proj_layer(key_value_states))\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.92 GiB total capacity; 2.79 GiB already allocated; 8.50 MiB free; 2.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "MODEL_NAME = \"google/t5-v1_1-base\"\n",
    "RUN_NAME = \"all_modalities\"\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "print(\"Initializing t5 model...\")\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(\"/home/kastan/thesis/video-pretrained-transformer/model/yt_pretrain_vqa_val_finetune\", torch_dtype=torch.float32, low_cpu_mem_usage=True).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "\n",
    "t5.train()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=t5.parameters(), lr=1e-4)\n",
    "\n",
    "iter_losses = []\n",
    "\n",
    "one_input_shape = [BATCH_SIZE, 768, 768]\n",
    "att_mask_shape = [BATCH_SIZE, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0, 0] = 1\n",
    "attn_mask_arr[0, 1] = 1\n",
    "attn_mask_arr[0, 2] = 1\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) \n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        if i % 500 == 0:\n",
    "            t5.save_pretrained(f\"/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/{RUN_NAME}_iter{i}\")\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        question_embed, img_embed, scene_graph_embed, labels, _ = batch\n",
    "\n",
    "        question_embed = question_embed.to(device)\n",
    "        img_embed = img_embed.to(device)\n",
    "        scene_graph_embed = scene_graph_embed.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        input_embeds_arr[0, 0, :] = img_embed\n",
    "        input_embeds_arr[0, 1, :] = question_embed\n",
    "        input_embeds_arr[0, 2, :] = scene_graph_embed\n",
    "\n",
    "        # labels = t5_tokenizer(\"hi my name is pranav\", return_tensors=\"pt\").input_ids.to(device)\n",
    "        # print(\"labels shaep: \", labels.shape)\n",
    "\n",
    "        outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        assert not torch.isnan(loss).any()\n",
    "\n",
    "        iter_losses.append(loss)\n",
    "\n",
    "        loss.sum().backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(t5.parameters(), 1e-2)\n",
    "\n",
    "        optimizer.step()\n",
    "        # print(\"✅ Successful training iteration\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Loss: \", loss.item())\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch} done.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5.save_pretrained(f\"/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/{RUN_NAME}_iter{19401}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19756\n"
     ]
    }
   ],
   "source": [
    "print(len(iter_losses))\n",
    "\n",
    "new_iter_losses = [str(t.item()) for t in iter_losses]\n",
    "\n",
    "# print(new_iter_losses)\n",
    "file1 = open('/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/iter_losses.txt', 'w')\n",
    "file1.writelines(\",\".join(new_iter_losses))\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8134 [00:00<12:26, 10.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the man doing in the street?\n",
      "answers:  [['crossing it', 'walking', 'walking', 'crossing', 'crossing road', 'walking', 'crossing', 'walking', 'crossing', 'walking']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 102/8134 [00:08<11:00, 12.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the player wearing around his head?\n",
      "answers:  [['hat', 'cap', 'hat', 'cap', 'cap', 'hat', 'hat', 'hat', 'hat', 'hat']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 202/8134 [00:16<10:57, 12.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Where would luggage go?\n",
      "answers:  [['in back of train', 'on top', 'inside', 'under seats', 'roof rack', 'top', 'top', 'roof rack', 'on top of bus', 'on roof']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 302/8134 [00:25<11:07, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What does the red sign with the yellow M mean?\n",
      "answers:  [[\"mcdonald's\", \"mcdonald's\", 'mcdonalds', 'mcdonalds', \"mcdonald's\", \"mcdonald's\", 'mcdonald', \"mcdonald's\", \"mcdonald's restaurant ahead\", \"mcdonald's\"]]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 402/8134 [00:33<11:05, 11.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Are the motorcycles parked legally?\n",
      "answers:  [['yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 502/8134 [00:41<10:28, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the player's number?\n",
      "answers:  [['46', '46', '46', '46', '46', '46', '46', '46', '46', '46']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 602/8134 [00:49<10:11, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What type of place is this?\n",
      "answers:  [['school', 'school', 'school', 'outdoors', 'school', 'outdoors', 'school', 'school', \"boy's school\", 'school']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 702/8134 [00:58<10:23, 11.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  How many pastries are in this picture?\n",
      "answers:  [['0', '0', '47', '30', '0', '63', '50', 'lot', '20', '0']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 802/8134 [01:06<09:58, 12.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What creature is depicted on the yellow sign?\n",
      "answers:  [['dragon', 'dragon', 'snake', 'dragon', 'dragon', 'dragon', 'dragon', 'dragon', 'snake', 'dragon']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 902/8134 [01:14<09:59, 12.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  How many people are in this picture?\n",
      "answers:  [['5', '5', '5', '5', '4', '5', '5', '5', '5', '5']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1002/8134 [01:23<10:03, 11.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  How large is this living space?\n",
      "answers:  [['small hospital room', 'small', 'small', 'small', 'small', 'fifty square feet', 'short', 'small', '10x12', 'small']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 1102/8134 [01:31<09:50, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the person in red holding?\n",
      "answers:  [['plate', 'plate of food', 'plate', 'plate of food', 'plate', 'her dinner plate', 'plate', 'plate', 'plate of food', 'food']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1202/8134 [01:40<09:41, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Do the tires look matching?\n",
      "answers:  [['yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1302/8134 [01:48<09:31, 11.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is on the ground?\n",
      "answers:  [['leaves', 'leaves', 'fire hydrant', 'fire hydrant', 'leaves', 'leaves', 'leaves', 'leaves', 'fire hydrant', 'leaves']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1402/8134 [01:56<09:25, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Is there a glare on the glass?\n",
      "answers:  [['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1502/8134 [02:05<09:12, 12.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What fruit is hanging from the wall?\n",
      "answers:  [['0', '0', '0', '0', '0', '0', 'curtain', 'banana', '0', 'pizza']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1602/8134 [02:13<09:14, 11.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Is the person holding the umbrella real?\n",
      "answers:  [['no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1702/8134 [02:22<08:57, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Is this belt in motion?\n",
      "answers:  [['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1802/8134 [02:30<08:43, 12.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is this person doing?\n",
      "answers:  [['taking picture', 'talking', 'taking pic', 'taking picture', 'taking picture', 'looking at cell phone', 'looking at their phone', 'taking photo', 'taking photo', 'taking picture']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1902/8134 [02:39<08:36, 12.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What color are the stripes?\n",
      "answers:  [['white', 'white', 'white', 'white', 'white', 'white', 'blue', 'white', 'black and white', 'white']]\n",
      "outputs:  ['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1983/8134 [02:46<08:34, 11.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m attn_mask_arr[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     23\u001b[0m attn_mask_arr[\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(test_loader)):\n\u001b[1;32m     26\u001b[0m     question_embed, img_embed, scene_graph_embed, labels, question_ids \u001b[39m=\u001b[39m batch\n\u001b[1;32m     28\u001b[0m     answers \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[21], line 23\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m sg_input \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mtokenize(scene_strs, truncate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode(): \u001b[39m# even faster than no_grad()\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m# image_features = torch.unsqueeze(clip_model.encode_image(image_input), dim=1)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39m# text_features = torch.unsqueeze(clip_model.encode_text(text_input), dim=1)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[39m# sg_features = torch.unsqueeze(clip_model.encode_text(sg_input), dim=1)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     image_features \u001b[39m=\u001b[39m clip_model\u001b[39m.\u001b[39;49mencode_image(image_input)\n\u001b[1;32m     24\u001b[0m     text_features \u001b[39m=\u001b[39m clip_model\u001b[39m.\u001b[39mencode_text(text_input)\n\u001b[1;32m     25\u001b[0m     sg_features \u001b[39m=\u001b[39m clip_model\u001b[39m.\u001b[39mencode_text(sg_input)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/clip/model.py:341\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(image\u001b[39m.\u001b[39;49mtype(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype))\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/clip/model.py:232\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    229\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(x)\n\u001b[1;32m    231\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    233\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    235\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_post(x[:, \u001b[39m0\u001b[39m, :])\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/clip/model.py:203\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresblocks(x)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/clip/model.py:191\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    190\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x))\n\u001b[0;32m--> 191\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(x))\n\u001b[1;32m    192\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/clip/model.py:168\u001b[0m, in \u001b[0;36mQuickGELU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 168\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39m1.702\u001b[39;49m \u001b[39m*\u001b[39;49m x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "BATCH_SIZE = 1\n",
    "MODEL_STR = \"google/t5-v1_1-base\"\n",
    "# t5_eval = T5ForConditionalGeneration.from_pretrained(\"/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/all_modalities_iter19401\", torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "t5_eval = T5ForConditionalGeneration.from_pretrained(\"/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/all_modalities_iter5000\", torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_STR, return_special_tokens_mask=True)\n",
    "\n",
    "question_to_answer = test_dataset.get_question_to_answer_dict()\n",
    "\n",
    "t5_eval.eval()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "iter_accuracies = []\n",
    "\n",
    "one_input_shape = [BATCH_SIZE, 768, 768]\n",
    "att_mask_shape = [BATCH_SIZE, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0, 0] = 1\n",
    "attn_mask_arr[0, 1] = 1\n",
    "attn_mask_arr[0, 2] = 1\n",
    "\n",
    "for i, batch in enumerate(tqdm(test_loader)):\n",
    "    question_embed, img_embed, scene_graph_embed, labels, question_ids = batch\n",
    "\n",
    "    answers = []\n",
    "    for question_id in question_ids:\n",
    "        answers.append([answer_obj[\"answer\"] for answer_obj in question_to_answer[question_id][\"answers\"]])\n",
    "\n",
    "    # To view the image, question and possible answers, uncomment\n",
    "    # image_id = question_to_answer[question_id][\"image_id\"]\n",
    "    # padded_image_id = \"\".join((12 - len(str(image_id)))*[\"0\"]) + str(image_id)\n",
    "    # img_path = os.path.join(self.img_path, f\"COCO_{self.pseudo_mode}2014_{padded_image_id}.jpg\")\n",
    "    # print(img_path)\n",
    "    # print(\"question: \", test_dataset.qid_to_question[question_ids[0]])\n",
    "    # print(answers)\n",
    "\n",
    "    question_embed = question_embed.to(device)\n",
    "    img_embed = img_embed.to(device)\n",
    "    scene_graph_embed = scene_graph_embed.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    input_embeds_arr[0, 0, :] = img_embed\n",
    "    input_embeds_arr[0, 1, :] = question_embed\n",
    "    input_embeds_arr[0, 2, :] = scene_graph_embed\n",
    "\n",
    "    # input_embeds_arr = torch.cat((question_embed, img_embed, scene_graph_embed), dim=1)\n",
    "\n",
    "    # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "    # output_sequences = t5_eval.generate(inputs_embeds=input_embeds_arr,  attention_mask=attn_mask_arr, do_sample=False)\n",
    "    \n",
    "    # outputs = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "    outputs = ['no']\n",
    "    if i % 100 == 0:\n",
    "        print(\"question: \", test_dataset.qid_to_question[question_ids[0]])\n",
    "        print(\"answers: \", answers)\n",
    "        print(\"outputs: \", outputs)\n",
    "\n",
    "    for j, output in enumerate(outputs):\n",
    "        curr_answers = answers[j]\n",
    "\n",
    "        # evaluation metric for VQA: https://visualqa.org/evaluation.html\n",
    "        iter_accuracies.append(min(curr_answers.count(str(output)) / 3.0, 1.0))\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", sum(iter_accuracies)/len(iter_accuracies))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2390317700453855"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(iter_accuracies)/len(iter_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('eval_clip_t5_torch19')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6ba738257b4305cdf5ec2b607b42681cdf9e18b5e0fdd3b30eb9151dd5dfb32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
