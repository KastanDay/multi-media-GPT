{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_name = 'google/flan-t5-xxl'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Context: '\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs.input_ids.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True))\n",
    "print(tokenizer.decode([1193, 6327, 10, 3], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing.parallel_processing.TVQA_eval import TVQA_eval\n",
    "\n",
    "tvqa_eval = TVQA_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = tvqa_eval.get_clip_embed_from_vid_name(\"s08e06_seg02_clip_17\")\n",
    "# print(len(embeddings))\n",
    "# print(embeddings[0].shape)\n",
    "# working ^^ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, question in enumerate(tvqa_eval.train_qa_json):\n",
    "  # if i != 3:\n",
    "  #   continue\n",
    "  print(type(question))\n",
    "  print(question)\n",
    "  vecs, ans = tvqa_eval.create_context_vectors(question)\n",
    "  ans_list = tvqa_eval.get_answers_from_question(question)\n",
    "\n",
    "  print(question)\n",
    "  print(vecs)\n",
    "  print(vecs[0].shape)\n",
    "  print(ans)\n",
    "  print(ans_list)\n",
    "\n",
    "  if i > 5:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(vecs, ans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvqa_eval.get_subtitle_from_clip('s08e06_seg02_clip_17', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvqa_eval.get_subtitle_from_clip('met_s06e09_seg02_clip_15', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tvqa_eval.subtitles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tvqa_eval.train_qa_json[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "ones = torch.ones((1024))\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = torch.ones((1024))\n",
    "context_vector = torch.ones((1024))\n",
    "attn_mask_arr[ != -100] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4273, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "huggingface_model_name = 'google/flan-t5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model_name, return_special_tokens_mask=True)\n",
    "label_tokenized = tokenizer(\"yes\", padding=True, truncation=True).input_ids\n",
    "label_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEPKAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/utils/miniconda3/envs/vpt/lib/python3.8/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.2.19) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/teton/vpt/data/benchmark_datasets/TVQA/_deeplake/mar_28_TVQA_encode_tvqa_whole loaded successfully.\n",
      "\n",
      "Dataset(path='/mnt/teton/vpt/data/benchmark_datasets/TVQA/_deeplake/mar_28_TVQA_encode_tvqa_whole', read_only=True, tensors=['context_vector', 'label'])\n",
      "\n",
      "     tensor       htype          shape           dtype  compression\n",
      "    -------      -------        -------         -------  ------- \n",
      " context_vector  generic  (117730, 1024, 1024)  float32   None   \n",
      "     label        text        (117730, 1)         str     None   \n",
      "941.866\n"
     ]
    }
   ],
   "source": [
    "import deeplake as dl\n",
    "import numpy as np\n",
    "ds = dl.load(\"/mnt/teton/vpt/data/benchmark_datasets/TVQA/_deeplake/mar_28_TVQA_encode_tvqa_whole\", read_only=True)\n",
    "ds.summary()\n",
    "\n",
    "print(ds.size_approx() / 1e9) # bytes to GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024)\n",
      "yes\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "yes\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "yes\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "yes\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n",
      "(1024, 1024)\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(ds):\n",
    "  print(val.context_vector.numpy().shape)\n",
    "  print(val.label.text())\n",
    "  if i > 20: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import deeplake as dl\n",
    "import numpy as np\n",
    "\n",
    "BATCH_NAME = 'tvqa_whole'\n",
    "RESULTS_DATASET_PATH = f'/mnt/teton/vpt/data/benchmark_datasets/TVQA/_deeplake/mar_28_TVQA_encode_{BATCH_NAME}'\n",
    "output_ds = dl.empty(RESULTS_DATASET_PATH, overwrite=True)\n",
    "\n",
    "with output_ds:\n",
    "  output_ds.create_tensor(\"context_vector\", htype=\"generic\", dtype=np.float32, sample_compression=None)\n",
    "  output_ds.create_tensor(\"label\", htype=\"text\", dtype=str, sample_compression=None)\n",
    "  output_ds.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from enum import Enum\n",
    "\n",
    "import accelerate\n",
    "import lovely_tensors as lt\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import T5EncoderModel, T5Tokenizer, logging\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "# suppress: Some weights of the model checkpoint at google/flan-t5-large were not used when initializing model.\n",
    "# This is expected because we're initializing the encoder-only. So the decoder weights are not used.\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# TODO: Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM.\n",
    "os.environ[\"RAY_memory_monitor_refresh_ms\"] = \"0\"  # prevents ray from killing the process when it runs out of memory\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/teton/utils/cache/huggingface'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/mnt/teton/utils/cache/datasets'\n",
    "\n",
    "# chosen_datatype = torch.float16\n",
    "chosen_datatype = torch.float32\n",
    "model_name = \"google/flan-t5-large\"\n",
    "\n",
    "\n",
    "class FlanT5Encoder:\n",
    "\n",
    "  def __init__(self, device: str = \"cuda:1\"):\n",
    "    self.device = device if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"In FlanT5Encoder\", self.device)\n",
    "    self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    self.model = T5EncoderModel.from_pretrained(model_name, torch_dtype=chosen_datatype).to(self.device)\n",
    "    # self.model = T5EncoderModel.from_pretrained(\n",
    "    #     \"google/flan-t5-large\",\n",
    "    #     # set device_map to only cuda:0\n",
    "    #     # device_map=\"sequential\",\n",
    "    #     torch_dtype=torch.float16).to(device)  # I would really like to use this, but I think it's not supported by numpy or something.\n",
    "    # self.model = T5EncoderModel.from_pretrained(\"google/flan-t5-large\",\n",
    "    #                                             device_map=\"auto\",\n",
    "    #                                             torch_dtype=torch.float16,\n",
    "    #                                             max_memory={\n",
    "    #                                                 0: \"24GiB\",\n",
    "    #                                                 1: \"0GiB\",\n",
    "    #                                             })\n",
    "\n",
    "  def encode(self, input_dict):  #batch):\n",
    "    \"\"\"\n",
    "    OLD param batch: list of {'db_index': int, 'caption': str}\n",
    "    param input_dict: dict of {'db_index': int, 'caption': str}. batch_size is 1\n",
    "    return: list of np.arrays, each of different shape [NUM_TOKENS, 1024]\n",
    "\n",
    "    Important to pad to \"max_length\" so we can stack the inputs.\n",
    "    keeping truncation=False for now because we really don't expect to go over length (with 15-word sequences), and I want to see errors if we do.\n",
    "    \"\"\"\n",
    "    last_hidden_states_batch = []\n",
    "    # for input_dict in batch:\n",
    "    with torch.inference_mode():\n",
    "      # print(\"ABOUT TO ENCODE CAPTION: \", input_dict[\"caption\"])\n",
    "      tokens = self.tokenizer(input_dict[\"caption\"], return_tensors=\"pt\", padding=False, truncation=False).to(self.device)\n",
    "      # print(\"TOKENS: \", tokens)\n",
    "      lhs = self.model(**tokens).last_hidden_state\n",
    "\n",
    "      # CAST FROM 32 to 16 bit via .half() !!\n",
    "      lhs = lhs.half().detach().cpu().numpy().reshape(-1, 1024)\n",
    "      # print(\"LAST HIDDEN STATE: \", lhs)\n",
    "      last_hidden_states_batch.append({'last_hidden_states': lhs, 'db_index': input_dict['db_index']})\n",
    "    # return: list of np.arrays, each of different shape [NUM_TOKENS, 1024]\n",
    "    return last_hidden_states_batch\n",
    "\n",
    "  def encode_tvqa(self, sentence, truncate_shape=804):\n",
    "\n",
    "    def pad_or_truncate_tensor(tensor):\n",
    "      target_shape = [truncate_shape, 1024]\n",
    "      tensor_shape = tensor.shape\n",
    "\n",
    "      # If tensor shape is larger than the target shape, truncate the tensor\n",
    "      if tensor_shape[0] > target_shape[0]:\n",
    "        truncated_tensor = tensor[:target_shape[0], :]\n",
    "        return truncated_tensor\n",
    "\n",
    "      # If tensor shape is smaller than the target shape, pad the tensor\n",
    "      elif tensor_shape[0] < target_shape[0]:\n",
    "        padding_shape = (target_shape[0] - tensor_shape[0], target_shape[1])\n",
    "        padded_tensor = torch.nn.functional.pad(tensor, (0, 0, 0, padding_shape[0]), value=-100)\n",
    "        return padded_tensor\n",
    "\n",
    "      # If tensor shape is already the target shape, return the tensor\n",
    "      else:\n",
    "        return tensor\n",
    "\n",
    "    # Tokenize the sentence and convert it to a PyTorch tensor\n",
    "    tokens = self.tokenizer(sentence, return_tensors=\"pt\", padding=False, truncation=False).to(self.device)\n",
    "\n",
    "    # Generate the last hidden layer of the CLIP encoder\n",
    "    lhs = self.model(**tokens).last_hidden_state\n",
    "\n",
    "    # Truncate or pad last hidden states\n",
    "    truncated_states = pad_or_truncate_tensor(lhs.squeeze(0))\n",
    "    truncated_states = truncated_states.cpu()\n",
    "    new_tensor = truncated_states\n",
    "    if model_name == \"google/flan-t5-small\":\n",
    "      new_tensor = torch.full((truncated_states.shape[0], 1024), -100)\n",
    "      # Copy the original tensor's values to the first 512 columns of the new tensor\n",
    "      new_tensor[:, :512] = truncated_states\n",
    "    return new_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = FlanT5Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = d.encode_tvqa(\"hello world\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
