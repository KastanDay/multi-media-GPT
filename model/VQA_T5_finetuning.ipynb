{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping for real run. \n",
    "\n",
    "Todo: \n",
    "* Define `max_source_length` and `max_target_length` for the model (otherwise truncated).\n",
    "padding token should be replaced with -100, which is the 'ignore_index' of `CrossEntorpyLoss` in PT and TF. For Flax, use `decoder_attention_mask`. \n",
    "Attention_mask. ensures madding tokens of inputs are ignored. \n",
    "\n",
    "* Install apex. \"model will automatically use apex.normalization.FusedRMSNorm instead of T5LayerNorm.\" The former uses an optimized fused kernel which is several times faster than the latter.\n",
    "\n",
    "A note on model sizes: \n",
    "T5-11B (original, not v1.1) weights in float32 are 45.2GB. \n",
    "See this post for using huggingface endpoints on SINGLE GPU for cheap inference: https://www.philschmid.de/deploy-t5-11b\n",
    "Uses mixed precision and sharding, and LLM.int8(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel, T5Model, T5Config, AutoModelWithLMHead\n",
    "import accelerate\n",
    "# import wandb\n",
    "from tqdm import tqdm\n",
    "import lovely_tensors as lt\n",
    "import math\n",
    "from PIL import Image\n",
    "lt.monkey_patch()\n",
    "# !wandb login  -- reactivate later\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split VAL2014 into train and test datasets \n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for i, img_name in enumerate(os.listdir(\"/home/kastan/thesis/video-pretrained-transformer/vqa/val2014\")):\n",
    "    # if i >= 38000:\n",
    "    if i >= 39000:\n",
    "        test_set.append(os.path.join(\"/home/kastan/thesis/video-pretrained-transformer/vqa/val2014\", img_name))\n",
    "    # elif i <= 10000:\n",
    "    elif i <= 5000:\n",
    "        train_set.append(os.path.join(\"/home/kastan/thesis/video-pretrained-transformer/vqa/val2014\", img_name))\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
    "''' GLOBALS '''\n",
    "NUM_EPOCHS = 1\n",
    "MODEL_NAME = \"google/t5-v1_1-base\"\n",
    "RUN_NAME = \"all_modalities\"\n",
    "BATCH_SIZE = 1\n",
    "BASE_DIR = '/scratch/bbki/kastanday/whisper'\n",
    "use_scene_graph = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Split VAL2014 into train and test datasets \\n\\nBASE_DIR = \\'/scratch/bbki/kastanday/whisper\\'\\n\\ntrain_set = []\\ntest_set = []\\n\\nfor i, img_name in enumerate(os.listdir(f\"{BASE_DIR}/vqa/val2014\")):\\n    if i >= 39000:\\n        test_set.append(os.path.join(f\"{BASE_DIR}/vqa/val2014\", img_name))\\n    elif i <= 5000:\\n        train_set.append(os.path.join(f\"{BASE_DIR}/vqa/val2014\", img_name))\\n\\nprint(len(train_set))\\nprint(len(test_set))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Split VAL2014 into train and test datasets \n",
    "\n",
    "BASE_DIR = '/scratch/bbki/kastanday/whisper'\n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for i, img_name in enumerate(os.listdir(f\"{BASE_DIR}/vqa/val2014\")):\n",
    "    if i >= 39000:\n",
    "        test_set.append(os.path.join(f\"{BASE_DIR}/vqa/val2014\", img_name))\n",
    "    elif i <= 5000:\n",
    "        train_set.append(os.path.join(f\"{BASE_DIR}/vqa/val2014\", img_name))\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "Initializing clip and scene graph models...\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: \", device)\n",
    "\n",
    "# Initialize preprocessing models for collate_fn \n",
    "print(\"Initializing clip and scene graph models...\")\n",
    "clip_model, clip_preprocess = clip.load('ViT-L/14@336px', device)\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-base\", return_special_tokens_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_imgs = []\n",
    "    input_questions = []\n",
    "    answers = []\n",
    "    scene_strs = []\n",
    "    question_ids = []\n",
    "    for elt in batch:\n",
    "        img, question, answer, scene_graph_str, question_id = elt\n",
    "        input_imgs.append(clip_preprocess(Image.fromarray(img)).unsqueeze(0))\n",
    "        input_questions.append(question)\n",
    "        answers.append(answer)\n",
    "        scene_strs.append(scene_graph_str)\n",
    "        question_ids.append(question_id)\n",
    "\n",
    "    image_input = torch.cat(input_imgs).to(device)\n",
    "    text_input = clip.tokenize(input_questions, truncate=True).to(device)\n",
    "    sg_input = clip.tokenize(scene_strs, truncate=True).to(device)\n",
    "\n",
    "    with torch.inference_mode(): # even faster than no_grad()\n",
    "        # image_features = torch.unsqueeze(clip_model.encode_image(image_input), dim=1)\n",
    "        # text_features = torch.unsqueeze(clip_model.encode_text(text_input), dim=1)\n",
    "        # sg_features = torch.unsqueeze(clip_model.encode_text(sg_input), dim=1)\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "        sg_features = clip_model.encode_text(sg_input)\n",
    "\n",
    "    # labels = t5_tokenizer(answers, padding=\"longest\", max_length=128, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    # labels[labels == t5_tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # print(\"answers: \", answers)\n",
    "    labels = t5_tokenizer(answers, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # print(\"labels: \", labels)\n",
    "\n",
    "    return image_features, text_features, sg_features, labels, question_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing train dataset...\n",
      "Stored annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214354/214354 [00:12<00:00, 17268.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing question to answer dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26297/26297 [00:00<00:00, 2538787.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed question to answer dictionary...\n",
      "Processing scene graph json...\n",
      "Created img name to scene graph mapping...\n",
      "Stored questions...\n",
      "Created question ID to question mapping...\n",
      "train dataset contains 26297 annotations\n",
      "Constructing test dataset...\n",
      "Stored annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214354/214354 [00:04<00:00, 50840.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing question to answer dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8134/8134 [00:00<00:00, 2290156.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed question to answer dictionary...\n",
      "Processing scene graph json...\n",
      "Created img name to scene graph mapping...\n",
      "Stored questions...\n",
      "Created question ID to question mapping...\n",
      "test dataset contains 8134 annotations\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class VQA(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations, img_path, mode=\"train\"):\n",
    "        self.annotations = [] \n",
    "        self.questions = None\n",
    "        self.img_path = img_path\n",
    "        self.qid_to_question = {}\n",
    "        self.mode = mode\n",
    "\n",
    "        self.img_to_scene_graph = {}\n",
    "        self.question_to_answer = {}\n",
    "\n",
    "        print(f\"Constructing {self.mode} dataset...\")\n",
    "        with open(annotations[f\"{self.mode}_annotations\"]) as f:\n",
    "            all_annotations = json.load(f)['annotations']\n",
    "            print(\"Stored annotations...\")\n",
    "\n",
    "            for annotation in tqdm(all_annotations):\n",
    "                image_id = annotation[\"image_id\"]\n",
    "                padded_image_id = \"\".join((12 - len(str(image_id)))*[\"0\"]) + str(image_id)\n",
    "                img_path = os.path.join(self.img_path, f\"COCO_{self.pseudo_mode}2014_{padded_image_id}.jpg\")\n",
    "                \n",
    "                # if annotation[\"answer_type\"] == \"yes/no\":\n",
    "                if img_set is not None:\n",
    "                    if img_path in img_set:\n",
    "                        self.annotations.append(annotation)\n",
    "                else:\n",
    "                    self.annotations.append(annotation)\n",
    "            self.annotations = all_annotations\n",
    "\n",
    "        self.construct_question_to_answer_dict()\n",
    "\n",
    "        with open(annotations[f\"{self.mode}_scene_graph_json\"]) as f:\n",
    "            print(\"Processing scene graph json...\")\n",
    "            for obj in f:\n",
    "                json_obj = json.loads(eval(eval(obj)))\n",
    "                if str(json_obj[\"input_img_path\"]) not in self.img_to_scene_graph:\n",
    "                    self.img_to_scene_graph[str(json_obj[\"input_img_path\"])] = str(json_obj[\"scene_graph_string\"])\n",
    "                else:\n",
    "                    print(\"Duplicate scene graphs exist!\")\n",
    "\n",
    "            print(\"Created img name to scene graph mapping...\")\n",
    "\n",
    "       \n",
    "        with open(annotations[f\"{self.mode}_questions\"]) as f:\n",
    "            self.questions = json.load(f)[\"questions\"]\n",
    "            print(f\"Stored questions...\")\n",
    "            for question in self.questions:\n",
    "                self.qid_to_question[question[\"question_id\"]] = question[\"question\"]\n",
    "            print(\"Created question ID to question mapping...\")\n",
    "            \n",
    "        \n",
    "\n",
    "        print(f\"{self.mode} dataset contains {len(self.annotations)} annotations\")\n",
    "\n",
    "    \n",
    "    def construct_question_to_answer_dict(self):\n",
    "        print(\"Constructing question to answer dictionary...\")\n",
    "        for annotation in tqdm(self.annotations):\n",
    "            self.question_to_answer[annotation[\"question_id\"]] = annotation\n",
    "            \n",
    "        print(\"Constructed question to answer dictionary...\")\n",
    "\n",
    "    def get_question_to_answer_dict(self):\n",
    "        return self.question_to_answer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        curr_annotation = self.annotations[idx]\n",
    "\n",
    "        question_id = curr_annotation[\"question_id\"]\n",
    "        image_id = curr_annotation[\"image_id\"]\n",
    "        answers = curr_annotation[\"answers\"]\n",
    "\n",
    "        # Don't need these for now but may need them for future ablations\n",
    "        # question_type = curr_annotation[\"question_type\"]\n",
    "        # answer_type = curr_annotation[\"answers\"]\n",
    "        # multiple_choice_answer = curr_annotation[\"multiple_choice_answer\"]\n",
    "\n",
    "        # VQA has multiple possible answers, can modify this later to use other answers\n",
    "        answer_choice = answers[0][\"answer\"]\n",
    "        \n",
    "        img = None\n",
    "\n",
    "        question = self.qid_to_question.get(question_id, None)\n",
    "\n",
    "        assert question is not None\n",
    "\n",
    "        padded_image_id = \"\".join((12 - len(str(image_id)))*[\"0\"]) + str(image_id)\n",
    "\n",
    "        img_path = os.path.join(self.img_path, f\"COCO_{self.mode}2014_{padded_image_id}.jpg\")\n",
    "\n",
    "        assert os.path.exists(img_path)\n",
    "            \n",
    "        img = np.asarray(Image.open(img_path))\n",
    "\n",
    "        scene_graph_str = self.img_to_scene_graph.get(img_path, None)\n",
    "       \n",
    "        assert scene_graph_str is not None\n",
    "        \n",
    "        return img, question, answer_choice, scene_graph_str, question_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "# we use the original val set as our test set since we have the annotations available (test annotations not available)\n",
    "# this will be useful for error analysis later on\n",
    "\n",
    "annotations = {\n",
    "    \"train_questions\": f\"{BASE_DIR}/vqa/v2_OpenEnded_mscoco_train2014_questions.json\",\n",
    "    \"test_questions\": f\"{BASE_DIR}/vqa/v2_OpenEnded_mscoco_test2014_questions.json\",\n",
    "    \"train_annotations\": f\"{BASE_DIR}/vqa/v2_mscoco_train2014_annotations.json\",\n",
    "    \"test_annotations\": f\"{BASE_DIR}/vqa/v2_mscoco_test2014_annotations.json\",\n",
    "    \"train_scene_graph_json\": f\"{BASE_DIR}/vqa/train_scene_graph.json\",\n",
    "    \"test_scene_graph_json\": f\"{BASE_DIR}/vqa/test_scene_graph.json\"\n",
    "}\n",
    "\n",
    "train_dataset = VQA(annotations, f\"{BASE_DIR}/vqa/train2014\", mode=\"train\")\n",
    "# test_dataset = VQA(annotations, f\"{BASE_DIR}/vqa/test2014\", mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing t5 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/26297 [00:01<12:52:42,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  7.837615489959717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/26297 [00:06<2:42:45,  2.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39misnan(loss)\u001b[39m.\u001b[39many()\n\u001b[1;32m     61\u001b[0m iter_losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m---> 63\u001b[0m loss\u001b[39m.\u001b[39;49msum()\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     65\u001b[0m \u001b[39m# torch.nn.utils.clip_grad_norm_(t5.parameters(), 1e-2)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/eval_clip_t5_torch19/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "MODEL_NAME = \"google/t5-v1_1-base\"\n",
    "RUN_NAME = \"scene_graph_ablation\"\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "print(\"Initializing t5 model...\")\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# t5 = T5ForConditionalGeneration.from_pretrained(\"/home/kastan/thesis/video-pretrained-transformer/model/yt_pretrain_vqa_val_finetune\", torch_dtype=torch.float32, low_cpu_mem_usage=True).to(device)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(\"/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/youtube_pretrained_yesno_iter19500\", torch_dtype=torch.float32, low_cpu_mem_usage=True).to(device)\n",
    "print(\"Initializing t5 model...\")\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=True).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "\n",
    "t5.train()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=t5.parameters(), lr=1e-4)\n",
    "\n",
    "iter_losses = []\n",
    "\n",
    "one_input_shape = [BATCH_SIZE, 768, 768]\n",
    "att_mask_shape = [BATCH_SIZE, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0, 0] = 1\n",
    "attn_mask_arr[0, 1] = 1\n",
    "# attn_mask_arr[0, 2] = 1\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) \n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        if i % 500 == 0:\n",
    "            t5.save_pretrained(f\"{BASE_DIR}/vqa/model_ckpts/{RUN_NAME}_iter{i}\")\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        question_embed, img_embed, scene_graph_embed, labels, _ = batch\n",
    "\n",
    "        question_embed = question_embed.to(device)\n",
    "        img_embed = img_embed.to(device)\n",
    "        scene_graph_embed = scene_graph_embed.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        input_embeds_arr[0, 0, :] = img_embed\n",
    "        input_embeds_arr[0, 1, :] = question_embed\n",
    "        # input_embeds_arr[0, 2, :] = scene_graph_embed\n",
    "\n",
    "        # labels = t5_tokenizer(\"hi my name is pranav\", return_tensors=\"pt\").input_ids.to(device)\n",
    "        # print(\"labels shaep: \", labels.shape)\n",
    "\n",
    "        outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        assert not torch.isnan(loss).any()\n",
    "\n",
    "        iter_losses.append(loss)\n",
    "\n",
    "        loss.sum().backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(t5.parameters(), 1e-2)\n",
    "\n",
    "        optimizer.step()\n",
    "        # print(\"✅ Successful training iteration\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Loss: \", loss.item())\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch} done.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5.save_pretrained(f\"/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/{RUN_NAME}_iter{19222}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19222\n"
     ]
    }
   ],
   "source": [
    "print(len(iter_losses))\n",
    "\n",
    "new_iter_losses = [str(t.item()) for t in iter_losses]\n",
    "\n",
    "# print(new_iter_losses)\n",
    "file1 = open('/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/iter_losses_youtube_pretrained.txt', 'w')\n",
    "file1.writelines(\",\".join(new_iter_losses))\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8134 [00:00<20:06,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the man doing in the street?\n",
      "answers:  [['crossing it', 'walking', 'walking', 'crossing', 'crossing road', 'walking', 'crossing', 'walking', 'crossing', 'walking']]\n",
      "outputs:  ['no']\n",
      "img:  393226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 102/8134 [00:14<19:25,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the player wearing around his head?\n",
      "answers:  [['hat', 'cap', 'hat', 'cap', 'cap', 'hat', 'hat', 'hat', 'hat', 'hat']]\n",
      "outputs:  ['no']\n",
      "img:  885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 202/8134 [00:29<19:20,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Where would luggage go?\n",
      "answers:  [['in back of train', 'on top', 'inside', 'under seats', 'roof rack', 'top', 'top', 'roof rack', 'on top of bus', 'on roof']]\n",
      "outputs:  ['no']\n",
      "img:  395113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 302/8134 [00:43<18:54,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What does the red sign with the yellow M mean?\n",
      "answers:  [[\"mcdonald's\", \"mcdonald's\", 'mcdonalds', 'mcdonalds', \"mcdonald's\", \"mcdonald's\", 'mcdonald', \"mcdonald's\", \"mcdonald's restaurant ahead\", \"mcdonald's\"]]\n",
      "outputs:  ['no']\n",
      "img:  134870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 402/8134 [00:57<18:42,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Are the motorcycles parked legally?\n",
      "answers:  [['yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes']]\n",
      "outputs:  ['no']\n",
      "img:  529698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 502/8134 [01:12<18:01,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the player's number?\n",
      "answers:  [['46', '46', '46', '46', '46', '46', '46', '46', '46', '46']]\n",
      "outputs:  ['no']\n",
      "img:  138553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 602/8134 [01:26<17:37,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What type of place is this?\n",
      "answers:  [['school', 'school', 'school', 'outdoors', 'school', 'outdoors', 'school', 'school', \"boy's school\", 'school']]\n",
      "outputs:  ['no']\n",
      "img:  9420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 702/8134 [01:40<17:50,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  How many pastries are in this picture?\n",
      "answers:  [['0', '0', '47', '30', '0', '63', '50', 'lot', '20', '0']]\n",
      "outputs:  ['no']\n",
      "img:  404243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 802/8134 [01:55<17:25,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What creature is depicted on the yellow sign?\n",
      "answers:  [['dragon', 'dragon', 'snake', 'dragon', 'dragon', 'dragon', 'dragon', 'dragon', 'snake', 'dragon']]\n",
      "outputs:  ['no']\n",
      "img:  537620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 902/8134 [02:09<17:13,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  How many people are in this picture?\n",
      "answers:  [['5', '5', '5', '5', '4', '5', '5', '5', '5', '5']]\n",
      "outputs:  ['no']\n",
      "img:  276673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1002/8134 [02:24<17:11,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  How large is this living space?\n",
      "answers:  [['small hospital room', 'small', 'small', 'small', 'small', 'fifty square feet', 'short', 'small', '10x12', 'small']]\n",
      "outputs:  ['no']\n",
      "img:  409116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 1102/8134 [02:38<16:49,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the person in red holding?\n",
      "answers:  [['plate', 'plate of food', 'plate', 'plate of food', 'plate', 'her dinner plate', 'plate', 'plate', 'plate of food', 'food']]\n",
      "outputs:  ['no']\n",
      "img:  279491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1202/8134 [02:52<16:39,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Do the tires look matching?\n",
      "answers:  [['yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no']]\n",
      "outputs:  ['no']\n",
      "img:  150129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1302/8134 [03:07<16:42,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is on the ground?\n",
      "answers:  [['leaves', 'leaves', 'fire hydrant', 'fire hydrant', 'leaves', 'leaves', 'leaves', 'leaves', 'fire hydrant', 'leaves']]\n",
      "outputs:  ['no']\n",
      "img:  545000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1402/8134 [03:21<16:41,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Is there a glare on the glass?\n",
      "answers:  [['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes']]\n",
      "outputs:  ['no']\n",
      "img:  153901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1502/8134 [03:36<15:46,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What fruit is hanging from the wall?\n",
      "answers:  [['0', '0', '0', '0', '0', '0', 'curtain', 'banana', '0', 'pizza']]\n",
      "outputs:  ['no']\n",
      "img:  286981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1602/8134 [03:50<15:50,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Is the person holding the umbrella real?\n",
      "answers:  [['no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no']]\n",
      "outputs:  ['no']\n",
      "img:  179199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1702/8134 [04:05<15:15,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Is this belt in motion?\n",
      "answers:  [['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes']]\n",
      "outputs:  ['no']\n",
      "img:  422280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1802/8134 [04:19<15:05,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is this person doing?\n",
      "answers:  [['taking picture', 'talking', 'taking pic', 'taking picture', 'taking picture', 'looking at cell phone', 'looking at their phone', 'taking photo', 'taking photo', 'taking picture']]\n",
      "outputs:  ['no']\n",
      "img:  292488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1902/8134 [04:34<14:54,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What color are the stripes?\n",
      "answers:  [['white', 'white', 'white', 'white', 'white', 'white', 'blue', 'white', 'black and white', 'white']]\n",
      "outputs:  ['no']\n",
      "img:  376667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2000/8134 [04:48<14:44,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.2389999999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "BATCH_SIZE = 1\n",
    "MODEL_STR = \"google/t5-v1_1-base\"\n",
    "# t5_eval = T5ForConditionalGeneration.from_pretrained(\"/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/all_modalities_iter19401\", torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "# t5_eval = T5ForConditionalGeneration.from_pretrained(\"/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/youtube_pretrained_iter19222\", torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "t5_eval = T5ForConditionalGeneration.from_pretrained(\"/home/kastan/thesis/video-pretrained-transformer/vqa/model_ckpts/scene_graph_ablation_iter20000\", torch_dtype=torch.float32, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_STR, return_special_tokens_mask=True)\n",
    "\n",
    "question_to_answer = test_dataset.get_question_to_answer_dict()\n",
    "\n",
    "t5_eval.eval()\n",
    "\n",
    "# TOOD: add in val dataset here. and check check for if use_scenegraph\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "iter_accuracies = []\n",
    "\n",
    "one_input_shape = [BATCH_SIZE, 768, 768]\n",
    "att_mask_shape = [BATCH_SIZE, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0, 0] = 1\n",
    "attn_mask_arr[0, 1] = 1\n",
    "attn_mask_arr[0, 2] = 1\n",
    "\n",
    "cnt = 0\n",
    "for i, batch in enumerate(tqdm(test_loader)):\n",
    "\n",
    "    if cnt >= 2000:\n",
    "        break\n",
    "    \n",
    "    cnt += 1\n",
    "    \n",
    "    question_embed, img_embed, scene_graph_embed, labels, question_ids = batch\n",
    "\n",
    "    answers = []\n",
    "    for question_id in question_ids:\n",
    "        answers.append([answer_obj[\"answer\"] for answer_obj in question_to_answer[question_id][\"answers\"]])\n",
    "\n",
    "    # To view the image, question and possible answers, uncomment\n",
    "    # image_id = question_to_answer[question_id][\"image_id\"]\n",
    "    # padded_image_id = \"\".join((12 - len(str(image_id)))*[\"0\"]) + str(image_id)\n",
    "    # img_path = os.path.join(self.img_path, f\"COCO_{self.pseudo_mode}2014_{padded_image_id}.jpg\")\n",
    "    # print(img_path)\n",
    "    # print(\"question: \", test_dataset.qid_to_question[question_ids[0]])\n",
    "    # print(answers)\n",
    "\n",
    "    question_embed = question_embed.to(device)\n",
    "    img_embed = img_embed.to(device)\n",
    "    scene_graph_embed = scene_graph_embed.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    input_embeds_arr[0, 0, :] = img_embed\n",
    "    input_embeds_arr[0, 1, :] = question_embed\n",
    "    input_embeds_arr[0, 2, :] = scene_graph_embed\n",
    "\n",
    "    # input_embeds_arr = torch.cat((question_embed, img_embed, scene_graph_embed), dim=1)\n",
    "\n",
    "    # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "    output_sequences = t5_eval.generate(inputs_embeds=input_embeds_arr,  attention_mask=attn_mask_arr, do_sample=False)\n",
    "    \n",
    "    # outputs = ['no']\n",
    "        print(\"outputs: \", outputs)\n",
    "        print(\"img: \", test_dataset.question_to_answer[question_ids[0]][\"image_id\"])\n",
    "\n",
    "    for j, output in enumerate(outputs):\n",
    "        curr_answers = answers[j]\n",
    "\n",
    "        # evaluation metric for VQA: https://visualqa.org/evaluation.html\n",
    "        iter_accuracies.append(min(curr_answers.count(str(output)) / 3.0, 1.0))\n",
    "\n",
    "print(\"Accuracy: \", sum(iter_accuracies)/len(iter_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(iter_accuracies)/len(iter_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('v4_clip_preprocessing_yt1b')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d3db6712dfc0a86429b8fb30f027b0a323b7f50feb5f633548a08dcfe308692"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
