{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping for real run. \n",
    "\n",
    "Todo: \n",
    "* Define `max_source_length` and `max_target_length` for the model (otherwise truncated).\n",
    "padding token should be replaced with -100, which is the 'ignore_index' of `CrossEntorpyLoss` in PT and TF. For Flax, use `decoder_attention_mask`. \n",
    "Attention_mask. ensures madding tokens of inputs are ignored. \n",
    "\n",
    "* Install apex. \"model will automatically use apex.normalization.FusedRMSNorm instead of T5LayerNorm.\" The former uses an optimized fused kernel which is several times faster than the latter.\n",
    "\n",
    "A note on model sizes: \n",
    "T5-11B (original, not v1.1) weights in float32 are 45.2GB. \n",
    "See this post for using huggingface endpoints on SINGLE GPU for cheap inference: https://www.philschmid.de/deploy-t5-11b\n",
    "Uses mixed precision and sharding, and LLM.int8(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config, AutoModelWithLMHead\n",
    "import accelerate\n",
    "import wandb\n",
    "import lovely_tensors as lt\n",
    "import math\n",
    "from PIL import Image\n",
    "lt.monkey_patch()\n",
    "# !wandb login  -- reactivate later\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODEL SELECTION\n",
    "\n",
    "T5 V1.1 --  https://huggingface.co/docs/transformers/model_doc/t5v1.1 && https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511\n",
    "small - base - large - 3b/xl - 11b/xxl\n",
    "\n",
    "OG: t5-small\n",
    "\n",
    "'google/t5-base-lm-adapt' # largest on my server (without float16)\n",
    "'google/t5-xl-lm-adapt'\n",
    "\n",
    "google/t5-v1_1-large\n",
    "'''\n",
    "\n",
    "# MODEL_SIZE = \"t5-base\"\n",
    "MODEL_NAME = \"google/t5-small-lm-adapt\"\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) — Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing train dataset...\n",
      "Stored annotations...\n",
      "Stored questions...\n",
      "Created question ID to question mapping...\n",
      "Constructing test dataset...\n",
      "Stored annotations...\n",
      "Stored questions...\n",
      "Created question ID to question mapping...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class VQA(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations, img_path, mode=\"train\"):\n",
    "        self.annotations = None \n",
    "        self.questions = None\n",
    "        self.img_path = img_path\n",
    "        self.qid_to_question = {}\n",
    "        self.mode = mode\n",
    "\n",
    "        print(f\"Constructing {self.mode} dataset...\")\n",
    "        with open(annotations[f\"{self.mode}_annotations\"]) as f:\n",
    "            self.annotations = json.load(f)['annotations']\n",
    "            print(\"Stored annotations...\")\n",
    "\n",
    "        with open(annotations[f\"{self.mode}_questions\"]) as f:\n",
    "            self.questions = json.load(f)[\"questions\"]\n",
    "            print(\"Stored questions...\")\n",
    "            for question in self.questions:\n",
    "                self.qid_to_question[question[\"question_id\"]] = question[\"question\"]\n",
    "            print(\"Created question ID to question mapping...\")\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(\"google/t5-small-lm-adapt\", return_special_tokens_mask=True)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        curr_annotation = self.annotations[idx]\n",
    "\n",
    "        question_id = curr_annotation[\"question_id\"]\n",
    "        image_id = curr_annotation[\"image_id\"]\n",
    "        question_type = curr_annotation[\"question_type\"]\n",
    "        answer_type = curr_annotation[\"answers\"]\n",
    "        multiple_choice_answer = curr_annotation[\"multiple_choice_answer\"]\n",
    "        answers = curr_annotation[\"answers\"]\n",
    "\n",
    "        img = None\n",
    "        scene_graph = None \n",
    "\n",
    "        question = self.qid_to_question.get(question_id, None)\n",
    "        if question == None:\n",
    "            print(\"question doesn't exist\")\n",
    "            return question, img, answers\n",
    "\n",
    "        padded_image_id = \"\".join((12 - len(str(image_id)))*[\"0\"]) + str(image_id)\n",
    "        print(\"image id: \", padded_image_id)\n",
    "        pseudo_mode = \"train\" if self.mode == \"train\" else \"val\"\n",
    "\n",
    "        if not os.path.exists(os.path.join(self.img_path, f\"COCO_{pseudo_mode}2014_{padded_image_id}.jpg\")):\n",
    "            print(\"image doesn't exist\")\n",
    "            return question, img, answers\n",
    "            \n",
    "        img = Image.open(os.path.join(self.img_path, f\"COCO_{pseudo_mode}2014_{padded_image_id}.jpg\"))\n",
    "        img = np.asarray(img)\n",
    "\n",
    "        # TODO: Add in scene graph extraction code, random for now\n",
    "        one_input_shape = [1, 512]\n",
    "        scene_graph = np.random.rand( *one_input_shape ).astype(np.float16) \n",
    "        scene_graph = torch.from_numpy(scene_graph)\n",
    "\n",
    "        # TODO: Add in clip preprocessing code, random for now\n",
    "        one_input_shape = [1, 512]\n",
    "        clip_img = np.random.rand( *one_input_shape ).astype(np.float16) \n",
    "        clip_img = torch.from_numpy(clip_img)\n",
    "\n",
    "        clip_text = np.random.rand( *one_input_shape ).astype(np.float16) \n",
    "        clip_text = torch.from_numpy(clip_text)\n",
    "\n",
    "\n",
    "        # print(\"real answers: \", answers)\n",
    "        \n",
    "        return clip_text, clip_img, answers[0][\"answer\"], scene_graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "# we use the original val set as our test set since we have the annotations available (test annotations not available)\n",
    "# this will be useful for error analysis later on\n",
    "\n",
    "annotations = {\n",
    "    \"train_questions\": \"/home/kastan/thesis/video-pretrained-transformer/vqa/v2_OpenEnded_mscoco_train2014_questions.json\",\n",
    "    \"test_questions\": \"/home/kastan/thesis/video-pretrained-transformer/vqa/v2_OpenEnded_mscoco_val2014_questions.json\",\n",
    "    \"train_annotations\": \"/home/kastan/thesis/video-pretrained-transformer/vqa/v2_mscoco_train2014_annotations.json\",\n",
    "    \"test_annotations\": \"/home/kastan/thesis/video-pretrained-transformer/vqa/v2_mscoco_val2014_annotations.json\"\n",
    "}\n",
    "\n",
    "train_dataset = VQA(annotations, \"/home/kastan/thesis/video-pretrained-transformer/vqa/train2014\", mode=\"train\")\n",
    "test_dataset = VQA(annotations, \"/home/kastan/thesis/video-pretrained-transformer/vqa/val2014\", mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image id:  000000080818\n",
      "image id:  000000461181\n",
      "image id:  000000042297\n",
      "image id:  000000571780\n",
      "image id:  000000509482\n",
      "image id:  000000558678\n",
      "image id:  000000259687\n",
      "image id:  000000260035\n",
      "image id:  000000296809\n",
      "image id:  000000094416\n",
      "image id:  000000189975\n",
      "image id:  000000470532\n",
      "image id:  000000393661\n",
      "image id:  000000306722\n",
      "image id:  000000317756\n",
      "image id:  000000182373\n",
      "input embeds arr:  torch.Size([16, 3, 512])\n",
      "labels shape:  torch.Size([16, 4])\n",
      "tensor f16 grad NllLossBackward0 cuda:0 9.734\n",
      "✅ Successful training iteration\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m✅ Successful training iteration\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "MODEL_NAME = \"google/t5-small-lm-adapt\"\n",
    "\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "\n",
    "t5.train()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=t5.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        question_embed, img_embed, answers, scene_graph_embed = batch\n",
    "        # input_ids = batch['input_ids'].to(device)\n",
    "        # attention_mask = batch['attention_mask'].to(device)\n",
    "        # labels = batch['labels'].to(device)\n",
    "\n",
    "        # print(\"question shape: \", question.shape)\n",
    "        # print(\"img shape: \", img.shape)\n",
    "        # print(\"scene graph shape; \", scene_graph.shape)\n",
    "        # print(\"answers shape: \", answers.shape)\n",
    "\n",
    "        question_embed = question_embed.to(device)\n",
    "        img_embed = img_embed.to(device)\n",
    "        scene_graph_embed = scene_graph_embed.to(device)\n",
    "        \n",
    "\n",
    "        # TODO: Clean up answer pickng and tokenization\n",
    "        labels = tokenizer(list(answers), padding=\"longest\", max_length=128, truncation=True, return_tensors=\"pt\").input_ids\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        input_embeds_arr = torch.cat((question_embed, img_embed, scene_graph_embed), dim=1)\n",
    "        print(\"input embeds arr: \", input_embeds_arr.shape)\n",
    "        # print(\"answers: \", answers)\n",
    "        print(\"labels shape: \", labels.shape)\n",
    "\n",
    "        # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "        outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "        # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.shape\n",
    "        print(loss)\n",
    "\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        print(\"✅ Successful training iteration\")\n",
    "        1/0\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image id:  000000262148\n",
      "input embeds arr:  torch.Size([1, 3, 512])\n",
      "labels shape:  torch.Size([1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mbatch_decode(output_sequences, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# loss = outputs[0]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# loss.shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# optimizer.step()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# print(\"✅ Successful training iteration\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/VQA_T5_finetuning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "t5.eval()\n",
    "\n",
    "for batch in test_loader:\n",
    "\n",
    "    question_embed, img_embed, answers, scene_graph_embed = batch\n",
    "    # input_ids = batch['input_ids'].to(device)\n",
    "    # attention_mask = batch['attention_mask'].to(device)\n",
    "    # labels = batch['labels'].to(device)\n",
    "\n",
    "    # print(\"question shape: \", question.shape)\n",
    "    # print(\"img shape: \", img.shape)\n",
    "    # print(\"scene graph shape; \", scene_graph.shape)\n",
    "    # print(\"answers shape: \", answers.shape)\n",
    "\n",
    "    question_embed = question_embed.to(device)\n",
    "    img_embed = img_embed.to(device)\n",
    "    scene_graph_embed = scene_graph_embed.to(device)\n",
    "    \n",
    "\n",
    "    # TODO: Clean up answer pickng and tokenization\n",
    "    labels = tokenizer(list(answers), padding=\"longest\", max_length=128, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    input_embeds_arr = torch.cat((question_embed, img_embed, scene_graph_embed), dim=1)\n",
    "    print(\"input embeds arr: \", input_embeds_arr.shape)\n",
    "    # print(\"answers: \", answers)\n",
    "    print(\"labels shape: \", labels.shape)\n",
    "\n",
    "    # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "    # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "    output_sequences = t5.generate(inputs_embeds=input_embeds_arr, do_sample=False)\n",
    "\n",
    "    print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))\n",
    "    # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "    # loss = outputs[0]\n",
    "    # loss.shape\n",
    "    # print(loss)\n",
    "\n",
    "    # loss.sum().backward()\n",
    "    # optimizer.step()\n",
    "    # print(\"✅ Successful training iteration\")\n",
    "    1/0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[1, 512, 512] f16 n=262144 x∈[5.901e-06, 1.000] μ=0.499 σ=inf cuda:0\n",
      "tensor[1, 512, 512] f16 n=262144 x∈[1.252e-06, 1.000] μ=0.500 σ=inf cuda:0\n",
      "tensor[1, 512, 512] f64 n=262144 x∈[1.000, 1.000] μ=1.000 σ=0. cuda:0\n",
      "tensor[1, 7] i64 x∈[1, 5295] μ=1.130e+03 σ=1.940e+03 cuda:0 [[37, 5295, 1782, 410, 8, 378, 1]]\n"
     ]
    }
   ],
   "source": [
    "''' PREP EMBEDDING INPUTS '''\n",
    "# shape = (batch_size, 'words', embedding_dim) -- here 'words' == each of our embeddings, like clip and language.\n",
    "# one_input_shape = [6, 512, 512]\n",
    "one_input_shape = [1, 512, 512]\n",
    "att_mask_shape = [1, 512]\n",
    "\n",
    "decoder_input_embeds_arr = np.random.rand( *one_input_shape ).astype(np.float16) # need fp32\n",
    "decoder_input_embeds_arr = torch.from_numpy(decoder_input_embeds_arr).to(device)\n",
    "input_embeds_arr = np.random.rand( *one_input_shape ).astype(np.float16)\n",
    "input_embeds_arr = torch.from_numpy(input_embeds_arr).to(device)\n",
    "attn_mask_arr = np.ones( att_mask_shape )\n",
    "attn_mask_arr = torch.from_numpy(attn_mask_arr).to(device)\n",
    "\n",
    "print(decoder_input_embeds_arr)\n",
    "print(input_embeds_arr)\n",
    "print(attn_mask_arr)\n",
    "\n",
    "''' Decoder gets the tokenized caption. Shape is (batch_size, max_caption_length). Use padding to make it fit. '''\n",
    "# WORKING example, but easier with numpy.\n",
    "# import torch.nn.functional as F\n",
    "# decoder_input_ids = tokenizer(\"This is the target output sentence, aka the video caption. I like tacos because they are so delicious.\", return_tensors=\"pt\").input_ids.to(device)\n",
    "# decoder_input_ids = F.pad(decoder_input_ids, (0, (512-decoder_input_ids.shape[1])), value=tokenizer.pad_token_id)\n",
    "# print(decoder_input_ids.shape)\n",
    "# decoder_input_ids\n",
    "\n",
    "labels = tokenizer(\"The cute dog did the things\", return_tensors=\"pt\").input_ids.to(device)\n",
    "# labels = torch.from_numpy(np.random.randint(1, 10_000, size=(one_input_shape[0], one_input_shape[2]))).to(device)\n",
    "print(labels)\n",
    "# labels = torch.cat((labels, torch.ones((1, 512-7), dtype=int).to(device)), dim=1)\n",
    "# print(labels)\n",
    "\n",
    "# labels = torch.from_numpy(np.random.randint(1, 10_000, size=(one_input_shape[0], one_input_shape[2]))).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function\n",
    "\n",
    "T5 forward() docs: https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5ForConditionalGeneration.forward\n",
    "\n",
    "Todo: investigate difference between decoder `decoder_input_ids` and `lm_labels`.\n",
    "For example: \n",
    "```\n",
    "outputs = t5(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
    "```\n",
    "\n",
    "I think `loss.sum()` is for multi-iteration loss. I was inadverdently using it 6 batches.\n",
    "https://discuss.pytorch.org/t/loss-backward-raises-error-grad-can-be-implicitly-created-only-for-scalar-outputs/12152 \n",
    "loss.backward() # T5 RuntimeError: grad can be implicitly created only for scalar outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "In transformer stack layer: \n",
      "input embeds:  torch.Size([1, 512, 512])\n",
      "batch size, seq length:  1 512\n",
      "mask seq length:  512\n",
      "past key values:  [None, None, None, None, None, None, None, None]\n",
      "extended attention mask:  tensor[1, 1, 512, 512] f16 \u001b[38;2;127;127;127mall_zeros\u001b[0m cuda:0\n",
      "head_mask:  [None, None, None, None, None, None, None, None]\n",
      "cross_attn_head_mask:  [None, None, None, None, None, None, None, None]\n",
      "present key value states:  None\n",
      "all hidden states:  None\n",
      "all attentions:  None\n",
      "all cross attentions:  None\n",
      "hidden states:  tensor[1, 512, 512] f16 n=262144 x∈[0., 1.111] μ=0.500 σ=inf cuda:0\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 6)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "final hidden states:  tensor[1, 512, 512] f16 n=262144 x∈[-0.857, 0.752] μ=0.000 σ=0.142 grad NativeDropoutBackward0 cuda:0\n",
      "encoder outputs:  BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor[1, 512, 512] f16 n=262144 x∈[-0.857, 0.752] μ=0.000 σ=0.142 grad NativeDropoutBackward0 cuda:0, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
      "hidden_states:  tensor[1, 512, 512] f16 n=262144 x∈[-0.857, 0.752] μ=0.000 σ=0.142 grad NativeDropoutBackward0 cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In transformer stack layer: \n",
      "input embeds:  torch.Size([1, 7, 512])\n",
      "batch size, seq length:  1 7\n",
      "mask seq length:  7\n",
      "attnetion mask:  tensor[1, 7] x∈[1.000, 1.000] μ=1.000 σ=0. cuda:0 [[1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000]]\n",
      "past key values:  [None, None, None, None, None, None, None, None]\n",
      "extended attention mask:  tensor[1, 1, 7, 7] f16 n=49 x∈[-6.550e+04, 0.] μ=-2.808e+04 σ=inf cuda:0\n",
      "poggers\n",
      "head_mask:  [None, None, None, None, None, None, None, None]\n",
      "cross_attn_head_mask:  [None, None, None, None, None, None, None, None]\n",
      "present key value states:  ()\n",
      "all hidden states:  None\n",
      "all attentions:  None\n",
      "all cross attentions:  None\n",
      "hidden states:  tensor[1, 7, 512] f16 n=3584 x∈[-160.000, 80.562] μ=-0.336 σ=inf grad NativeDropoutBackward0 cuda:0\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 6)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (512) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m t5\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m outputs \u001b[39m=\u001b[39m t5\u001b[39m.\u001b[39;49mforward(inputs_embeds\u001b[39m=\u001b[39;49minput_embeds_arr, attention_mask\u001b[39m=\u001b[39;49mattn_mask_arr, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/models/t5/modeling_t5.py:1696\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m decoder_input_ids\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1695\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1696\u001b[0m     attention_mask \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1697\u001b[0m \u001b[39mif\u001b[39;00m decoder_attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1698\u001b[0m     decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/custom_huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/models/t5/modeling_t5.py:1078\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1064\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1065\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m     )\n\u001b[1;32m   1076\u001b[0m     \u001b[39m#print(\"poggers 3\")\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1078\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1079\u001b[0m         hidden_states,\n\u001b[1;32m   1080\u001b[0m         attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1081\u001b[0m         position_bias\u001b[39m=\u001b[39mposition_bias,\n\u001b[1;32m   1082\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1083\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1084\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39mencoder_decoder_position_bias,\n\u001b[1;32m   1085\u001b[0m         layer_head_mask\u001b[39m=\u001b[39mlayer_head_mask,\n\u001b[1;32m   1086\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[1;32m   1087\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[1;32m   1088\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1089\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1090\u001b[0m     )\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# #print(\"layer outputs: \", layer_outputs)\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \n\u001b[1;32m   1094\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/custom_huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/models/t5/modeling_t5.py:701\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     query_length \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m cross_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m1\u001b[39;49m](\n\u001b[1;32m    702\u001b[0m     hidden_states,\n\u001b[1;32m    703\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    704\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    705\u001b[0m     position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m    706\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m    707\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mcross_attn_past_key_value,\n\u001b[1;32m    708\u001b[0m     query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[1;32m    709\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    710\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    712\u001b[0m hidden_states \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    714\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/custom_huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/models/t5/modeling_t5.py:615\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    603\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    604\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    613\u001b[0m ):\n\u001b[1;32m    614\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 615\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEncDecAttention(\n\u001b[1;32m    616\u001b[0m         normed_hidden_states,\n\u001b[1;32m    617\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    618\u001b[0m         key_value_states\u001b[39m=\u001b[39;49mkey_value_states,\n\u001b[1;32m    619\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    620\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    621\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    622\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    623\u001b[0m         query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[1;32m    624\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[1;32m    626\u001b[0m     layer_output \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[1;32m    627\u001b[0m     outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/custom_huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/models/t5/modeling_t5.py:531\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    528\u001b[0m         position_bias \u001b[39m=\u001b[39m position_bias[:, :, \u001b[39m-\u001b[39mhidden_states\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) :, :]\n\u001b[1;32m    530\u001b[0m     \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 531\u001b[0m         position_bias \u001b[39m=\u001b[39m position_bias \u001b[39m+\u001b[39;49m mask  \u001b[39m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpruned_heads:\n\u001b[1;32m    534\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(position_bias\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (512) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "t5.train()\n",
    "\n",
    "\n",
    "# outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels)\n",
    "# outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "loss = outputs[0]\n",
    "loss.shape\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successful training iteration\n"
     ]
    }
   ],
   "source": [
    "''' backwards pass '''\n",
    "optimizer = torch.optim.Adam(params =  t5.parameters(), lr=1e-4)\n",
    "optimizer.zero_grad()\n",
    "loss.sum().backward()\n",
    "optimizer.step()\n",
    "print(\"✅ Successful training iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
    "# The model is put into train mode and then we enumerate over the training loader and passed to the defined network \n",
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%10 == 0:\n",
    "            wandb.log({\"Training Loss\": loss.item()})\n",
    "\n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # xm.optimizer_step(optimizer)  # FOR TPU\n",
    "        # xm.mark_step()                # FOR TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # WandB – Initialize a new run\n",
    "    wandb.init(project=\"transformers_tutorials_summarization\")\n",
    "\n",
    "    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
    "    # Defining some key variables that will be used later on in the training  \n",
    "    config = wandb.config          # Initialize config\n",
    "    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
    "    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
    "    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n",
    "    config.VAL_EPOCHS = 1 \n",
    "    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "    config.SEED = 42               # random seed (default: 42)\n",
    "    config.MAX_LEN = 512\n",
    "    config.SUMMARY_LEN = 150 \n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(config.SEED) # pytorch random seed\n",
    "    np.random.seed(config.SEED) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    \n",
    "\n",
    "    # Importing and Pre-Processing the domain data\n",
    "    # Selecting the needed columns only. \n",
    "    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
    "    df = pd.read_csv('./data/news_summary.csv',encoding='latin-1')\n",
    "    df = df[['text','ctext']]\n",
    "    df.ctext = 'summarize: ' + df.ctext\n",
    "    print(df.head())\n",
    "\n",
    "    \n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
    "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    print(\"FULL Dataset: {}\".format(df.shape))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        'batch_size': config.TRAIN_BATCH_SIZE,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    val_params = {\n",
    "        'batch_size': config.VALID_BATCH_SIZE,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "    \n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    # Log metrics with wandb\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    # Training loop\n",
    "    print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "    for epoch in range(config.TRAIN_EPOCHS):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "\n",
    "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "    # Saving the dataframe as predictions.csv\n",
    "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "    for epoch in range(config.VAL_EPOCHS):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv('./models/predictions.csv')\n",
    "        print('Output Files generated for review')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('custom_huggingface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08d608f700504c6b03e88c868d0cc9b143978899209b13a888b26c423352d24d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
