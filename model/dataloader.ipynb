{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate clip\n",
    "import clip\n",
    "\n",
    "MODEL_SIZE = 'ViT-L/14@336px'  # Best models are (1st) ViT-L/14@336px and (2nd) ViT-L/14. I don't recommend going lower.  \n",
    "clip_instance, clip_preprocess = clip.load(MODEL_SIZE, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config, AutoModelWithLMHead\n",
    "\n",
    "'''\n",
    "MODEL SELECTION\n",
    "\n",
    "T5 V1.1 --  https://huggingface.co/docs/transformers/model_doc/t5v1.1 && https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511\n",
    "small - base - large - 3b/xl - 11b/xxl\n",
    "\n",
    "OG: t5-small\n",
    "\n",
    "'google/t5-base-lm-adapt' # largest on my server (without float16)\n",
    "'google/t5-xl-lm-adapt'\n",
    "\n",
    "google/t5-v1_1-large\n",
    "'''\n",
    "\n",
    "# MODEL_SIZE = \"t5-base\"\n",
    "MODEL_NAME = \"google/t5-v1_1-base\"\n",
    "# MODEL_NAME = \"google/t5-base-lm-adapt\"\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=False).to(device) # float16, True\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) â€” Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental.\n",
    "optimizer = torch.optim.Adam(params =  t5.parameters(), lr=1e-4) # Typically, 1e-4 and 3e-4 work well for most problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"parallel_15\"\n",
    "REMOTE_WHISPER_FILE = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_whisper_output.jsonl'\n",
    "REMOTE_CLIP_DIR  = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_clip_output'\n",
    "REMOTE_SCENE_FILE = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_scene_output.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:43, 103.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\n",
      "tensor grad NllLossBackward0 cuda:0 4.869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [03:01, 88.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\n",
      "tensor grad NllLossBackward0 cuda:0 4.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [04:28, 88.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\n",
      "tensor grad NllLossBackward0 cuda:0 4.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [05:47, 84.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\n",
      "tensor grad NllLossBackward0 cuda:0 5.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [06:59, 80.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\n",
      "tensor grad NllLossBackward0 cuda:0 3.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [08:00, 73.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\n",
      "tensor grad NllLossBackward0 cuda:0 3.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [08:53, 88.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39m''' backwards pass '''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     loss\u001b[39m.\u001b[39;49msum()\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkastan/home/kastan/thesis/video-pretrained-transformer/model/dataloader.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss ðŸ‘‡ðŸ‘‡ðŸ‘‡\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Iterate through the batch\n",
    "clip_15 = os.listdir(REMOTE_CLIP_DIR)\n",
    "\n",
    "# Initialize embeddings\n",
    "one_input_shape = [1, 768, 768]\n",
    "att_mask_shape = [1, 768]\n",
    "embed_shape = [1, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "attn_mask_arr[0][2] = 1\n",
    "\n",
    "t5.train()\n",
    "\n",
    "with jsonlines.open(REMOTE_SCENE_FILE, 'r') as scene_reader:\n",
    "    # Zipping the scene graph with the clip + whisper embeddings\n",
    "    \n",
    "    # itr over videos\n",
    "    for scene_seg_list, clip_npz_path in tqdm.tqdm(zip(scene_reader, glob.glob(os.path.join(REMOTE_CLIP_DIR, '*'), recursive = True))):\n",
    "        try:\n",
    "            np_loaded = np.load(clip_npz_path, allow_pickle=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load compressed numpy: {e}\")\n",
    "            continue\n",
    "        object_list_of_str = []\n",
    "        scene_seg_list = json.loads(scene_seg_list)\n",
    "        \n",
    "        print(len(tqdm.tqdm))\n",
    "        \n",
    "        # iterate over segments\n",
    "        for segment_index in range(np_loaded['arr_0'].item()['total_segments']):\n",
    "            # print(np_loaded[f'arr_{segment_index}'].item()['captions'])\n",
    "            frame_embedding       = np_loaded[f'arr_{segment_index}'].item()['frame_embeddings']\n",
    "            caption_embedding     = np_loaded[f'arr_{segment_index}'].item()['text_caption_embeddings']\n",
    "            whisper_text_captions = np_loaded[f'arr_{segment_index}'].item()['captions']\n",
    "            \n",
    "            frame_embedding       = torch.from_numpy(frame_embedding.reshape((768,))).to(device)\n",
    "            caption_embedding     = torch.from_numpy(caption_embedding).to(device)\n",
    "\n",
    "            scene_caption = scene_seg_list[segment_index]\n",
    "            scene_caption = clip.tokenize(scene_caption).to(device)\n",
    "            with torch.inference_mode(): # even faster than no_grad()\n",
    "                scene_embedding = clip_instance.encode_text(scene_caption)\n",
    "            scene_embedding = scene_embedding.reshape((768,))\n",
    "\n",
    "            # Update embedding array\n",
    "            input_embeds_arr[0][0] = frame_embedding\n",
    "            input_embeds_arr[0][1] = caption_embedding\n",
    "            input_embeds_arr[0][2] = scene_embedding\n",
    "            # Set to torch\n",
    "            decoder_input_embeds_arr = np.random.rand( *one_input_shape )  # .astype(np.float16) # need fp32\n",
    "            decoder_input_embeds_arr = decoder_input_embeds_arr\n",
    "            input_embeds_arr = input_embeds_arr\n",
    "            attn_mask_arr = attn_mask_arr\n",
    "            \n",
    "            # print(\"Input shapes:\", scene_embedding, caption_embedding, frame_embedding)\n",
    "            labels = t5_tokenizer(whisper_text_captions, return_tensors=\"pt\").input_ids.to(device)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "            outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            ''' backwards pass '''\n",
    "            optimizer.zero_grad()\n",
    "            loss.sum().backward()\n",
    "            optimizer.step()\n",
    "        print(\"Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\")\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5.save_pretrained(\"BIG_PENIS_PREVAILS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Inference with custom model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/nlp_v2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config, AutoModelWithLMHead\n",
    "\n",
    "'''\n",
    "MODEL SELECTION\n",
    "\n",
    "T5 V1.1 --  https://huggingface.co/docs/transformers/model_doc/t5v1.1 && https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511\n",
    "small - base - large - 3b/xl - 11b/xxl\n",
    "\n",
    "OG: t5-small\n",
    "\n",
    "'google/t5-base-lm-adapt' # largest on my server (without float16)\n",
    "'google/t5-xl-lm-adapt'\n",
    "\n",
    "google/t5-v1_1-large\n",
    "'''\n",
    "\n",
    "# MODEL_SIZE = \"t5-base\"\n",
    "MODEL_NAME = \"google/t5-v1_1-base\"\n",
    "OUR_FINETUNED_MODEL = \"\"\n",
    "# MODEL_NAME = \"google/t5-base-lm-adapt\"\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=False).to(device) # float16, True\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) â€” Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the output ðŸ‘‡ðŸ‘‡ðŸ‘‡\n",
      ". Thanks for the video. Thanks for the video. Thanks for the video. I really\n"
     ]
    }
   ],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# training\n",
    "# input_ids = t5_tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "# labels = t5_tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "# outputs = t5(input_ids=input_ids, labels=labels)\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits\n",
    "\n",
    "# inference\n",
    "input_ids = t5_tokenizer(\n",
    "    \"Hey guys welcome back to the channel\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = t5.generate(input_ids)\n",
    "print(\"Here's the output ðŸ‘‡ðŸ‘‡ðŸ‘‡\")\n",
    "print(t5_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# studies have shown that owning a dog is good for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp_v2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "612b182cb4c3e0acfd877acc6c10f43d075b0ae43380d6b249d2d2b5490153b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
