{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the better code is in tfRecord_to_numpy.py\n",
    "to run this notebook: \n",
    "sudo docker run -it --rm -v /home/kastan/nlp/YT_8M/BEST_notebooks:/home/8M tensorflow/tensorflow:1.15.0 bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 02:30:30.257925: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Provides readers configured for different datasets.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "# import utils\n",
    "\n",
    "def Dequantize(feat_vector, max_quantized_value=2, min_quantized_value=-2):\n",
    "  \"\"\"Dequantize the feature from the byte format to the float format.\n",
    "  Args:\n",
    "    feat_vector: the input 1-d vector.\n",
    "    max_quantized_value: the maximum of the quantized value.\n",
    "    min_quantized_value: the minimum of the quantized value.\n",
    "  Returns:\n",
    "    A float vector which has the same shape as feat_vector.\n",
    "  \"\"\"\n",
    "  assert max_quantized_value > min_quantized_value\n",
    "  quantized_range = max_quantized_value - min_quantized_value\n",
    "  scalar = quantized_range / 255.0\n",
    "  bias = (quantized_range / 512.0) + min_quantized_value\n",
    "  return feat_vector * scalar + bias\n",
    "\n",
    "def resize_axis(tensor, axis, new_size, fill_value=0):\n",
    "  \"\"\"Truncates or pads a tensor to new_size on on a given axis.\n",
    "  Truncate or extend tensor such that tensor.shape[axis] == new_size. If the\n",
    "  size increases, the padding will be performed at the end, using fill_value.\n",
    "  Args:\n",
    "    tensor: The tensor to be resized.\n",
    "    axis: An integer representing the dimension to be sliced.\n",
    "    new_size: An integer or 0d tensor representing the new value for\n",
    "      tensor.shape[axis].\n",
    "    fill_value: Value to use to fill any new entries in the tensor. Will be cast\n",
    "      to the type of tensor.\n",
    "  Returns:\n",
    "    The resized tensor.\n",
    "  \"\"\"\n",
    "  tensor = tf.convert_to_tensor(tensor)\n",
    "  shape = tf.unstack(tf.shape(tensor))\n",
    "\n",
    "  pad_shape = shape[:]\n",
    "  pad_shape[axis] = tf.maximum(0, new_size - shape[axis])\n",
    "\n",
    "  shape[axis] = tf.minimum(shape[axis], new_size)\n",
    "  shape = tf.stack(shape)\n",
    "\n",
    "  resized = tf.concat([\n",
    "      tf.slice(tensor, tf.zeros_like(shape), shape),\n",
    "      tf.fill(tf.stack(pad_shape), tf.cast(fill_value, tensor.dtype))\n",
    "  ], axis)\n",
    "\n",
    "  # Update shape.\n",
    "  new_shape = tensor.get_shape().as_list()  # A copy is being made.\n",
    "  new_shape[axis] = new_size\n",
    "  resized.set_shape(new_shape)\n",
    "  return resized\n",
    "\n",
    "\n",
    "class BaseReader(object):\n",
    "  \"\"\"Inherit from this class when implementing new readers.\"\"\"\n",
    "\n",
    "  def prepare_reader(self, unused_filename_queue):\n",
    "    \"\"\"Create a thread for generating prediction and label tensors.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "class YT8MFrameFeatureReader(BaseReader):\n",
    "  \"\"\"Reads TFRecords of SequenceExamples.\n",
    "  The TFRecords must contain SequenceExamples with the sparse in64 'labels'\n",
    "  context feature and a fixed length byte-quantized feature vector, obtained\n",
    "  from the features in 'feature_names'. The quantized features will be mapped\n",
    "  back into a range between min_quantized_value and max_quantized_value.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(  # pylint: disable=dangerous-default-value\n",
    "      self,\n",
    "      num_classes=3862,\n",
    "      feature_sizes=[1024, 128],\n",
    "      feature_names=[\"rgb\", \"audio\"],\n",
    "      max_frames=300,\n",
    "      segment_labels=False,\n",
    "      segment_size=5):\n",
    "    \"\"\"Construct a YT8MFrameFeatureReader.\n",
    "    Args:\n",
    "      num_classes: a positive integer for the number of classes.\n",
    "      feature_sizes: positive integer(s) for the feature dimensions as a list.\n",
    "      feature_names: the feature name(s) in the tensorflow record as a list.\n",
    "      max_frames: the maximum number of frames to process.\n",
    "      segment_labels: if we read segment labels instead.\n",
    "      segment_size: the segment_size used for reading segments.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(feature_names) == len(feature_sizes), (\n",
    "        \"length of feature_names (={}) != length of feature_sizes (={})\".format(\n",
    "            len(feature_names), len(feature_sizes)))\n",
    "\n",
    "    self.num_classes = num_classes\n",
    "    self.feature_sizes = feature_sizes\n",
    "    self.feature_names = feature_names\n",
    "    self.max_frames = max_frames\n",
    "    self.segment_labels = segment_labels\n",
    "    self.segment_size = segment_size\n",
    "\n",
    "  def get_video_matrix(self, features, feature_size, max_frames,\n",
    "                       max_quantized_value, min_quantized_value):\n",
    "    \"\"\"Decodes features from an input string and quantizes it.\n",
    "    Args:\n",
    "      features: raw feature values\n",
    "      feature_size: length of each frame feature vector\n",
    "      max_frames: number of frames (rows) in the output feature_matrix\n",
    "      max_quantized_value: the maximum of the quantized value.\n",
    "      min_quantized_value: the minimum of the quantized value.\n",
    "    Returns:\n",
    "      feature_matrix: matrix of all frame-features\n",
    "      num_frames: number of frames in the sequence\n",
    "    \"\"\"\n",
    "    decoded_features = tf.reshape(\n",
    "        tf.cast(tf.decode_raw(features, tf.uint8), tf.float32),\n",
    "        [-1, feature_size])\n",
    "\n",
    "    num_frames = tf.minimum(tf.shape(decoded_features)[0], max_frames)\n",
    "    feature_matrix = Dequantize(decoded_features, max_quantized_value,\n",
    "                                      min_quantized_value)\n",
    "    feature_matrix = resize_axis(feature_matrix, 0, max_frames)\n",
    "    return feature_matrix, num_frames\n",
    "\n",
    "  def prepare_reader(self,\n",
    "                     filename_queue,\n",
    "                     max_quantized_value=2,\n",
    "                     min_quantized_value=-2):\n",
    "    \"\"\"Creates a single reader thread for YouTube8M SequenceExamples.\n",
    "    Args:\n",
    "      filename_queue: A tensorflow queue of filename locations.\n",
    "      max_quantized_value: the maximum of the quantized value.\n",
    "      min_quantized_value: the minimum of the quantized value.\n",
    "    Returns:\n",
    "      A dict of video indexes, video features, labels, and frame counts.\n",
    "    \"\"\"\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    return self.prepare_serialized_examples(serialized_example,\n",
    "                                            max_quantized_value,\n",
    "                                            min_quantized_value)\n",
    "\n",
    "  def prepare_serialized_examples(self,\n",
    "                                  serialized_example,\n",
    "                                  max_quantized_value=2,\n",
    "                                  min_quantized_value=-2):\n",
    "    \"\"\"Parse single serialized SequenceExample from the TFRecords.\"\"\"\n",
    "\n",
    "    # Read/parse frame/segment-level labels.\n",
    "    context_features = {\n",
    "        \"id\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    if self.segment_labels:\n",
    "      context_features.update({\n",
    "          # There is no need to read end-time given we always assume the segment\n",
    "          # has the same size.\n",
    "          \"segment_labels\": tf.io.VarLenFeature(tf.int64),\n",
    "          \"segment_start_times\": tf.io.VarLenFeature(tf.int64),\n",
    "          \"segment_scores\": tf.io.VarLenFeature(tf.float32)\n",
    "      })\n",
    "    else:\n",
    "      context_features.update({\"labels\": tf.io.VarLenFeature(tf.int64)})\n",
    "    sequence_features = {\n",
    "        feature_name: tf.io.FixedLenSequenceFeature([], dtype=tf.string)\n",
    "        for feature_name in self.feature_names\n",
    "    }\n",
    "    contexts, features = tf.io.parse_single_sequence_example(\n",
    "        serialized_example,\n",
    "        context_features=context_features,\n",
    "        sequence_features=sequence_features)\n",
    "\n",
    "    # loads (potentially) different types of features and concatenates them\n",
    "    num_features = len(self.feature_names)\n",
    "    assert num_features > 0, \"No feature selected: feature_names is empty!\"\n",
    "\n",
    "    assert len(self.feature_names) == len(self.feature_sizes), (\n",
    "        \"length of feature_names (={}) != length of feature_sizes (={})\".format(\n",
    "            len(self.feature_names), len(self.feature_sizes)))\n",
    "\n",
    "    num_frames = -1  # the number of frames in the video\n",
    "    feature_matrices = [None] * num_features  # an array of different features\n",
    "    for feature_index in range(num_features):\n",
    "      feature_matrix, num_frames_in_this_feature = self.get_video_matrix(\n",
    "          features[self.feature_names[feature_index]],\n",
    "          self.feature_sizes[feature_index], self.max_frames,\n",
    "          max_quantized_value, min_quantized_value)\n",
    "      if num_frames == -1:\n",
    "        num_frames = num_frames_in_this_feature\n",
    "\n",
    "      feature_matrices[feature_index] = feature_matrix\n",
    "\n",
    "    # cap the number of frames at self.max_frames\n",
    "    num_frames = tf.minimum(num_frames, self.max_frames)\n",
    "\n",
    "    # concatenate different features\n",
    "    video_matrix = tf.concat(feature_matrices, 1)\n",
    "\n",
    "    # Partition frame-level feature matrix to segment-level feature matrix.\n",
    "    if self.segment_labels:\n",
    "      start_times = contexts[\"segment_start_times\"].values\n",
    "      # Here we assume all the segments that started at the same start time has\n",
    "      # the same segment_size.\n",
    "      uniq_start_times, seg_idxs = tf.unique(start_times,\n",
    "                                             out_idx=tf.dtypes.int64)\n",
    "      # TODO(zhengxu): Ensure the segment_sizes are all same.\n",
    "      segment_size = self.segment_size\n",
    "      # Range gather matrix, e.g., [[0,1,2],[1,2,3]] for segment_size == 3.\n",
    "      range_mtx = tf.expand_dims(uniq_start_times, axis=-1) + tf.expand_dims(\n",
    "          tf.range(0, segment_size, dtype=tf.int64), axis=0)\n",
    "      # Shape: [num_segment, segment_size, feature_dim].\n",
    "      batch_video_matrix = tf.gather_nd(video_matrix,\n",
    "                                        tf.expand_dims(range_mtx, axis=-1))\n",
    "      num_segment = tf.shape(batch_video_matrix)[0]\n",
    "      batch_video_ids = tf.reshape(tf.tile([contexts[\"id\"]], [num_segment]),\n",
    "                                   (num_segment,))\n",
    "      batch_frames = tf.reshape(tf.tile([segment_size], [num_segment]),\n",
    "                                (num_segment,))\n",
    "\n",
    "      # For segment labels, all labels are not exhausively rated. So we only\n",
    "      # evaluate the rated labels.\n",
    "\n",
    "      # Label indices for each segment, shape: [num_segment, 2].\n",
    "      label_indices = tf.stack([seg_idxs, contexts[\"segment_labels\"].values],\n",
    "                               axis=-1)\n",
    "      label_values = contexts[\"segment_scores\"].values\n",
    "      sparse_labels = tf.sparse.SparseTensor(label_indices, label_values,\n",
    "                                             (num_segment, self.num_classes))\n",
    "      batch_labels = tf.sparse.to_dense(sparse_labels, validate_indices=False)\n",
    "\n",
    "      sparse_label_weights = tf.sparse.SparseTensor(\n",
    "          label_indices, tf.ones_like(label_values, dtype=tf.float32),\n",
    "          (num_segment, self.num_classes))\n",
    "      batch_label_weights = tf.sparse.to_dense(sparse_label_weights,\n",
    "                                               validate_indices=False)\n",
    "    else:\n",
    "      # Process video-level labels.\n",
    "      label_indices = contexts[\"labels\"].values\n",
    "      sparse_labels = tf.sparse.SparseTensor(\n",
    "          tf.expand_dims(label_indices, axis=-1),\n",
    "          tf.ones_like(contexts[\"labels\"].values, dtype=tf.bool),\n",
    "          (self.num_classes,))\n",
    "      labels = tf.sparse.to_dense(sparse_labels,\n",
    "                                  default_value=False,\n",
    "                                  validate_indices=False)\n",
    "      # convert to batch format.\n",
    "      batch_video_ids = tf.expand_dims(contexts[\"id\"], 0)\n",
    "      batch_video_matrix = tf.expand_dims(video_matrix, 0)\n",
    "      batch_labels = tf.expand_dims(labels, 0)\n",
    "      batch_frames = tf.expand_dims(num_frames, 0)\n",
    "      batch_label_weights = None\n",
    "\n",
    "    output_dict = {\n",
    "        \"video_ids\": batch_video_ids,\n",
    "        \"video_matrix\": batch_video_matrix,\n",
    "        \"labels\": batch_labels,\n",
    "        \"num_frames\": batch_frames,\n",
    "    }\n",
    "    if batch_label_weights is not None:\n",
    "      output_dict[\"label_weights\"] = batch_label_weights\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v1.data' has no attribute 'string_input_producer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/nlp/YT_8M/BEST_notebooks/v2_process_frame_level.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bv72ubuntu/home/kastan/nlp/YT_8M/BEST_notebooks/v2_process_frame_level.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m filenames \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mglob(\u001b[39m'\u001b[39m\u001b[39m./YT_8M_data/*.tfrecord\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bv72ubuntu/home/kastan/nlp/YT_8M/BEST_notebooks/v2_process_frame_level.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# print(filenames)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bv72ubuntu/home/kastan/nlp/YT_8M/BEST_notebooks/v2_process_frame_level.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m filequeue \u001b[39m=\u001b[39m tf1\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mstring_input_producer(filenames)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bv72ubuntu/home/kastan/nlp/YT_8M/BEST_notebooks/v2_process_frame_level.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m my_class \u001b[39m=\u001b[39m YT8MFrameFeatureReader()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bv72ubuntu/home/kastan/nlp/YT_8M/BEST_notebooks/v2_process_frame_level.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m my_class\u001b[39m.\u001b[39mprepare_reader()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.compat.v1.data' has no attribute 'string_input_producer'"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf1\n",
    "filenames = tf.io.gfile.glob('./YT_8M_data/*.tfrecord')\n",
    "# print(filenames)\n",
    "filequeue = tf1.data.string_input_producer(filenames)\n",
    "\n",
    "my_class = YT8MFrameFeatureReader()\n",
    "my_class.prepare_reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cpu_nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e37f0a8afa730d972bb2bedc65a55e2e751f510b12eab90711d3d898844e1392"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
