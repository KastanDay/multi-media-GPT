{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoModelWithLMHead, T5Config, T5ForConditionalGeneration, T5Model, T5Tokenizer)\n",
    "# 783,150,080. ~800M params.\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(\"google/t5-v1_1-large\",\n",
    "                                                torch_dtype=torch.float32,\n",
    "                                                low_cpu_mem_usage=False).to(device)  # float16, True\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-large\", return_special_tokens_mask=True)\n",
    "learning_rate = 1e-4  # also good: 3e-4\n",
    "# low_cpu_mem_usage(bool, optional) — Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental.\n",
    "optimizer = torch.optim.AdamW(params=t5.parameters(),\n",
    "                              lr=learning_rate)  # Typically, 1e-4 and 3e-4 work well for most problems\n",
    "pytorch_total_params = sum(p.numel() for p in t5.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/mambaforge/envs/nlp_v2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"num_decoder_layers\": 6,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"transformers_version\": \"4.25.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": \"google/t5-v1_1-xl\"\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (AutoModelWithLMHead, T5Config, T5ForConditionalGeneration, T5Model, T5Tokenizer)\n",
    "T5Config(\"google/t5-v1_1-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"num_decoder_layers\": 6,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"transformers_version\": \"4.25.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": \"google/t5-v1_1-xxl\"\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T5Config(\"google/t5-v1_1-xxl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "# DEBUGGING \n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"parallel_15\"\n",
    "REMOTE_WHISPER_FILE = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_whisper_output.jsonl'\n",
    "REMOTE_CLIP_DIR  = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_clip_output'\n",
    "REMOTE_SCENE_FILE = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_scene_output.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate clip\n",
    "import clip\n",
    "\n",
    "MODEL_SIZE = 'ViT-L/14@336px'  # Best models are (1st) ViT-L/14@336px and (2nd) ViT-L/14. I don't recommend going lower.  \n",
    "clip_instance, clip_preprocess = clip.load(MODEL_SIZE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, OPTForCausalLM\n",
    "t5 = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "t5_tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "learning_rate       = 1e-4  # also good: 3e-4\n",
    "optimizer = torch.optim.AdamW(params =  t5.parameters(), lr=learning_rate) # Typically, 1e-4 and 3e-4 work well for most problems\n",
    "\n",
    "# prompt = \"Hey, are you consciours? Can you talk to me?\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate\n",
    "# generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "# tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForCausalLM, RobertaTokenizer\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "t5 = RobertaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=False).to(device) # float16, True\n",
    "t5_tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config, AutoModelWithLMHead\n",
    "\n",
    "'''\n",
    "MODEL SELECTION\n",
    "\n",
    "T5 V1.1 --  https://huggingface.co/docs/transformers/model_doc/t5v1.1 && https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511\n",
    "small - base - large - 3b/xl - 11b/xxl\n",
    "\n",
    "OG: t5-small\n",
    "\n",
    "'google/t5-base-lm-adapt' # largest on my server (without float16)\n",
    "'google/t5-xl-lm-adapt'\n",
    "\n",
    "google/t5-v1_1-large\n",
    "'''\n",
    "\n",
    "# MODEL_SIZE = \"t5-base\"\n",
    "MODEL_NAME = \"google/t5-v1_1-base\"\n",
    "# MODEL_NAME = \"google/t5-base-lm-adapt\"\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=False).to(device) # float16, True\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) — Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental.\n",
    "learning_rate       = 1e-4  # also good: 3e-4\n",
    "optimizer = torch.optim.AdamW(params =  t5.parameters(), lr=learning_rate) # Typically, 1e-4 and 3e-4 work well for most problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t5.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cockpit ML Debugger\n",
    "# https://cockpit.readthedocs.io/en/latest/examples/01_basic_fmnist.html\n",
    "from backpack import extend\n",
    "from cockpit import Cockpit, CockpitPlotter\n",
    "from cockpit.utils.configuration import configuration\n",
    "\n",
    "t5 = extend(t5)\n",
    "\n",
    "individual_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Create Cockpit and a plotter\n",
    "cockpit = Cockpit(t5.parameters(), quantities=configuration(\"full\"))\n",
    "plotter = CockpitPlotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' just for trying the debugger cockpit'''\n",
    "\n",
    "\n",
    "# Iterate through the batch\n",
    "clip_15 = os.listdir(REMOTE_CLIP_DIR)\n",
    "\n",
    "# Initialize embeddings\n",
    "one_input_shape = [1, 768, 768]\n",
    "att_mask_shape = [1, 768]\n",
    "embed_shape = [1, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "attn_mask_arr[0][2] = 1 # no clip\n",
    "\n",
    "t5.train()\n",
    "\n",
    "global_step = 0 \n",
    "\n",
    "with jsonlines.open(REMOTE_SCENE_FILE, 'r') as scene_reader:\n",
    "    # Zipping the scene graph with the clip + whisper embeddings\n",
    "    \n",
    "    # itr over videos\n",
    "    for scene_seg_list, clip_npz_path in tqdm.tqdm(zip(scene_reader, glob.glob(os.path.join(REMOTE_CLIP_DIR, '*'), recursive = True))):\n",
    "        try:\n",
    "            np_loaded = np.load(clip_npz_path, allow_pickle=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load compressed numpy: {e}\")\n",
    "            continue\n",
    "        object_list_of_str = []\n",
    "        scene_seg_list = json.loads(scene_seg_list)\n",
    "        \n",
    "        # iterate over segments\n",
    "        for segment_index in range(np_loaded['arr_0'].item()['total_segments']):\n",
    "            # print(np_loaded[f'arr_{segment_index}'].item()['captions'])\n",
    "            frame_embedding       = np_loaded[f'arr_{segment_index}'].item()['frame_embeddings']\n",
    "            caption_embedding     = np_loaded[f'arr_{segment_index}'].item()['text_caption_embeddings']\n",
    "            whisper_text_captions = np_loaded[f'arr_{segment_index}'].item()['captions']\n",
    "            \n",
    "            frame_embedding       = torch.from_numpy(frame_embedding.reshape((768,))).to(device)\n",
    "            caption_embedding     = torch.from_numpy(caption_embedding).to(device)\n",
    "\n",
    "            # Update embedding array\n",
    "            input_embeds_arr[0][0] = frame_embedding\n",
    "            input_embeds_arr[0][1] = caption_embedding\n",
    "            \n",
    "            # print(\"Input shapes:\")\n",
    "            # print(caption_embedding)\n",
    "            print(frame_embedding)\n",
    "            labels = t5_tokenizer(whisper_text_captions, return_tensors=\"pt\").input_ids.to(device)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "            outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            print(\"loss\")\n",
    "            print(loss)\n",
    "            logits = outputs[1]\n",
    "            # print(\"logits\")\n",
    "            # print(logits)\n",
    "            individual_losses = individual_loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            # print(\"individual_losses\")\n",
    "            # print(individual_losses)\n",
    "            # loss = \n",
    "            for item in outputs:\n",
    "                print(item)\n",
    "            ''' backwards pass '''\n",
    "            # loss.sum().backward()\n",
    "            # backward pass\n",
    "            with cockpit(\n",
    "                global_step,\n",
    "                info={\n",
    "                    \"batch_size\": one_input_shape[0],\n",
    "                    \"individual_losses\": individual_losses,\n",
    "                    \"loss\": loss,\n",
    "                    \"optimizer\": optimizer,\n",
    "                },\n",
    "            ):\n",
    "                loss.backward(create_graph=cockpit.create_graph(global_step))\n",
    "            \n",
    "            # optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            print(f\"step: {global_step}\")\n",
    "            plotter.plot(cockpit)\n",
    "            \n",
    "            print(\"Loss 👇👇👇\")\n",
    "            print(loss)\n",
    "            break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaConfig\n",
    "# OPTConfig.from_pretrained(\"roberta-base\")\n",
    "from transformers import OPTConfig\n",
    "OPTConfig.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' More complete '''\n",
    "\n",
    "# Initialize embeddings\n",
    "# one_input_shape = [1, 768, 768]\n",
    "# att_mask_shape = [1, 768]\n",
    "# embed_shape = [1, 768]\n",
    "one_input_shape = [1, 2048, 768]  # torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)\n",
    "att_mask_shape = [1, 2048]        # \n",
    "embed_shape = [1, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape, device=device)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape, device=device)\n",
    "labels    = torch.zeros(att_mask_shape, dtype=int, device=device)\n",
    "print(labels.dtype)\n",
    "labels[:len(labels)] = -100\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "# attn_mask_arr[0][2] = 1 # no clip\n",
    "\n",
    "t5.train()\n",
    "\n",
    "global_step = 0 \n",
    "\n",
    "with jsonlines.open(REMOTE_SCENE_FILE, 'r') as scene_reader:\n",
    "    # Zipping the scene graph with the clip + whisper embeddings\n",
    "    # itr over videos\n",
    "    for scene_seg_list, clip_npz_path in tqdm.tqdm(zip(scene_reader, glob.glob(os.path.join(REMOTE_CLIP_DIR, '*'), recursive = True))):\n",
    "        try:\n",
    "            np_loaded = np.load(clip_npz_path, allow_pickle=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load compressed numpy: {e}\")\n",
    "            continue\n",
    "        object_list_of_str = []\n",
    "        scene_seg_list = json.loads(scene_seg_list)\n",
    "        \n",
    "        # iterate over segments\n",
    "        for segment_index in range(np_loaded['arr_0'].item()['total_segments']):\n",
    "            frame_embedding       = np_loaded[f'arr_{segment_index}'].item()['frame_embeddings']\n",
    "            caption_embedding     = np_loaded[f'arr_{segment_index}'].item()['text_caption_embeddings']\n",
    "            whisper_text_captions = np_loaded[f'arr_{segment_index}'].item()['captions']\n",
    "            \n",
    "            frame_embedding       = torch.from_numpy(frame_embedding.reshape((768,))).to(device)\n",
    "            caption_embedding     = torch.from_numpy(caption_embedding).to(device)\n",
    "\n",
    "            scene_caption = scene_seg_list[segment_index]\n",
    "            scene_caption = clip.tokenize(scene_caption).to(device)\n",
    "            with torch.inference_mode(): # even faster than no_grad()\n",
    "                scene_embedding = clip_instance.encode_text(scene_caption)\n",
    "            scene_embedding = scene_embedding.reshape((768,))\n",
    "\n",
    "            # Update embedding array\n",
    "            input_embeds_arr[0][0] = frame_embedding\n",
    "            input_embeds_arr[0][1] = caption_embedding\n",
    "            input_embeds_arr[0][2] = scene_embedding\n",
    "            \n",
    "            print(\"Input shapes:\")\n",
    "            print(input_embeds_arr.shape)\n",
    "            print(scene_embedding.shape)\n",
    "            print(caption_embedding.shape)\n",
    "            print(frame_embedding.shape)\n",
    "            tokenized_labels = t5_tokenizer(whisper_text_captions, return_tensors=\"pt\").input_ids.to(device)\n",
    "            print(\"tokenized_labels\")\n",
    "            print(tokenized_labels)\n",
    "            print(\"labels\")\n",
    "            print(labels)\n",
    "            print(labels.dtype)\n",
    "            labels[0][0:len(tokenized_labels[0])] = tokenized_labels[0]\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "            outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True, return_last_hidden_state=True)\n",
    "            print(outputs['last_hidden_state'].shape)\n",
    "            \n",
    "            results = t5.lm_head(outputs['last_hidden_state'])\n",
    "            print(results)\n",
    "            # loss = outputs[0]\n",
    "            # ''' backwards pass '''\n",
    "            # loss.sum().backward()\n",
    "            # # optimizer step\n",
    "            # optimizer.zero_grad()\n",
    "            # optimizer.step()\n",
    "            # global_step += 1\n",
    "            print(f\"step: {global_step}\")\n",
    "            \n",
    "            print(\"Loss 👇👇👇\")\n",
    "            print(loss)\n",
    "            break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' More complete -- NO CLIP (for convienence) '''\n",
    "\n",
    "# Initialize embeddings\n",
    "one_input_shape = [1, 768, 768]\n",
    "att_mask_shape = [1, 768]\n",
    "embed_shape = [1, 768]\n",
    "# one_input_shape = [1, 2048, 768]  # torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)\n",
    "# att_mask_shape = [1, 2048]        # \n",
    "# embed_shape = [1, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape, device=device)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape, device=device)\n",
    "labels    = torch.zeros(att_mask_shape, dtype=int, device=device)\n",
    "print(labels.dtype)\n",
    "labels[:len(labels)] = -100\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "# attn_mask_arr[0][2] = 1 # no clip\n",
    "\n",
    "t5.train()\n",
    "\n",
    "global_step = 0 \n",
    "\n",
    "with jsonlines.open(REMOTE_SCENE_FILE, 'r') as scene_reader:\n",
    "    # Zipping the scene graph with the clip + whisper embeddings\n",
    "    # itr over videos\n",
    "    for scene_seg_list, clip_npz_path in tqdm.tqdm(zip(scene_reader, glob.glob(os.path.join(REMOTE_CLIP_DIR, '*'), recursive = True))):\n",
    "        try:\n",
    "            np_loaded = np.load(clip_npz_path, allow_pickle=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load compressed numpy: {e}\")\n",
    "            continue\n",
    "        object_list_of_str = []\n",
    "        scene_seg_list = json.loads(scene_seg_list)\n",
    "        \n",
    "        # iterate over segments\n",
    "        for segment_index in range(np_loaded['arr_0'].item()['total_segments']):\n",
    "            frame_embedding       = np_loaded[f'arr_{segment_index}'].item()['frame_embeddings']\n",
    "            caption_embedding     = np_loaded[f'arr_{segment_index}'].item()['text_caption_embeddings']\n",
    "            whisper_text_captions = np_loaded[f'arr_{segment_index}'].item()['captions']\n",
    "            \n",
    "            frame_embedding       = torch.from_numpy(frame_embedding.reshape((768,))).to(device)\n",
    "            caption_embedding     = torch.from_numpy(caption_embedding).to(device)\n",
    "\n",
    "            scene_caption = scene_seg_list[segment_index]\n",
    "            # scene_caption = clip.tokenize(scene_caption).to(device)\n",
    "            # with torch.inference_mode(): # even faster than no_grad()\n",
    "                # scene_embedding = clip_instance.encode_text(scene_caption)\n",
    "            # scene_embedding = scene_embedding.reshape((768,))\n",
    "\n",
    "            # Update embedding array\n",
    "            input_embeds_arr[0][0] = frame_embedding\n",
    "            input_embeds_arr[0][1] = caption_embedding\n",
    "            # input_embeds_arr[0][2] = scene_embedding\n",
    "            \n",
    "            print(\"Input shapes:\")\n",
    "            print(input_embeds_arr.shape)\n",
    "            # print(scene_embedding.shape)\n",
    "            print(caption_embedding.shape)\n",
    "            print(frame_embedding.shape)\n",
    "            tokenized_labels = t5_tokenizer(whisper_text_captions, return_tensors=\"pt\").input_ids.to(device)\n",
    "            print(\"tokenized_labels\")\n",
    "            print(tokenized_labels)\n",
    "            print(\"labels\")\n",
    "            print(labels)\n",
    "            print(labels.dtype)\n",
    "            labels[0][0:len(tokenized_labels[0])] = tokenized_labels[0]\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "            \n",
    "            decoder_input_ids = t5.forward()\n",
    "            output_ids = t5.generate(attention_mask=attn_mask_arr, decoder_input_ids=decoder_input_ids, inputs_embeds=input_embeds_arr, max_length=100, num_beams=4)\n",
    "\n",
    "            outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True, output_hidden_states=True)\n",
    "            print(outputs['last_hidden_state'].shape)\n",
    "            \n",
    "            results = t5.lm_head(outputs['last_hidden_state'])\n",
    "            print(results)\n",
    "            # loss = outputs[0]\n",
    "            # ''' backwards pass '''\n",
    "            # loss.sum().backward()\n",
    "            # # optimizer step\n",
    "            # optimizer.zero_grad()\n",
    "            # optimizer.step()\n",
    "            # global_step += 1\n",
    "            print(f\"step: {global_step}\")\n",
    "            \n",
    "            print(\"Loss 👇👇👇\")\n",
    "            print(loss)\n",
    "            break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = t5.lm_head(outputs['encoder_last_hidden_state'][0][0], shared_embedding=t5.shared)\n",
    "res.plt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5.get_output_layer_with_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['encoder_last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = outputs['encoder_last_hidden_state']\n",
    "output_ids = t5.generate(attention_mask=attn_mask_arr, inputs_embeds=input_embeds_arr, max_length=768, num_beams=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t5_tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['encoder_last_hidden_state']\n",
    "\n",
    "for i in range(40):\n",
    "  print(i, \"  \",outputs['encoder_last_hidden_state'][0][i])\n",
    "  # display(outputs['encoder_last_hidden_state'][0][i].plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5.save_pretrained(\"BIG_PENIS_PREVAILS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0][0:27]\n",
    "# len(tokenized_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Inference with custom model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FINE-TUNED T5\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import OPTForCausalLM\n",
    "\n",
    "# MODEL_VERSION_NAME   = \"opt_yt_pretrain_adamW_1e-3_iter69000\"\n",
    "# TOKENIZER_NAME       = \"facebook/opt-125m\"\n",
    "\n",
    "# BASE_DIR            = '/scratch/bbki/kastanday/whisper'\n",
    "# BASE_DIR            = '/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train'\n",
    "# MODEL_SAVE_PATH     = f'{BASE_DIR}/MODEL_CHECKPOINTS/{MODEL_VERSION_NAME}'\n",
    "\n",
    "TOKENIZER_NAME       = \"google/t5-v1_1-base\"\n",
    "t5     = AutoModelWithLMHead.from_pretrained(\"/mnt/storage_hdd/thesis/MODEL_CHECKPOINTS/T5_labels_are_half_batch_15_adamW_iter622500\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate clip\n",
    "import clip\n",
    "\n",
    "MODEL_SIZE = 'ViT-L/14@336px'  # Best models are (1st) ViT-L/14@336px and (2nd) ViT-L/14. I don't recommend going lower.  \n",
    "clip_instance, clip_preprocess = clip.load(MODEL_SIZE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get standard image and embed with CLIP.\n",
    "import deeplake as dl\n",
    "clip_dataset_path = f'/mnt/storage_ssd/dummy_clip_results_parallel_15'\n",
    "ds = dl.load(clip_dataset_path)\n",
    "print(ds.summary())\n",
    "\n",
    "frame_embedding = ds.pooled_clip_embedding[0].numpy()\n",
    "frame_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT = \"Hey guys welcome back to the channel, \"\n",
    "# PROMPT = \"I REALLY LIKE EATING ASS\"\n",
    "# PROMPT = \"TODAY WE ARE GOING TO\"\n",
    "PROMPT = \"IN THIS VIDEO WE WILL\"\n",
    "# PROMPT = \"THE MOST COMMON WORD IN THE ENGLISH LANGUAGE\"\n",
    "# PROMPT = \"IN THIS VIDEO WE WILL\"\n",
    "# PROMPT = \"I LOVE VIDEO GAMES BECAUSE I LOVE KILLING\"\n",
    "PROMPT = \"The secret to cooking a good burrito is\"\n",
    "\n",
    "\n",
    "# OLD/naive inference\n",
    "# input_ids = tokenizer(PROMPT, return_tensors=\"pt\").input_ids\n",
    "# outputs = model.generate(input_ids)\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "\n",
    "# Initialize embeddings\n",
    "one_input_shape = [1, 768, 768]\n",
    "att_mask_shape = [1, 768]\n",
    "input_embeds_arr = torch.zeros(one_input_shape, device=device)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape, device=device)\n",
    "# labels    = torch.zeros(att_mask_shape, dtype=int, device=device)\n",
    "# labels[:] = -100\n",
    "\n",
    "# device = 'cpu'\n",
    "caption_tokenized = clip.tokenize(PROMPT).to(device)\n",
    "with torch.inference_mode():\n",
    "    caption_embedding = clip_instance.encode_text(caption_tokenized)\n",
    "caption_embedding = caption_embedding.reshape((768,))\n",
    "\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "attn_mask_arr[0][2] = 1\n",
    "input_embeds_arr[0][0] = torch.tensor(frame_embedding)\n",
    "input_embeds_arr[0][1] = caption_embedding\n",
    "input_embeds_arr[0][2] = caption_embedding\n",
    "\n",
    "# TODO: generate attn_mask_arr, input_embeds_arr\n",
    "\n",
    "output_ids = t5.generate(attention_mask=attn_mask_arr, inputs_embeds=input_embeds_arr, max_length=768, num_beams=10)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying GPT-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2Config.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_caption = '1123 2 3 4 5 6 7'.split(\" \")\n",
    "import itertools\n",
    "first_half_of_whisper_caption  = list(itertools.islice(scene_caption, 0, len(scene_caption)//2))\n",
    "second_half_of_whisper_caption = list(itertools.islice(scene_caption, len(scene_caption)//2, None))\n",
    "\" \".join(first_half_of_whisper_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_half_of_whisper_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(scene_caption)//2\n",
    "parts = [scene_caption[i:i+n] for i in range(0, len(scene_caption), n)]\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e886ec343e9bf984999ac0f41f2df9adf20d1645c65a0a2833dfda78d79ad6ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
