{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Google 8M\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 01:03:40.796220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob all .tfrecord files in ./YT_8M_data directory\n",
    "filenames = tf.io.gfile.glob('./YT_8M_data/*.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def fake_to_real_ytid(ytid):\n",
    "  assert len(ytid) == 4\n",
    "  x = f\"http://data.yt8m.org/2/j/i/{ytid[0:2]}/{ytid}.js\"\n",
    "  r = requests.get(x)\n",
    "  if r.status_code != 200:\n",
    "    raise Exception(\"Video was probably removed:\", r.text)\n",
    "    \n",
    "  return r.text[10:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_captions(captions, start):\n",
    "  end = start + 5\n",
    "  relevant_captions = []\n",
    "  for caption in captions:\n",
    "    caption_start = caption['start']\n",
    "    caption_end = caption_start + caption['duration']\n",
    "    # Case 1: started before\n",
    "    if (end > caption_start and end < caption_end) or (start > caption_start and start < caption_end):\n",
    "      relevant_captions.append(caption['text'])\n",
    "  return relevant_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save python dict to json file named 'captions.json'\n",
    "import json\n",
    "def save_captions(captions):\n",
    "  with open('./8M_all_captions.json', 'w') as f:\n",
    "    json.dump(all_captions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo use ray for parallel downloading\n",
    "\n",
    "TEST_FILE_LIMIT = 2000\n",
    "\n",
    "# DATA FORMAT: captions_dict[video_id] = [caption1, caption2, ...]\n",
    "all_captions = {}\n",
    "num_no_caption = 0\n",
    "\n",
    "for itr, file in enumerate(filenames[:TEST_FILE_LIMIT]):\n",
    "  raw_dataset = tf.data.TFRecordDataset(file)\n",
    "  # for raw_record in raw_dataset:\n",
    "  for raw_record in raw_dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    start_times = example.features.feature['segment_start_times'].int64_list.value\n",
    "    fake_id = example.features.feature['id'].bytes_list.value[0].decode(\"utf-8\")\n",
    "\n",
    "    try:\n",
    "      yt_id = fake_to_real_ytid(fake_id)\n",
    "    except Exception as e:\n",
    "      # probably 403 error, forbidden (removed video)\n",
    "      num_no_caption += 1\n",
    "      continue\n",
    "\n",
    "    try:\n",
    "      url = YouTubeTranscriptApi.get_transcript(yt_id)\n",
    "    except Exception as e:\n",
    "      num_no_caption += 1\n",
    "      # print(\"No captions for\", yt_id)#, e) # probably no captions\n",
    "      continue\n",
    "\n",
    "    caption_list = []\n",
    "    # daniel put your code here\n",
    "    for index, start_time in enumerate(start_times):\n",
    "      captions = get_captions(url, start_time)\n",
    "      concat_captions = \" \".join(captions)\n",
    "      caption_list.append(concat_captions)\n",
    "      # todo: Add this caption_dict to the current dataframe \n",
    "    # print(caption_dict)\n",
    "    all_captions[yt_id] = caption_list\n",
    "    if itr % 10 == 0: print(f\"Iteration {itr}: running missing_caption ratio (higher is bad): \", num_no_caption / itr)\n",
    "    break\n",
    "\n",
    "print(\"No caption ratio: \", num_no_caption / TEST_FILE_LIMIT)\n",
    "print(all_captions)\n",
    "\n",
    "save_captions_to_json(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_captions_to_json(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save python dict to json file named 'captions.json'\n",
    "import json\n",
    "def save_captions_to_json(captions):\n",
    "  with open('8M_all_captions.json', 'w') as f:\n",
    "    json.dump(all_captions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_all_captions = all_captions\n",
    "saved_all_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.TFRecordDataset(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "raw_dataset = tf.data.TFRecordDataset(\"./YT_8M_data/validate0002.tfrecord\")\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    print(example)\n",
    "    # print(example.features.feature.keys())\n",
    "    # print(example.features.feature['labels'].int64_list.value)\n",
    "\n",
    "id = example.features.feature['id'].bytes_list.value[0].decode(\"utf-8\")\n",
    "# labels = example.features.feature['labels'].int64_list.value\n",
    "# segment_labels = example.features.feature['segment_labels'].int64_list.value\n",
    "# segment_scores = example.features.feature['segment_scores'].float_list.value\n",
    "# segment_start_times = example.features.feature['segment_start_times'].float_list.value\n",
    "# segment_end_times = example.features.feature['segment_end_times'].float_list.value\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting real YT vid IDs\n",
    "ID conversion: https://research.google.com/youtube8m/video_id_conversion.html\n",
    "\n",
    "construct a URI like /AB/ABCD.js (note: first 2 characters are repeated!), and append it to the URL data.yt8m.org/2/j/i.\n",
    "\n",
    "\n",
    "data.yt8m.org/2/j/i/nX/nXSc.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_to_real_ytid(ytid):\n",
    "  assert len(ytid) == 4\n",
    "  return f\"data.yt8m.org/2/j/i/{ytid[0:2]}/{ytid}.js\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penis = fake_to_real_ytid('pFac')\n",
    "penis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget data.yt8m.org/2/j/i/pF/pFac.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching transcripts from youtube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Full example. The benefit is we get the language code, and we can translate it if necessary.\n",
    "More programming work to manually check for translation. \n",
    "'''\n",
    "\n",
    "# video_id = ['bAYa_YjdHvI', 'AjpzS1xogJc']\n",
    "video_id = 'V1Z3OhDHzB4'\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# from Exception import suppress \n",
    "from contextlib import suppress\n",
    "\n",
    "# retrieve the available transcripts\n",
    "with suppress(Exception):\n",
    "    transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "    # print(\"\")\n",
    "else: \n",
    "\n",
    "    \n",
    "# iterate over all available transcripts\n",
    "for transcript in transcript_list:\n",
    "\n",
    "    # the Transcript object provides metadata properties\n",
    "    print(\n",
    "        transcript.video_id,\n",
    "        transcript.language_code,\n",
    "        transcript.language,\n",
    "        # whether it has been manually created or generated by YouTube\n",
    "        # transcript.is_generated,\n",
    "        # whether this transcript can be translated or not\n",
    "        \"is_translatable\", transcript.is_translatable,\n",
    "        # a list of languages the transcript can be translated to\n",
    "        # transcript.translation_languages,\n",
    "    )\n",
    "\n",
    "    # fetch the actual transcript data\n",
    "    print(transcript.fetch())\n",
    "\n",
    "    # translating the transcript will return another transcript object\n",
    "    # print(transcript.translate('en').fetch())\n",
    "\n",
    "# you can also directly filter for the language you are looking for, using the transcript list\n",
    "# transcript = transcript_list.find_transcript(['en'])  \n",
    "\n",
    "# or just filter for manually created transcripts  \n",
    "# transcript = transcript_list.find_manually_created_transcript(['en'])  \n",
    "\n",
    "# or automatically generated ones  \n",
    "# transcript = transcript_list.find_generated_transcript(['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = transcript_list.find_transcript(['en'])  \n",
    "print(transcript.fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET TRANSCRIPTS IN JSON. \n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import JSONFormatter\n",
    "\n",
    "# Must be a single transcript.\n",
    "transcript = YouTubeTranscriptApi.get_transcript('bAYa_YjdHvI')\n",
    "\n",
    "formatter = JSONFormatter()\n",
    "\n",
    "# .format_transcript(transcript) turns the transcript into a JSON string.\n",
    "json_formatted = formatter.format_transcript(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "video_id = 'bAYa_YjdHvI'\n",
    "# transcript_list = YouTubeTranscriptApi.list_transcripts(ytid)\n",
    "\n",
    "transcript_list = YouTubeTranscriptApi.get_transcripts([video_id], languages=['en'])\n",
    "\n",
    "# transcript.language_code\n",
    "# print(transcript_list)\n",
    "transcript_list[0].language_code\n",
    "# print(transcript_list.find_transcript(['en']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = 'bAYa_YjdHvI'\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "  \n",
    "# assigning srt variable with the list\n",
    "# of dictonaries obtained by the get_transcript() function\n",
    "srt = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "  \n",
    "# prints the result\n",
    "print(srt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning to raise custom exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    \"\"\"Base class for other exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "class empty_string_error(Error):\n",
    "    \"\"\"Raised when the input value is too large\"\"\"\n",
    "    # return \"String is empty, dumbass\"\n",
    "    pass\n",
    "\n",
    "try:\n",
    "  if(\"Your Condition\"):\n",
    "    raise empty_string_error\n",
    "  else:\n",
    "    print(\"OUTPUT\")\n",
    "except empty_string_error as e:\n",
    "  print(\"raising empty string error\", e)\n",
    "  print(\" \")\n",
    "finally:\n",
    "  print(\"Mandatory code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cpu_nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e37f0a8afa730d972bb2bedc65a55e2e751f510b12eab90711d3d898844e1392"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
